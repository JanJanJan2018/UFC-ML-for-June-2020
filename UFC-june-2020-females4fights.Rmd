---
title: "UFC ML actions and reactions females: Nunez"
author: "Janis Corona"
date: "5/18/2020-5/22/2020"
output: html_document
---

This script builds on top of a previous script to extract actions and reactions from the notes on fighter X1 and X2 in the UFC. These extracted features are used as dummy variables to predict the outcome of a landed hit or other selected target variable out of the given features and added features using machine learning on these numeric values. This script is for four separate opponents of **Amanda Nunez**: **Germaine, Tate, Pennington, and Rousey**. Only the first round up to five minutes or however long the fight lasted were recorded in the notes.

Github files [repository](https://github.com/JanJanJan2018/UFC-ML-for-June-2020)

These are the actions extracting and a brief description:

- **cross**: punch/strike/overhead hit/etc to hit with the top four knuckles of hand upon opponent's body or more likely face or side of the head, this could mean right cross or left cross, even when in the mount position of ground work the fighter could choose to hit opponent with a cross instead of a hammer or hook or upper cut punch, usually lifting the hand to add in gravity and acceleration with own strength and downward body movement intending to knock out, add to get opponent to release grip if in a lock, or to damage the face or cause pain

- **jab**: This is done normally when breaking the ice or trying to open up opponent, it is not intended to be a power shot or heavy hit, usually used repetitively or to stun before delivering a cross which is a heavier hit powered from momentum added from the hips etc. Typically done in the stand up dance when testing out opponent when neither is getting too close for combat

- **hammer**: This is a strike typically done in a ground in pound mount position, but can be while in the mount on top or on the bottom loosening a choke hold. The hit is done to smash and stun the face of opponent, and typically not a favorite, because it can hurt the fighter as much as the opponent due to the part of the hand being used. This type of strike is used for fast repetitive bursts to stun and get out of a lock when on the floor striking or preventing a choke or joint lock. The side of the fist used is the pinky side in alignment with the elbow. Can be lifted above the head to get more power in accelerating with gravity and body weight to hit opponent's face. This is why some fighters hit with a regular jab or cross instead to lessen damage to wrist and hand while stiking opponent. All strikes cause damage, but this is not likely to knock an opponent out, also the hand can be thrown off side to make the person get off balance and lose body control in the ground fight. The male fighters use this more than the females.

- **hook**: This is a heavy hit or used in combination with other strikes like the jab or cross, because if the first hit, opponent can just back away unless their against the cage or ropes. It stuns and can knock the breath out of opponent, adds to chances of getting a TKO or technical knock out or knockout. Its a sneaky punch delivered when opponents arms are up or down blocking head shots and/or body kicks. 

- **upper**: This is a puch delivered from upward momentum to strike the chin when up close in a clinch or hold of some sort. Not a powerful punch compared to other strikes, used to stun and to set up for other heavy hits or barrages of various strikes at once.

- **kick**: The push kick is the most powerful to move opponent off balance with little effort, but the muay thai kick (mt abrev.) is a very powerful kick if your skilled in the pivot and trained your shins to deliver bone to bone or head kicks with its snapping movement. It breaks down the balance and you see fighters deliver this to the lead leg to knock the opponent off balance and prevent leverage needed for heavy hits or crosses, it can also make the opponent stumble if they get their lead leg kicked hard enough to knock out of stance. Some go for the inner leg as well to knock off balance. This kick is also shown aimed at the outer lead leg Iliotibial or IT band to cause pain and test out their kick to lead up to a head kick or the opponent's defenses. Usually some punches or an attempt at a takedown of opponent occurs, unless they step backwards.

- **elbow**: This is a strike used standing up at close range, or in a ground and pound full mount position to drop with gravity while aiming the elbow towards the opponents face, not sure if the target is the forehead, but most opponents getting hit with the elbow end up with cuts/slashes/gashes on their forehead. This could be a technical knockout or some sort of stoppage if the opponent can't fight because blood gets in their eyes, and a doctor dismisses the fight due to inability to continue from injury. When cut, blood drips into the eyes. And it is said can cause blindness if the calcium hardens from the blood in the eye.

- **takedown**: This happens when the fighter thinks they want to either body slam the opponent or have a better chance at ground and pound and a chance at mounting opponent once taken down, or will out wrestle the opponent with a joint lock or choke. Wrestling may look less violent, but wrestling can leave joints in repair for 6-12 months in rehab, break ribs from the chest locks, and make the opponent lose oxygen with neck holds and pass out. Usually fighters tap out if they think they will pass out, and would rather tap out than get choked out or know they are not a strong enough wrestler to break apart the hold. Once it is locked, it can be held for many minutes until the fight round ends, but usually skilled fighters will find other ways to disarm their opponent with wrestling, hits, body shifting, etc.

- **knees**: These are used in clinches or holds of some sort, and can be aimed with high knees, jumping knees, knees striking the IT bands, or the abdomen, to stun and loosen grip to gain control. They can be very powerful if used right. Mazvidal knocked out a guy in under 5 seconds with a flying knee to the forehead. That was the only time I recognized seeing a knockout from a knee. It can cut the face and make the nose or lips bleed if hit with the knee.

We are going to run this script first, but the notes do keep track of which arm was used when making a cross, knee, or other strike other than takedown. Left is L and right is R before the name of the action/strike used and after either lands or misses.

As the extracted actions are listed, there is not an extracted description of which arm was used for the strike or action. There are also some types of wrestling moves noted in the notes, but the entirety of the observations didn't continually list the hold if still in a specific hold that second, only any changes observed. We can add in these features at the end.

Here is a brief description pulled from Wikipedia on **BJJ moves** that aren't extracted as features but could be useful:

**Guard**: person dominating on back with legs around opponent's waste, controls top opponent

**half Guard**: person dominating on back controling with one leg of the top opponent preventing the opponent from passing or gaining side control

**open Guard**: variations where person is on back but legs not wrapped around top opponent's waste, preventing the top opponent from striking or passing with feet or shins

**side control**: top person dominates at the side pinning shoulders and/or hips while striking and trying to gain a lock or choke

**full mount**: person on top dominates with legs above person on ground's hips and knees in armpits to prevent bottom opponent from striking or trying to gain control

**back mount**: ankles in thighs of person who is being dominated by person on back to maneuver a choke or lock

Aside: Having experienced this myself I know that there are things that happen in the hold that aren't illustrated to the viewers. Such as, if the person locks there legs around the rib cage, they are squeezing in an attempt to break their ribs or lower their ability to breathe with limited lung cavity movement, and the idea is to cut off the carotid with a foot, ankle arm and your arm when choking until the person taps out of passes out. FYI, experience matters. Having had 3 months training and going up against someone with 2 years experience and a height advantage I did not last longer than 59 seconds in limited mma, where punches to the face are illegal. FYI I was 27 and the opponent was 14 and both of us females at that time (I plan on dying a female with no changes in that feature until that finite point in time). Do not underestimate age. Also, it takes quite a bit of maneuvering to get out of a hold once locked. Also, too much wrestling leaves viewers who want to see hits or strikes and movement on the floor towards the opponent and not dancing around the ring to avoid opponent is how these fighters make a name for themselves. In other words, these fighters have to be to some extent bat crazy.



```{r, error=FALSE, message=FALSE, warning=FALSE}
library(dplyr)

```


```{r}
Nunez <- read.csv('Nunez4fights.csv', header=TRUE, sep=',', 
                 na.strings=c('','NA'))

colnames(Nunez)

```

Remove the instances with no action from either fighter X1 or X2.
```{r}
Added <- filter(Nunez, Nunez$FighterActionReactions.X1 !=0 | Nunez$FightersActionsReactions.X2 !=0)
```

Look at the notes on each fighter. The X1 is Felicia, the X2 is one of the three opponent's.
```{r}
head(Added$FighterActionReactions.X1, 10)
head(Added$FightersActionsReactions.X2,10)
```


List the unique notes for X1.
```{r}
unique(Added$FighterActionReactions.X1)
```

There are 155 unique actions for Amanda amongst these three opponents. Out of how many observations?
```{r}
dim(Added)
```

There are 393 observations in the data that excludes observations with no noticeable action or reaction from X1 or X2.

The unique actions/reactions from the opponent's separately but collectively are:
```{r}
unique(Added$FightersActionsReactions.X2)
```

There are 140 unique actions or reactions among the four different opponents' 1st round fight with Amanda. Only the first fight with Germaine counts the wrestling because the other 3 fights were not available to view the actions at the time of this document and were recycled from a script that looked at only the stand up fight for Tate, Rousey, and Pennington with Amanda.


Get fighter X1's list of actions and reactions to split on.
```{r}
Sym <- strsplit(as.character(Added$FighterActionReactions.X1), ',')

```


Create 1st sequence to grab actions from the table by index of observation that
the action occured; this grabs the index of occurence in the table from each observation as a vector.
```{r}
sq1 <- lapply(Sym,'[',1)
head(sq1,10)
```

```{r}
sq2 <- lapply(Sym,'[',2)
sq3 <- lapply(Sym,'[',3)
```

Lets also get the ground moves added to our data as features from Amanda's fight with Germaine only. The ground moves are the 'holding...hold' for the fighter holding some hold or body part as either an arm, back, full mount, back mount, etc. There is also a note of when the fighter 'loses...hold' or stops holding that was added but doesn't mean it isn't ground work done or submitted but that in some instances switched from a full mount hold to a side mount control hold, of lost back mount hold to holding back when legs not fully locked, etc. There is also the options for the opponent who 'breaks...hold' or 'caught in...hold' to add to this data for machine learning. I also want to distinguish between muay thai (mt) and push kick instead of just counting all kicks. The ground work was intentionally added to the first sequence before the comma separator, so we just need to add it as a grab from each fighter. But there are multiple holds in some instances and breaks, caught, and lost holds. So we need to account for the multiple holds. In some holds the opponent is being held or holding a hold but still getting hit, or 'ground and pound' and this is normal in mma. The opponent could be holding the body, arm, or hand to avoid getting hit while in a hold like full mount or full guard or back mount.
```{r}
hold <- grep('holding.+hold',sq1)
hold
```

There are more than a few instances for this fighter that involves ground work, clinches or holds while standing, and wrestling from the number of instances that a hold is used by this fighter. Now look at the number of instances this fighter is caught in a hold.This can be an arm hold when clinching while standing up or on the ground when the opponent grabs the arm to prevent getting hit by it or prevent a wrestling submission.

```{r}
caught <- grep('caught.*hold',sq1)
caught
```
More of the holds on Amanda by Germaine, because we miss the data on the other three fighters' wrestling moves, in either a limb to prevent striking or wrestling maneuvering, a mount or guard hold, or against the cage than she did.


Lets look at the lost holds.
```{r}
lost <- grep('loses.*hold', sq1)
lost
```
The lost holds for Amanda with Germaine are few. The other three fighters don't have the added wrestling moves. We will have to account for this when selecting the wrestling features for ML on this fighter later. We can just subset by Notes=='Germaine' when we get there. We're not there yet.

Now for the number of holds this fighter broke out of or the opponent (Germaine) quit holding.
```{r}
breaks <- grep('breaks.*hold',sq1)
breaks
```
Amanda or X1 got out of quite a bit of the attempted holds by X2 or Germaine.


Now lets check out the number of muay thai kicks and push kicks separately in the first sequence of each instance. When it comes to these actions, we can include all fighters, because these actions were recorded for the other fighters Tate, Rousey, and Pennington, as well as for Germaine.
```{r}
mtKicks <- grep('mt kick',sq1)
pushKicks <- grep('push kick',sq1)
mtKicks;pushKicks
```
We can see that X1 uses a lot of muay thai kicks, much more than she does push kicks in the 1st sequence.

Lets look at the right (R) or left (L) legs used to deliver the muay thai kicks or push kicks separately.
```{r}
LmtKicks <- grep('L mt kick',sq1)
RmtKicks <- grep('R mt kick',sq1)
Lpush <- grep('L push kick', sq1)
Rpush <- grep('R push kick', sq1)

LmtKicks;RmtKicks;Lpush;Rpush
```
X1 prefers to use the Right leg to deliver her muay thai kicks than her left leg, but does still use her left leg. Also, none of her push kicks come from her Left leg, they are all from her Right leg in the first sequence of up to three actions with each of four fighters' 1st rounds with her.

Lets look at the muay thai and push kicks by X1 in the 2nd and 3rd sequences as well to get an idea of how this fighter rates their own kicks in sequential order or usefulness.
```{r}
mtKicks2 <- grep('mt kick',sq2)
pushKicks2 <- grep('push kick',sq2)
mtKicks2;pushKicks2

mtKicks3 <- grep('mt kick',sq3)
pushKicks3 <- grep('push kick',sq3)
mtKicks3;pushKicks3

```
None of the 2nd or 3rd sequences involve any sort of kick. So, they must be used just for variation or surprise. There is also no point in seeing which is L or R as a kick if there are none.

***

Lets now look at the opponent's or X2's preference in the first second as an action or reaction by starting with the number of holds that X2 used. Noting that the wrestling actions are only valid for Amanda's fight with Germaine. 

This creates a list of the first sequence of three to select actions from for X2.
```{r}
SymX2 <- strsplit(as.character(paste(Added$FightersActionsReactions.X2)),',')
sq1X2 <- lapply(SymX2,'[',1)
sq2X2 <- lapply(SymX2,'[',2)
sq3X2 <- lapply(SymX2,'[',3)

```

Lets see the number of holds X1 (Amanda) was being held in by X1 (Germaine)
```{r}
holdX2 <- grep('holding.+hold',sq1X2)
holdX2
```
X2 used a lot of holds on X2 from the data provided above. 

Lets see the number of holds X2 was caught in.
```{r}
caughtX2 <- grep('caught.*hold',sq1X2)
caughtX2
```
There were quite a bit of holds X2 was held in from the above data.


Lets look at the lost holds by X2.
```{r}
lostX2 <- grep('loses.*hold', sq1X2)
lostX2
```
Some holds X2 either quit holding or X1 broke out of.

Now for the number of holds that x2 broke out of or the opponent quit holding.
```{r}
breaksX2 <- grep('breaks.*hold',sq1X2)
breaksX2
```
X2 also broke out of a fair share of holds.

Now lets check out the number of muay thai kicks and push kicks separately in the first sequence of each instance that X2 used.
```{r}
mtKicksX2 <- grep('mt kick',sq1X2)
pushKicksX2 <- grep('push kick',sq1X2)
mtKicksX2;pushKicksX2
```
We can see that X2 used a fair amount of push kicks and muay thai kicks. Recalling that we are now evaluating all four fighters as these actions were recorded for all 1st rounds with these fighters.
Keep in mind there are two other sequences that kicks could have been delivered, like in an open guard, where the opponent X2 uses an 'upward kick' which could be considered a push kick. Lets see.
```{r}
openPushX2 <- grep('upward.*kick',sq1X2)
openPushX2
```
We can see that some of the fighters did use the upward kicks and these are not duplicates of the muay thai or push kick indices. We can also add upward kicks, possibly from the X2 hold in open guard. This hold is more of a defense and is a lousy one but keeps them in the round longer. Its lousy because X1 can throw their legs to the side and still ground and pound, but the hits won't be as heavy as in a stand up bout, that could knock out this opponent.

We should also see if X1 has any upward kicks.
```{r}
openPushX1 <- grep('upward.*kick',sq1)
openPushX1
```
X1 doesn't use any upward kicks or use the open guard to kick off opponent.

How about in the 2nd and 3rd sequences, did X2 use any muay thai or push kicks?
```{r}
mtKicksX2b <- grep('mt kick',sq2X2)
pushKicksX2b <- grep('push kick',sq2X2)
mtKicksX2b;pushKicksX2b

mtKicksX2c <- grep('mt kick',sq3X2)
pushKicksX2c <- grep('push kick',sq3X2)
mtKicksX2c;pushKicksX2c

```
None of the fighters for x2 have used a muay thai or push kick. We know that there are some upward kicks for X2 only, so lets see if X2 also used the upward kick from open guard in the 2nd and 3rd sequence.
```{r}
openPushX2b <- grep('upward.*kick',sq2X2)
openPushX2c <- grep('upward.*kick',sq3X2)
openPushX2b;openPushX2c
```
X2 didn't use any upward kicks in the 2nd or 3rd sequences.

We should see which legs were prefered by those four fighters as X2, or either their Left or Right leg.
```{r}
LmtKicksX2 <- grep('L mt kick',sq1X2)
RmtKicksX2 <- grep('R mt kick',sq1X2)

LpushKicksX2 <- grep('L push kick',sq1X2)
RpushKicksX2 <- grep('R push kick',sq1X2)

LmtKicksX2;RmtKicksX2;LpushKicksX2;RpushKicksX2
```
Interestingly, none of the four fighters used their R leg for muay thai kicks, but used their Left leg instead. And the push kicks were mostly used by the Left leg and few used the Right leg for their push kicks. Those are the stand up kicks, not the upward kicks from open guard. The following is the upward kicks from open guard by X2 (Germaine only)
```{r}
LopenPushX2 <- grep('L upward.*kick',sq1X2)
RopenPushX2 <- grep('R upward.*kick',sq1X2)
LopenPushX2;RopenPushX2
```
Germaine used the upward kicks from open guard more with her Left leg than her Right leg.

The majority of muay thai kicks delivered to X1 by X2 (4 different opponents separately) used the left leg. This could be toward the lead leg to lessen the stability of X1 or knock off balance to reduce striking power delivered by X1's right cross, or to the head or body. Lets see.
```{r}
sq1X2[LmtKicksX2]
```
Most of the muay thai kicks by X2 to X1 are for the inside L leg or lead leg. Implying they are avoiding her right cross and want to keep her from having a strong Right cross by making her stability and leverage decline.
***

This next section of code will grab the indices of each specific action grep'd from the action text fields to count each action. We will do this for up to three sequences of actions/reactions in a second. An example of a few of the action vectors are displayed.
```{r}
kicks_sq1 <- grep('land.*kick', sq1)
elbows_sq1 <- grep('land.*elbow', sq1)
knees_sq1 <- grep('land.*knee', sq1)
jab_sq1 <- grep('land.*jab', sq1)
cross_sq1 <- grep('land.*cross', sq1)
hook_sq1 <- grep('land.*hook', sq1)
upper_sq1 <- grep('land.*upp', sq1)
takedown_sq1 <- grep('land.*takedown', sq1)
hammer_sq1 <- grep('land.*hammer', sq1)

hammer_sq1;takedown_sq1;upper_sq1;cross_sq1
```

missed in 1st sequence
```{r}
kicks_sq1m <- grep('miss.*kick', sq1)
elbows_sq1m <- grep('miss.*elbow', sq1)
knees_sq1m <- grep('miss.*knee', sq1)
jab_sq1m <- grep('miss.*jab', sq1)
cross_sq1m <- grep('miss.*cross', sq1)
hook_sq1m <- grep('miss.*hook', sq1)
upper_sq1m <- grep('miss.*upp', sq1)
takedown_sq1m <- grep('miss.*takedown', sq1)
hammer_sq1m <- grep('miss.*hammer', sq1)

```


landed in second sequence
```{r}
kicks_sq2 <- grep('land*kick', sq2)
elbows_sq2 <- grep('land.*elbow', sq2)
knees_sq2 <- grep('land.*knee', sq2)
jab_sq2 <- grep('land.*jab', sq2)
cross_sq2 <- grep('land.*cross', sq2)
hook_sq2 <- grep('land.*hook', sq2)
upper_sq2 <- grep('land.*upp', sq2)
takedown_sq2 <- grep('land.*takedown', sq2)
hammer_sq2 <- grep('land.*hammer', sq2)

```

missed in 2nd sequence
```{r}
kicks_sq2m <- grep('miss.*kick', sq2)
elbows_sq2m <- grep('miss.*elbow', sq2)
knees_sq2m <- grep('miss.*knee', sq2)
jab_sq2m <- grep('miss.*jab', sq2)
cross_sq2m <- grep('miss.*cross', sq2)
hook_sq2m <- grep('miss.*hook', sq2)
upper_sq2m <- grep('miss.*upp', sq2)
takedown_sq2m <- grep('miss.*takedown', sq2)
hammer_sq2m <- grep('miss.*hammer', sq2)

```


landed in 3rd sequence
```{r}
kicks_sq3 <- grep('land.*kick', sq3)
elbows_sq3 <- grep('land.*elbow', sq3)
knees_sq3 <- grep('land.*knee', sq3)
jab_sq3 <- grep('land.*jab', sq3)
cross_sq3 <- grep('land.*cross', sq3)
hook_sq3 <- grep('land.*hook', sq3)
upper_sq3 <- grep('land.*upp', sq3)
takedown_sq3 <- grep('land.*takedown', sq3)
hammer_sq3 <- grep('land.*hammer', sq3)

```

missed in 3rd sequence
```{r}
kicks_sq3m <- grep('miss.*kick', sq3)
elbows_sq3m <- grep('miss.*elbow', sq3)
knees_sq3m <- grep('miss.*knee', sq3)
jab_sq3m <- grep('miss.*jab', sq3)
cross_sq3m <- grep('miss.*cross', sq3)
hook_sq3m <- grep('miss.*hook', sq3)
upper_sq3m <- grep('miss.*upp', sq3)
takedown_sq3m <- grep('miss.*takedown', sq3)
hammer_sq3m <- grep('miss.*hammer', sq3)

```

get fighter2's list of actions/reactions
```{r}
sq1b <- sq1X2

```

lands 1st sequence X2, ends with 'b', no 'l' to either for lands
```{r}
kicks_sq1b <- grep('land.*kick', sq1b)
elbows_sq1b <- grep('land.*elbow', sq1b)
knees_sq1b <- grep('land.*knee', sq1b)
jab_sq1b <- grep('land.*jab', sq1b)
cross_sq1b <- grep('land.*cross', sq1b)
hook_sq1b <- grep('land.*hook', sq1b)
upper_sq1b <- grep('land.*upp', sq1b)
takedown_sq1b <- grep('land.*takedown', sq1b)
hammer_sq1b <- grep('land.*hammer', sq1b)

```

received by X1 in 1st sequence, duplicated above as 
equivalent to hits landed 1st seq of x2
```{r}
kicks_sq1r <- grep('land.*kick', sq1b)
elbows_sq1r <- grep('land.*elbow', sq1b)
knees_sq1r <- grep('land.*knee', sq1b)
jab_sq1r <- grep('land.*jab', sq1b)
cross_sq1r <- grep('land.*cross', sq1b)
hook_sq1r <- grep('land.*hook', sq1b)
upper_sq1r <- grep('land.*upp', sq1b)
takedown_sq1r <- grep('land.*takedown', sq1b)
hammer_sq1r <- grep('land.*hammer', sq1b)

```

missed in 1st sequence X2
```{r}
kicks_sq1bm <- grep('miss.*kick', sq1b)
elbows_sq1bm <- grep('miss.*elbow', sq1b)
knees_sq1bm <- grep('miss.*knee', sq1b)
jab_sq1bm <- grep('miss.*jab', sq1b)
cross_sq1bm <- grep('miss.*cross', sq1b)
hook_sq1bm <- grep('miss.*hook', sq1b)
upper_sq1bm <- grep('miss.*upp', sq1b)
takedown_sq1bm <- grep('miss.*takedown', sq1b)
hammer_sq1bm <- grep('miss.*hammer', sq1b)

```

received by x2 in 1st seq equivalent to lands by x1
```{r}
kicks_sq1br <- grep('land.*kick', sq1)
elbows_sq1br <- grep('land.*elbow', sq1)
knees_sq1br <- grep('land.*knee', sq1)
jab_sq1br <- grep('land.*jab', sq1)
cross_sq1br <- grep('land.*cross', sq1)
hook_sq1br <- grep('land.*hook', sq1)
upper_sq1br <- grep('land.*upp', sq1)
takedown_sq1br <- grep('land.*takedown', sq1)
hammer_sq1br <- grep('land.*hammer', sq1)

```

lands 2nd sequence x2
```{r}
sq2b <- sq2X2

```

```{r}
kicks_sq2b <- grep('land.*kick', sq2b)
elbows_sq2b <- grep('land.*elbow', sq2b)
knees_sq2b <- grep('land.*knee', sq2b)
jab_sq2b <- grep('land.*jab', sq2b)
cross_sq2b <- grep('land.*cross', sq2b)
hook_sq2b <- grep('land.*hook', sq2b)
upper_sq2b <- grep('land.*upp', sq2b)
takedown_sq2b <- grep('land.*takedown', sq2b)
hammer_sq2b <- grep('land.*hammer', sq2b)

```

received by X1 in 2nd sequence equivalent to hits landed by x2 seq 2
```{r}
kicks_sq2r <- grep('land.*kick', sq2b)
elbows_sq2r <- grep('land.*elbow', sq2b)
knees_sq2r <- grep('land.*knee', sq2b)
jab_sq2r <- grep('land.*jab', sq2b)
cross_sq2r <- grep('land.*cross', sq2b)
hook_sq2r <- grep('land.*hook', sq2b)
upper_sq2r <- grep('land.*upp', sq2b)
takedown_sq2r <- grep('land.*takedown', sq2b)
hammer_sq2r <- grep('land.*hammer', sq2b)

```

missed in 2nd sequence x2
```{r}
kicks_sq2bm <- grep('miss.*kick', sq2b)
elbows_sq2bm <- grep('miss.*elbow', sq2b)
knees_sq2bm <- grep('miss.*knee', sq2b)
jab_sq2bm <- grep('miss.*jab', sq2b)
cross_sq2bm <- grep('miss.*cross', sq2b)
hook_sq2bm <- grep('miss.*hook', sq2b)
upper_sq2bm <- grep('miss.*upp', sq2b)
takedown_sq2bm <- grep('miss.*takedown', sq2b)
hammer_sq2bm <- grep('miss.*hammer', sq2b)

```

received 2nd seq by x2 equivalent to hits landed by x1 in seq 2
```{r}
kicks_sq2br <- grep('land.*kick', sq2)
elbows_sq2br <- grep('land.*elbow', sq2)
knees_sq2br <- grep('land.*knee', sq2)
jab_sq2br <- grep('land.*jab', sq2)
cross_sq2br <- grep('land.*cross', sq2)
hook_sq2br <- grep('land.*hook', sq2)
upper_sq2br <- grep('land.*upp', sq2)
takedown_sq2br <- grep('land.*takedown', sq2)
hammer_sq2br <- grep('land.*hammer', sq2)

```

lands 3rd sequence x2
```{r}
sq3b <- sq3X2

```

```{r}
kicks_sq3b <- grep('land.*kick', sq3b)
elbows_sq3b <- grep('land.*elbow', sq3b)
knees_sq3b <- grep('land.*knee', sq3b)
jab_sq3b <- grep('land.*jab', sq3b)
cross_sq3b <- grep('land.*cross', sq3b)
hook_sq3b <- grep('land.*hook', sq3b)
upper_sq3b <- grep('land.*upp', sq3b)
takedown_sq3b <- grep('land.*takedown', sq3b)
hammer_sq3b <- grep('land.*hammer', sq3b)


```

received by X1 in 3rd sequence equivalent to hits landed by X2 in seq 3
```{r}
kicks_sq3r <- grep('land.*kick', sq3b)
elbows_sq3r <- grep('land.*elbow', sq3b)
knees_sq3r <- grep('land.*knee', sq3b)
jab_sq3r <- grep('land.*jab', sq3b)
cross_sq3r <- grep('land.*cross', sq3b)
hook_sq3r <- grep('land.*hook', sq3b)
upper_sq3r <- grep('land.*upp', sq3b)
takedown_sq3r <- grep('land.*takedown', sq3b)
hammer_sq3r <- grep('land.*hammer', sq3b)

```

missed in 3rd sequence x2
```{r}
kicks_sq3bm <- grep('miss.*kick', sq3b)
elbows_sq3bm <- grep('miss.*elbow', sq3b)
knees_sq3bm <- grep('miss.*knee', sq3b)
jab_sq3bm <- grep('miss.*jab', sq3b)
cross_sq3bm <- grep('miss.*cross', sq3b)
hook_sq3bm <- grep('miss.*hook', sq3b)
upper_sq3bm <- grep('miss.*upp', sq3b)
takedown_sq3bm <- grep('miss.*takedown', sq3b)
hammer_sq3bm <- grep('miss.*hammer', sq3b)

```

received in seq 3 by x2 equivalent to hits landed by x1 in seq3
```{r}
kicks_sq3br <- grep('land.*kick', sq3)
elbows_sq3br <- grep('land.*elbow', sq3)
knees_sq3br <- grep('land.*knee', sq3)
jab_sq3br <- grep('land.*jab', sq3)
cross_sq3br <- grep('land.*cross', sq3)
hook_sq3br <- grep('land.*hook', sq3)
upper_sq3br <- grep('land.*upp', sq3)
takedown_sq3br <- grep('land.*takedown', sq3)
hammer_sq3br <- grep('land.*hammer', sq3)

```

This adds the fields (54 fields for each sequence) to the table of actions per second, by
creating table extensions of Added, then renaming the 3rd sequence of actions.
```{r}
added_landed <- mutate(Added, Crossl.X1=0, Kneel.X1=0, Elbowl.X1=0, Hookl.X1=0, Jabl.X1=0, Kickl.X1=0,
                Crossl.X2=0, Kneel.X2=0, Elbowl.X2=0, Hookl.X2=0, Jabl.X2=0, Kickl.X2=0, upperl.X1=0,
                upperl.X2=0, takedownl.X1=0, takedownl.X2=0, hammerl.X1=0, hammerl.X2=0
                , Cross2l.X1=0, Knee2l.X1=0, Elbow2l.X1=0, Hook2l.X1=0, Jab2l.X1=0, Kick2l.X1=0,
                Cross2l.X2=0, Knee2l.X2=0, Elbow2l.X2=0, Hook2l.X2=0, Jab2l.X2=0, Kick2l.X2=0, upper2l.X1=0,
                upper2l.X2=0, takedown2l.X1=0, takedown2l.X2=0, hammer2l.X1=0, hammer2l.X2=0
                , Cross3l.X1=0, Knee3l.X1=0, Elbow3l.X1=0, Hook3l.X1=0, Jab3l.X1=0, Kick3l.X1=0,
                Cross3l.X2=0, Knee3l.X2=0, Elbow3l.X2=0, Hook3l.X2=0, Jab3l.X2=0, Kick3l.X2=0, upper3l.X1=0,
                upper3l.X2=0, takedown3l.X1=0, takedown3l.X2=0, hammer3l.X1=0, hammer3l.X2=0)

added_missed <- mutate(added_landed, Crossm.X1=0, Kneem.X1=0, Elbowm.X1=0, Hookm.X1=0, Jabm.X1=0, Kickm.X1=0,
                       Crossm.X2=0, Kneem.X2=0, Elbowm.X2=0, Hookm.X2=0, Jabm.X2=0, Kickm.X2=0, upperm.X1=0,
                       upperm.X2=0, takedownm.X1=0, takedownm.X2=0, hammerm.X1=0, hammerm.X2=0
                       , Cross2m.X1=0, Knee2m.X1=0, Elbow2m.X1=0, Hook2m.X1=0, Jab2m.X1=0, Kick2m.X1=0,
                       Cross2m.X2=0, Knee2m.X2=0, Elbow2m.X2=0, Hook2m.X2=0, Jab2m.X2=0, Kick2m.X2=0, upper2m.X1=0,
                       upper2m.X2=0, takedown2m.X1=0, takedown2m.X2=0, hammer2m.X1=0, hammer2m.X2=0
                       , Cross3m.X1=0, Knee3m.X1=0, Elbow3m.X1=0, Hook3m.X1=0, Jab3m.X1=0, Kick3m.X1=0,
                       Cross3m.X2=0, Knee3m.X2=0, Elbow3m.X2=0, Hook3m.X2=0, Jab3m.X2=0, Kick3m.X2=0, upper3m.X1=0,
                       upper3m.X2=0, takedown3m.X1=0, takedown3m.X2=0, hammer3m.X1=0, hammer3m.X2=0)

added_received <- mutate(added_missed, Crossr.X1=0, Kneer.X1=0, Elbowr.X1=0, Hookr.X1=0, Jabr.X1=0, Kickr.X1=0,
                       Crossr.X2=0, Kneer.X2=0, Elbowr.X2=0, Hookr.X2=0, Jabr.X2=0, Kickr.X2=0, upperr.X1=0,
                       upperr.X2=0, takedownr.X1=0, takedownr.X2=0, hammerr.X1=0, hammerr.X2=0
                       , Cross2r.X1=0, Knee2r.X1=0, Elbow2r.X1=0, Hook2r.X1=0, Jab2r.X1=0, Kick2r.X1=0,
                       Cross2r.X2=0, Knee2r.X2=0, Elbow2r.X2=0, Hook2r.X2=0, Jab2r.X2=0, Kick2r.X2=0, upper2r.X1=0,
                       upper2r.X2=0, takedown2r.X1=0, takedown2r.X2=0, hammer2r.X1=0, hammer2r.X2=0
                       , Cross3r.X1=0, Knee3r.X1=0, Elbow3r.X1=0, Hook3r.X1=0, Jab3r.X1=0, Kick3r.X1=0,
                       Cross3r.X2=0, Knee3r.X2=0, Elbow3r.X2=0, Hook3r.X2=0, Jab3r.X2=0, Kick3r.X2=0, upper3r.X1=0,
                       upper3r.X2=0, takedown3r.X1=0, takedown3r.X2=0, hammer3r.X1=0, hammer3r.X2=0)

```

Save original Added data table and make a new table called Added that is the combined
received, missed, and landed binary/dummy columns just mutated to each other above using dplyr.
```{r}
Added1 <- Added
```

```{r}
Added <- added_received
head(Added,10)
```

The following code adds a value of 1 if the grep'd binary field has a count in that index
of observation, otherwise, it will be 0.
```{r}
Added[cross_sq1,'Crossl.X1'] <- 1
Added[cross_sq1b,'Crossl.X2'] <- 1
Added[hook_sq1,'Hookl.X1'] <- 1
Added[hook_sq1b,'Hookl.X2'] <- 1
Added[jab_sq1,'Jabl.X1'] <- 1
Added[jab_sq1b,'Jabl.X2'] <- 1
Added[knees_sq1,'Kneel.X1'] <- 1
Added[knees_sq1b,'Kneel.X2'] <- 1
Added[elbows_sq1,'Elbowl.X1'] <- 1
Added[elbows_sq1b,'Elbowl.X2'] <- 1
Added[kicks_sq1,'Kickl.X1'] <- 1
Added[kicks_sq1b,'Kickl.X2'] <- 1
Added[upper_sq1,'upperl.X1'] <- 1
Added[upper_sq1b,'upperl.X2'] <- 1
Added[takedown_sq1,'takedownl.X1'] <- 1
Added[takedown_sq1b,'takedownl.X2'] <- 1
Added[hammer_sq1,'hammerl.X1'] <- 1
Added[hammer_sq1b,'hammerl.X2'] <- 1

Added[cross_sq2,'Cross2l.X1'] <- 1
Added[cross_sq2b,'Cross2l.X2'] <- 1
Added[hook_sq2,'Hook2l.X1'] <- 1
Added[hook_sq2b,'Hook2l.X2'] <- 1
Added[jab_sq2,'Jab2l.X1'] <- 1
Added[jab_sq2b,'Jab2l.X2'] <- 1
Added[knees_sq2,'Knee2l.X1'] <- 1
Added[knees_sq2b,'Knee2l.X2'] <- 1
Added[elbows_sq2,'Elbow2l.X1'] <- 1
Added[elbows_sq2b,'Elbow2l.X2'] <- 1
Added[kicks_sq2,'Kick2l.X1'] <- 1
Added[kicks_sq2b,'Kick2l.X2'] <- 1
Added[upper_sq2,'upper2l.X1'] <- 1
Added[upper_sq2b,'upper2l.X2'] <- 1
Added[takedown_sq2,'takedown2l.X1'] <- 1
Added[takedown_sq2b,'takedown2l.X2'] <- 1
Added[hammer_sq2,'hammer2l.X1'] <- 1
Added[hammer_sq2b,'hammer2l.X2'] <- 1

Added[cross_sq3,'Cross3l.X1'] <- 1
Added[cross_sq3b,'Cross3l.X2'] <- 1
Added[hook_sq3,'Hook3l.X1'] <- 1
Added[hook_sq3b,'Hook3l.X2'] <- 1
Added[jab_sq3,'Jab3l.X1'] <- 1
Added[jab_sq3b,'Jab3l.X2'] <- 1
Added[knees_sq3,'Knee3l.X1'] <- 1
Added[knees_sq3b,'Knee3l.X2'] <- 1
Added[elbows_sq3,'Elbow3l.X1'] <- 1
Added[elbows_sq3b,'Elbow3l.X2'] <- 1
Added[kicks_sq3,'Kick3l.X1'] <- 1
Added[kicks_sq3b,'Kick3l.X2'] <- 1
Added[upper_sq3,'upper3l.X1'] <- 1
Added[upper_sq3b,'upper3l.X2'] <- 1
Added[takedown_sq3,'takedown3l.X1'] <- 1
Added[takedown_sq3b,'takedown3l.X2'] <- 1
Added[hammer_sq3,'hammer3l.X1'] <- 1
Added[hammer_sq3b,'hammer3l.X2'] <- 1

Added[cross_sq1m,'Crossm.X1'] <- 1
Added[cross_sq1bm,'Crossm.X2'] <- 1
Added[hook_sq1m,'Hookm.X1'] <- 1
Added[hook_sq1bm,'Hookm.X2'] <- 1
Added[jab_sq1m,'Jabm.X1'] <- 1
Added[jab_sq1bm,'Jabm.X2'] <- 1
Added[knees_sq1m,'Kneem.X1'] <- 1
Added[knees_sq1bm,'Kneem.X2'] <- 1
Added[elbows_sq1m,'Elbowm.X1'] <- 1
Added[elbows_sq1bm,'Elbowm.X2'] <- 1
Added[kicks_sq1m,'Kickm.X1'] <- 1
Added[kicks_sq1bm,'Kickm.X2'] <- 1
Added[upper_sq1m,'upperm.X1'] <- 1
Added[upper_sq1bm,'upperm.X2'] <- 1
Added[takedown_sq1m,'takedownm.X1'] <- 1
Added[takedown_sq1bm,'takedownm.X2'] <- 1
Added[hammer_sq1m,'hammerm.X1'] <- 1
Added[hammer_sq1bm,'hammerm.X2'] <- 1

Added[cross_sq2m,'Cross2m.X1'] <- 1
Added[cross_sq2bm,'Cross2m.X2'] <- 1
Added[hook_sq2m,'Hook2m.X1'] <- 1
Added[hook_sq2bm,'Hook2m.X2'] <- 1
Added[jab_sq2m,'Jab2m.X1'] <- 1
Added[jab_sq2bm,'Jab2m.X2'] <- 1
Added[knees_sq2m,'Knee2m.X1'] <- 1
Added[knees_sq2bm,'Knee2m.X2'] <- 1
Added[elbows_sq2m,'Elbow2m.X1'] <- 1
Added[elbows_sq2bm,'Elbow2m.X2'] <- 1
Added[kicks_sq2m,'Kick2m.X1'] <- 1
Added[kicks_sq2bm,'Kick2m.X2'] <- 1
Added[upper_sq2m,'upper2m.X1'] <- 1
Added[upper_sq2bm,'upper2m.X2'] <- 1
Added[takedown_sq2m,'takedown2m.X1'] <- 1
Added[takedown_sq2bm,'takedown2m.X2'] <- 1
Added[hammer_sq2m,'hammer2m.X1'] <- 1
Added[hammer_sq2bm,'hammer2m.X2'] <- 1

Added[cross_sq3m,'Cross3m.X1'] <- 1
Added[cross_sq3bm,'Cross3m.X2'] <- 1
Added[hook_sq3m,'Hook3m.X1'] <- 1
Added[hook_sq3bm,'Hook3m.X2'] <- 1
Added[jab_sq3m,'Jab3m.X1'] <- 1
Added[jab_sq3bm,'Jab3m.X2'] <- 1
Added[knees_sq3m,'Knee3m.X1'] <- 1
Added[knees_sq3bm,'Knee3m.X2'] <- 1
Added[elbows_sq3m,'Elbow3m.X1'] <- 1
Added[elbows_sq3bm,'Elbow3m.X2'] <- 1
Added[kicks_sq3m,'Kick3m.X1'] <- 1
Added[kicks_sq3bm,'Kick3m.X2'] <- 1
Added[upper_sq3m,'upper3m.X1'] <- 1
Added[upper_sq3bm,'upper3m.X2'] <- 1
Added[takedown_sq3m,'takedown3m.X1'] <- 1
Added[takedown_sq3bm,'takedown3m.X2'] <- 1
Added[hammer_sq3m,'hammer3m.X1'] <- 1
Added[hammer_sq3bm,'hammer3m.X2'] <- 1

Added[cross_sq1r,'Crossr.X1'] <- 1
Added[cross_sq1br,'Crossr.X2'] <- 1
Added[hook_sq1r,'Hookr.X1'] <- 1
Added[hook_sq1br,'Hookr.X2'] <- 1
Added[jab_sq1r,'Jabr.X1'] <- 1
Added[jab_sq1br,'Jabr.X2'] <- 1
Added[knees_sq1r,'Kneer.X1'] <- 1
Added[knees_sq1br,'Kneer.X2'] <- 1
Added[elbows_sq1r,'Elbowr.X1'] <- 1
Added[elbows_sq1br,'Elbowr.X2'] <- 1
Added[kicks_sq1r,'Kickr.X1'] <- 1
Added[kicks_sq1br,'Kickr.X2'] <- 1
Added[upper_sq1r,'upperr.X1'] <- 1
Added[upper_sq1br,'upperr.X2'] <- 1
Added[takedown_sq1r,'takedownr.X1'] <- 1
Added[takedown_sq1br,'takedownr.X2'] <- 1
Added[hammer_sq1r,'hammerr.X1'] <- 1
Added[hammer_sq1br,'hammerr.X2'] <- 1

Added[cross_sq2r,'Cross2r.X1'] <- 1
Added[cross_sq2br,'Cross2r.X2'] <- 1
Added[hook_sq2r,'Hook2r.X1'] <- 1
Added[hook_sq2br,'Hook2r.X2'] <- 1
Added[jab_sq2r,'Jab2r.X1'] <- 1
Added[jab_sq2br,'Jab2r.X2'] <- 1
Added[knees_sq2r,'Knee2r.X1'] <- 1
Added[knees_sq2br,'Knee2r.X2'] <- 1
Added[elbows_sq2r,'Elbow2r.X1'] <- 1
Added[elbows_sq2br,'Elbow2r.X2'] <- 1
Added[kicks_sq2r,'Kick2r.X1'] <- 1
Added[kicks_sq2br,'Kick2r.X2'] <- 1
Added[upper_sq2r,'upper2r.X1'] <- 1
Added[upper_sq2br,'upper2r.X2'] <- 1
Added[takedown_sq2r,'takedown2r.X1'] <- 1
Added[takedown_sq2br,'takedown2r.X2'] <- 1
Added[hammer_sq2r,'hammer2r.X1'] <- 1
Added[hammer_sq2br,'hammer2r.X2'] <- 1

Added[cross_sq3r,'Cross3r.X1'] <- 1
Added[cross_sq3br,'Cross3r.X2'] <- 1
Added[hook_sq3r,'Hook3r.X1'] <- 1
Added[hook_sq3br,'Hook3r.X2'] <- 1
Added[jab_sq3r,'Jab3r.X1'] <- 1
Added[jab_sq3br,'Jab3r.X2'] <- 1
Added[knees_sq3r,'Knee3r.X1'] <- 1
Added[knees_sq3br,'Knee3r.X2'] <- 1
Added[elbows_sq3r,'Elbow3r.X1'] <- 1
Added[elbows_sq3br,'Elbow3r.X2'] <- 1
Added[kicks_sq3r,'Kick3r.X1'] <- 1
Added[kicks_sq3br,'Kick3r.X2'] <- 1
Added[upper_sq3r,'upper3r.X1'] <- 1
Added[upper_sq3br,'upper3r.X2'] <- 1
Added[takedown_sq3r,'takedown3r.X1'] <- 1
Added[takedown_sq3br,'takedown3r.X2'] <- 1
Added[hammer_sq3r,'hammer3r.X1'] <- 1
Added[hammer_sq3br,'hammer3r.X2'] <- 1

```

```{r}
colnames(Added)
```

```{r}
head(Added,10)
```


Removes SecondsIntoRound.
```{r}
Added2 <- Added[,-2] 
Seconds <- mutate(Added2, SecondsIntoRound=300-(as.numeric(Added2$Time)))
seconds <- Seconds[,c(1,181,2:180)]

```


```{r}
seconds$lastAction <- as.character(paste(lag(seconds$SecondsIntoRound,1)))
seconds$lastAction <- gsub('NA','0',seconds$lastAction)
seconds$lastAction <- as.numeric(paste(seconds$lastAction))
```


```{r}
colnames(seconds)[c(1:5,180:182)]
```

Rearrange the columns and remove the empty SecondsLastRoundAction field.
```{r}
seconds <- seconds[,c(1:2,182,4:181)]
colnames(seconds)[1:8]
```

Extension of seconds using mutate() of dplyr library, keeps value of seconds into round as
seconds since last action if no action that observation.
```{r}
last <- mutate(seconds, SecondsLastRoundAction = if_else(seconds$SecondsIntoRound -
                                                         seconds$lastAction > 0,
                                                       seconds$SecondsIntoRound -
                                                         seconds$lastAction,
                                                       seconds$SecondsIntoRound))

```

Reorders so that SecondsLastRoundAction is at front fields location.
```{r}
last <- last[,c(1:3,182,4:181)] 

```

Rearrange the order of the actions by fighter and landed, missed, and received. Also, add the counts for each accumulated landed actions, missed actions, and received actions per second observed.
```{r}
landX1 <- colnames(last)[c(21:26,33,35,37,39:44,51,53,55,57:62,69,71,73)]
landX2 <- colnames(last)[c(27:32,34,36,38,45:50,52,54,56,63:68,70,72,74)]

missX1 <- colnames(last)[c(75:80,87,89,91,93:98,105,107,109,111:116,123,125,127)]
missX2 <- colnames(last)[c(81:86,88,90,92,99:104,106,108,110,117:122,124,126,128)]

recvX1 <- colnames(last)[c(129:134,141,143,145,147:152,159,161,163,165:170,177,179,181)]
recvX2 <- colnames(last)[c(135:140,142,144,146,153:158,160,162,164,171:176,178,180,182)]

x1l <- mutate(last, TotLandsX1=last[,21]+last[,22]+last[,23]+last[,24]+last[,25]+
                last[,26]+last[,33]+last[,35]+last[,37]+last[,39]+last[,40]+
                last[,41]+last[,42]+last[,43]+last[,44]+last[,51]+last[,53]+
                last[,55]+last[,57]+last[,58]+last[,59]+last[,60]+last[,61]+
                last[,62]+last[,69]+last[,71]+last[,73])
x1m <- mutate(x1l, TotMissedX1=last[,75]+last[,76]+last[,77]+last[,78]+last[,79]+
                last[,80]+last[,87]+last[,89]+last[,91]+last[,93]+last[,94]+
                last[,95]+last[,96]+last[,97]+last[,98]+last[,105]+last[,107]+
                last[,109]+last[,111]+last[,112]+last[,113]+last[,114]+last[,115]+
                last[,116]+last[,123]+last[,125]+last[,127])
x1r <- mutate(x1m, TotReceivedX1=last[,129]+last[,130]+last[,131]+last[,132]+last[,133]+
                last[,134]+last[,141]+last[,143]+last[,145]+last[,147]+last[,148]+
                last[,149]+last[,150]+last[,151]+last[,152]+last[,159]+last[,161]+
                last[,163]+last[,165]+last[,166]+last[,167]+last[,168]+last[,169]+
                last[,170]+last[,177]+last[,179]+last[,181])

x2l <- mutate(x1r, TotLandsX2=last[,27]+last[,28]+last[,29]+last[,30]+last[,31]+
                last[,32]+last[,34]+last[,36]+last[,38]+last[,45]+last[,46]+
                last[,47]+last[,48]+last[,49]+last[,50]+last[,52]+last[,54]+
                last[,56]+last[,63]+last[,64]+last[,65]+last[,66]+last[,67]+
                last[,68]+last[,70]+last[,72]+last[,74])
x2m <- mutate(x2l, TotMissedX2=last[,81]+last[,82]+last[,83]+last[,84]+last[,85]+
                last[,86]+last[,88]+last[,90]+last[,92]+last[,99]+last[,100]+
                last[,101]+last[,102]+last[,103]+last[,104]+last[,106]+last[,108]+
                last[,110]+last[,117]+last[,118]+last[,119]+last[,120]+last[,121]+
                last[,122]+last[,124]+last[,126]+last[,128])
x2r <- mutate(x2m, TotReceivedX2=last[,135]+last[,136]+last[,137]+last[,138]+last[,139]+
                last[,140]+last[,142]+last[,144]+last[,146]+last[,153]+last[,154]+
                last[,155]+last[,156]+last[,157]+last[,158]+last[,160]+last[,162]+
                last[,164]+last[,171]+last[,172]+last[,173]+last[,174]+last[,175]+
                last[,176]+last[,178]+last[,180]+last[,182])

Added3 <- x2r[,c(1,2,3,4:7,183:185,11:13,186:188,17:182)]

colnames(Added3)
```

Create break points where last action in seconds is greater than the amount of seconds of the round. This is because there are three round 1's from three fights, where the cut off needs to be in place for the end of a round when counting seconds since last action of that round.
```{r}
break1 <- Added3$lastAction > Added3$SecondsIntoRound
break1
```

```{r}
break2 <- row.names(Added3[break1,])
break2
```

```{r}
bk2 <- as.numeric(break2)
bk2
```

```{r}
split1 <- bk2[1]
split1
```

```{r}
split2 <- bk2[2]
split2
```

```{r}
split3 <- bk2[3]
split3
```
First opponent:
```{r}
Table1 <- Added3[1:(split1-1),]
Table1
```
Second opponent:
```{r}
Table2 <- Added3[split1:(split2-1),]
Table2
```
Third opponent:
```{r}
Table3 <- Added3[split2:(split3-1),]
Table3
```

Fourth opponent:
```{r}
Table3b <- Added3[split3:(length(Added3$Round)),]
Table3b
```


This next section creates the cumulative actions of hits landed,missed, or received for each second
into the round for each of the three table splits by opponent, since only first round of various fights extracted and just created above.
```{r}
Table4 <- mutate(Table1, cmTotHitsR.X1=cumsum(TotReceivedX1), 
                cmTotHitsL.X1=cumsum(TotLandsX1),
                cmTotHitsM.X1=cumsum(TotMissedX1),
                cmTotHitsR.X2=cumsum(TotReceivedX2),
                cmTotHitsL.X2=cumsum(TotLandsX2),
                cmTotHitsM.X2=cumsum(TotMissedX2))

Table5 <- mutate(Table2, cmTotHitsR.X1=cumsum(TotReceivedX1), 
                 cmTotHitsL.X1=cumsum(TotLandsX1),
                 cmTotHitsM.X1=cumsum(TotMissedX1),
                 cmTotHitsR.X2=cumsum(TotReceivedX2),
                 cmTotHitsL.X2=cumsum(TotLandsX2),
                 cmTotHitsM.X2=cumsum(TotMissedX2))

Table6 <- mutate(Table3, cmTotHitsR.X1=cumsum(TotReceivedX1), 
                 cmTotHitsL.X1=cumsum(TotLandsX1),
                 cmTotHitsM.X1=cumsum(TotMissedX1),
                 cmTotHitsR.X2=cumsum(TotReceivedX2),
                 cmTotHitsL.X2=cumsum(TotLandsX2),
                 cmTotHitsM.X2=cumsum(TotMissedX2))

Table6b <- mutate(Table3b, cmTotHitsR.X1=cumsum(TotReceivedX1), 
                 cmTotHitsL.X1=cumsum(TotLandsX1),
                 cmTotHitsM.X1=cumsum(TotMissedX1),
                 cmTotHitsR.X2=cumsum(TotReceivedX2),
                 cmTotHitsL.X2=cumsum(TotLandsX2),
                 cmTotHitsM.X2=cumsum(TotMissedX2))

```

This combines the three table splits with cumulative sum of actions for each unique round or fighter.
```{r}
Table7 <- rbind(Table4,Table5,Table6,Table6b)
dim(Table4)[1]+dim(Table5)[1]+dim(Table6)[1]+dim(Table6b)[1]==dim(Added)[1]
```
All observations are accounted for in the subsets of tables since the sum of all observations in each subset table equals the number of observations in the table of all data with action notes for either of X1 or X2.


```{r}
colnames(Table7)

```

The following rearranges the table columns by actions landed, missed, and received into a new table.
```{r}
landX1 <- c(21:26,33,35,37,39:44,51,53,55,57:62,69,71,73)
landX2 <- c(27:32,34,36,38,45:50,52,54,56,63:68,70,72,74)

missX1 <- c(75:80,87,89,91,93:98,105,107,109,111:116,123,125,127)
missX2 <- c(81:86,88,90,92,99:104,106,108,110,117:122,124,126,128)

recvX1 <- c(129:134,141,143,145,147:152,159,161,163,165:170,177,179,181)
recvX2 <- c(135:140,142,144,146,153:158,160,162,164,171:176,178,180,182)

Table8 <- Table7[,c(1:20,landX1,landX2,missX1,missX2,recvX1,recvX2)]
colnames(Table8)

```

We should now add in the wrestling holds,caught in, breaks hold, and lost hold features for each fighter of X1 or X2 in each instance, not sequence. We already made these fields at the beginning of this script. Lets also add in the muay thai and push kicks for each sequence noting that these are already counted in the 'kicks' feature. We will just count them all as attempted action or reaction regardless if it landed or missed opponent. The target was blocked if not landed or ducked. We can account for these changes later in a different script if needed or you could do it yourself and not be critical of my work donations. Aside: mutalate misogynist monsters as a stepping stone instead of small creatures as a message to anybody who gets this nerdy break-down of violent and graphic content. Leave the squirrels, bunnys, guneau pigs, and females alone.

The 'breaks','caught','hold', and 'lost' are for X1, and the 'breaksX2','holdX2','caughtX2', and 
'lostX2' are for X2 ground or submit type wrestling actions or reactions. For the kicks, 'mtKicks' and 'pushKicks' are for X1 and 'mtKicksX2' and 'pushKicksX2' are for X2 actions or reactions. Those will be taken from only the 1st sequence for this fighter because none of the 2nd and 3rd sequences had any push or muay thai kicks from either fighter. The lists to pull from are sq1 for X1 and sq1b for X2. 

Lets make a new table from Table8.
```{r}
Table9 <- Table8
```

Create the placeholders for each action feature added.
```{r}
Table9$holdingX1 <- 0
Table9$holdingX2 <- 0
Table9$breaksHoldX1 <- 0
Table9$breaksHoldX2 <- 0 
Table9$caughtHoldX1 <- 0  
Table9$caughtHoldX2 <- 0  
Table9$lostHoldX1 <- 0  
Table9$lostHoldX2 <- 0   
```

```{r}
colnames(Table9)[183:190]
```

```{r}
Table9[hold,"holdingX1"] <- 1 
Table9[holdX2,"holdingX2"] <- 1 
Table9[breaks,"breaksHoldX1"] <- 1
Table9[breaksX2,"breaksHoldX2"] <- 1 
Table9[caught,"caughtHoldX1"] <- 1  
Table9[caughtX2,"caughtHoldX2"] <- 1  
Table9[lost,"lostHoldX1"] <- 1  
Table9[lostX2,"lostHoldX2"] <- 1   

```

Now add in the muay thai, push kicks, and open guard upward kicks to this table.
```{r}
Table9$muayThaiKickX1 <- 0
Table9$muayThaiKickX2 <- 0
Table9$pushKickX1 <- 0
Table9$pushKickX2 <- 0
Table9$openGuardKickX1 <- 0
Table9$openGuardKickX2 <- 0
```

```{r}
Table9[mtKicks,"muayThaiKickX1"] <- 1
Table9[mtKicksX2,"muayThaiKickX2"] <- 1
Table9[pushKicks,"pushKickX1"] <- 1
Table9[pushKicksX2,"pushKickX2"] <- 1
Table9[openPushX1,"openGuardKickX1"] <- 1
Table9[openPushX2,"openGuardKickX2"] <- 1
```

We should also add in the cumulative count of each instance of holding, lost holds, broken holds, and instances caught in a hold as well as the cumulative counts of muay thai and push kicks attempted by each fighter even though only Germaine has these actions recorded as X2. Since X2 has four different fighters, we need to split the Table 9 into a table of each opponent as we did earlier, get the cumulative counts, then recombine the table splits. We will keep this in mind when we run these features as predictors (only the wrestling features) that only Germaine can be used to predict the target of hit landed by X1 when that time comes.

We already have the break points for each new fighter as X2. So instead of redoing the break points to split data we will use those splits already created.
Create the three tables of the separate fighters of X2.
```{r}
table1_a <- Table9[1:(split1-1),]
table1_b <- Table9[split1:(split2-1),]
table1_c <- Table9[split2:(split3-1),]
table1_d <- Table9[split3:length(Table9$Round),]

dim(table1_a)[1]+dim(table1_b)[1]+dim(table1_c)[1]+dim(table1_d)[1]==dim(Table9)[1]
```
All observations are accounted for as the number of observations in all four subset tables of Table 9 add up to the total observations in Table 9.

The first fighter (as X2) table.
```{r}
table1_a$totalHoldsX1 <- cumsum(table1_a$holdingX1)
table1_a$totalHoldsX2 <- cumsum(table1_a$holdingX2)
table1_a$totalLostHoldsX1 <- cumsum(table1_a$lostHoldX1)
table1_a$totalLostHoldsX2 <- cumsum(table1_a$lostHoldX2)
table1_a$totalCaughtHoldsX1 <- cumsum(table1_a$caughtHoldX1)
table1_a$totalCaughtHoldsX2 <- cumsum(table1_a$caughtHoldX2)
table1_a$totalBreakOutHoldsX1 <- cumsum(table1_a$breaksHoldX1)
table1_a$totalBreakOutHoldsX2 <- cumsum(table1_a$breaksHoldX2)
table1_a$totalMuayThaiKicksX1 <- cumsum(table1_a$muayThaiKickX1)
table1_a$totalMuayThaiKicksX2 <- cumsum(table1_a$muayThaiKickX2)
table1_a$totalPushKicksX1 <- cumsum(table1_a$pushKickX1)
table1_a$totalPushKicksX2 <- cumsum(table1_a$pushKickX2)
table1_a$totalopenguardKicksX1 <- cumsum(table1_a$openGuardKickX1)
table1_a$totalopenguardKicksX2 <- cumsum(table1_a$openGuardKickX2)

tail(table1_a[,197:210],10)
```

The second fighter (as X2) table.
```{r}
table1_b$totalHoldsX1 <- cumsum(table1_b$holdingX1)
table1_b$totalHoldsX2 <- cumsum(table1_b$holdingX2)
table1_b$totalLostHoldsX1 <- cumsum(table1_b$lostHoldX1)
table1_b$totalLostHoldsX2 <- cumsum(table1_b$lostHoldX2)
table1_b$totalCaughtHoldsX1 <- cumsum(table1_b$caughtHoldX1)
table1_b$totalCaughtHoldsX2 <- cumsum(table1_b$caughtHoldX2)
table1_b$totalBreakOutHoldsX1 <- cumsum(table1_b$breaksHoldX1)
table1_b$totalBreakOutHoldsX2 <- cumsum(table1_b$breaksHoldX2)
table1_b$totalMuayThaiKicksX1 <- cumsum(table1_b$muayThaiKickX1)
table1_b$totalMuayThaiKicksX2 <- cumsum(table1_b$muayThaiKickX2)
table1_b$totalPushKicksX1 <- cumsum(table1_b$pushKickX1)
table1_b$totalPushKicksX2 <- cumsum(table1_b$pushKickX2)
table1_b$totalopenguardKicksX1 <- cumsum(table1_b$openGuardKickX1)
table1_b$totalopenguardKicksX2 <- cumsum(table1_b$openGuardKickX2)

tail(table1_b[,197:210],10)
```

The third fighter (as X2) table.
```{r}
table1_c$totalHoldsX1 <- cumsum(table1_c$holdingX1)
table1_c$totalHoldsX2 <- cumsum(table1_c$holdingX2)
table1_c$totalLostHoldsX1 <- cumsum(table1_c$lostHoldX1)
table1_c$totalLostHoldsX2 <- cumsum(table1_c$lostHoldX2)
table1_c$totalCaughtHoldsX1 <- cumsum(table1_c$caughtHoldX1)
table1_c$totalCaughtHoldsX2 <- cumsum(table1_c$caughtHoldX2)
table1_c$totalBreakOutHoldsX1 <- cumsum(table1_c$breaksHoldX1)
table1_c$totalBreakOutHoldsX2 <- cumsum(table1_c$breaksHoldX2)
table1_c$totalMuayThaiKicksX1 <- cumsum(table1_c$muayThaiKickX1)
table1_c$totalMuayThaiKicksX2 <- cumsum(table1_c$muayThaiKickX2)
table1_c$totalPushKicksX1 <- cumsum(table1_c$pushKickX1)
table1_c$totalPushKicksX2 <- cumsum(table1_c$pushKickX2)
table1_c$totalopenguardKicksX1 <- cumsum(table1_c$openGuardKickX1)
table1_c$totalopenguardKicksX2 <- cumsum(table1_c$openGuardKickX2)

tail(table1_c[,197:210],10)
```

The fourth fighter (as X2) table.
```{r}
table1_d$totalHoldsX1 <- cumsum(table1_d$holdingX1)
table1_d$totalHoldsX2 <- cumsum(table1_d$holdingX2)
table1_d$totalLostHoldsX1 <- cumsum(table1_d$lostHoldX1)
table1_d$totalLostHoldsX2 <- cumsum(table1_d$lostHoldX2)
table1_d$totalCaughtHoldsX1 <- cumsum(table1_d$caughtHoldX1)
table1_d$totalCaughtHoldsX2 <- cumsum(table1_d$caughtHoldX2)
table1_d$totalBreakOutHoldsX1 <- cumsum(table1_d$breaksHoldX1)
table1_d$totalBreakOutHoldsX2 <- cumsum(table1_d$breaksHoldX2)
table1_d$totalMuayThaiKicksX1 <- cumsum(table1_d$muayThaiKickX1)
table1_d$totalMuayThaiKicksX2 <- cumsum(table1_d$muayThaiKickX2)
table1_d$totalPushKicksX1 <- cumsum(table1_d$pushKickX1)
table1_d$totalPushKicksX2 <- cumsum(table1_d$pushKickX2)
table1_d$totalopenguardKicksX1 <- cumsum(table1_d$openGuardKickX1)
table1_d$totalopenguardKicksX2 <- cumsum(table1_d$openGuardKickX2)

tail(table1_d[,197:210],10)
```

Now we need to create a new table of these combined tables of cumulative sums.
```{r}
Table10 <- rbind(table1_a,table1_b,table1_c,table1_d)
dim(Table10)
dim(Table9)
```

```{r}
head(Table10)
```

Write this table of added action/reaction features to csv file.
```{r}
write.csv(Table10, 'Nunez4Fights_addedFeatures.csv', row.names=F)

```

***
***
***

# Machine Learning with R packages 

This section will test out these features in part to predict the target of X1 landing a hit or missing a hit. We have to exclude the features where X2 received a hit if the instance shows a hit landed, so the model learns well. Random forest, recursive partitioned trees, generalized linear models, knn, and gradient boosted models within the R packages of Caret will be used to see how well these features predict the accuracy in a hit landed. Note that we do have all wrestling features for this data set. 

```{r}
library(caret)
```


Make a table to predict the hit landed for X1 using all features except for the time, notes, X1 landed, X1 missed, X1 received, and X2 received features. 
```{r}
colnames(Table10)
```


```{r}
ML_table <- Table10[,c(2:16,48:74,102:128,183:206)]
str(ML_table)
```


Write this table out to use in Python later. Its easier than selecting the columns in Python.
```{r}
write.csv(ML_table, 'Amanda_ml_ready.csv', row.names=FALSE)
```

Our target variable is TotLandsX1 which will be a hit landed for X1 based on the other features that include the wrestling features of both X1 and X2, and actions of X2 for hits landed and missed to see how this can predict if X1 will land more hits based on the number of attempts on X1, the seconds holding and being held, the number of holds broken or lost, the seconds since last action by either X1 or X2, the seconds into the round, and cumulative totals for all landed, missed, and received hits for X1 and X2. Since all values are numeric, this will make it easier to classify the hit with linear regression as closer to zero or up to the max hits landed in an observation. We should see what that value is for the max number of hits landed in one second.
```{r}
max(ML_table$TotLandsX1)
```
Amanda has up to three hits landed in an instance. 


Lets split the data into a 70% training set and a 30% testing set.
```{r}
set.seed(12345)
inTrain <- createDataPartition(y=ML_table$TotLandsX1, p=0.7, list=FALSE)

trainingSet <- ML_table[inTrain,]
testingSet <- ML_table[-inTrain,]
```

Lets get the dimensions of the testing and training set to see how many observations are in each subset of our data.
```{r}
dim(testingSet)
dim(trainingSet)
```

testing set has 158 samples. Lets see how many of each total landed hits by X1 are in each subset.
```{r}
library(dplyr)

```


```{r}
summaryLandedHitsX1 <- trainingSet %>% group_by(TotLandsX1) %>% count(TotLandsX1)
summaryLandedHitsX1
```

There is only one instance where 3 hits were landed in the training subset it could lead to errors in prediction on the testing set, where recall, or other values could get misclassifed as 3, and if it is in our testing set, then the value isn't expected or shouldn't be so we would win on precision as none of the 3s were identified correctly (recall) but also no other classe other than a 3 were classified as 3 (precision). There are also much more 0s in our training set.

```{r}
summaryLandedHitsX1b <- testingSet %>% group_by(TotLandsX1) %>% count(TotLandsX1)
summaryLandedHitsX1b
```

There are no 3s in our testing set, and few 1s and 2s compared to the number of 0s. The recall could be low on the 2s, but precision should be good on the 0s and 1s.Due to the higher number of those instances being trained in our model.

Lets start with the random forest model using the boot method and also will be centering and scaling the features in our model. We will use the cv or cross validation which tests on subsets when training to build the model, and set classProbs to TRUE so that regression isn't done on our numeric values, but classification for classes: 0,1,or 2.
```{r, error=FALSE, message=FALSE, warning=FALSE}
rf_cv15 <- train(TotLandsX1~., method='rf', 
               na.action=na.pass,
               data=(trainingSet),  preProc = c("center", "scale"),
               trControl=trainControl(method='cv', classProbs = T), number=15)
```

```{r}
predRF_cv15 <- predict(rf_cv15, testingSet)

DF_cv15 <- data.frame(predRF_cv15, ActualHitsLanded=testingSet$TotLandsX1)

length_cv15 <- length(DF_cv15$ActualHitsLanded)

sum_cv15 <- sum(DF_cv15$predRF_boot==DF_cv15$ActualHitsLanded)

accRF_cv15 <- (sum_cv15/length_cv15)

accRF_cv15
```



```{r}
head(DF_cv15,30)
```

The values predicted are outside their class probabilities, probably due to the centered scaling or normalisaion. 

Lets try this again without preprocessing which will 
```{r, error=FALSE, message=FALSE, warning=FALSE}
rf_cv15b <- train(TotLandsX1~., method='rf', 
               na.action=na.pass,
               data=(trainingSet),  #preProc = c("center", "scale"),
               trControl=trainControl(method='cv', classProbs = T), number=15)
```


```{r}
predRF_cv15b <- predict(rf_cv15b, testingSet)

DF_cv15b <- data.frame(predRF_cv15b, ActualHitsLanded=testingSet$TotLandsX1)

length_cv15b <- length(DF_cv15b$ActualHitsLanded)

sum_cv15b <- sum(DF_cv15b$predRF_boot==DF_cv15b$ActualHitsLanded)

accRF_cv15b <- (sum_cv15b/length_cv15b)

accRF_cv15b
```


```{r}
head(DF_cv15b,30)
```


Lets add in an extra feature that will round these values to 0, 1, or 2 on both random forest models to get a realistic output. Even when not normalizing and using classProbs set to True, the classification is producing probability classes instead. Could be due to the numeric type of the target. Lets see if we change the numeric type to a factor if it classifies better. Or the same as our rounded features of the random forest models normalized and not normalized.
```{r}
trainingSetFactor <- trainingSet
testingSetFactor <- testingSet

trainingSetFactor$TotLandsX1 <- as.factor(paste(trainingSetFactor$TotLandsX1))
testingSetFactor$TotLandsX1 <- as.factor(paste(testingSetFactor$TotLandsX1))
```


```{r, error=FALSE, message=FALSE, warning=FALSE}
rf_cv15c <- train(TotLandsX1~., method='rf', 
               na.action=na.pass,
               data=(trainingSetFactor),  #preProc = c("center", "scale"),
               trControl=trainControl(method='cv'), number=15)
```


```{r}
predRF_cv15c <- predict(rf_cv15c, testingSetFactor)

DF_cv15c <- data.frame(predRF_cv15c, ActualHitsLanded=testingSet$TotLandsX1)

length_cv15c <- length(DF_cv15c$ActualHitsLanded)

sum_cv15c <- sum(DF_cv15c$predRF_cv15c==DF_cv15c$ActualHitsLanded)

accRF_cv15c <- (sum_cv15c/length_cv15c)

accRF_cv15c
```


```{r}
head(DF_cv15c,30)
```

It did work better, because the accuracy went from 0 to 100% accuracy in predicting the hits landed correctly.

Lets still add in the features to the previous random forest models to compare.Our minimum bound is 0 and
our maximum bound is 3 for hits landed. Lets set those boundaries and round to the nearest value within those bounds.
```{r}
DF_cv15$roundedPrediction <- ifelse(DF_cv15$predRF_cv15 < 0, 0, 
                                    ifelse(DF_cv15$predRF_cv15 > 3, 3,
                                           round(DF_cv15$predRF_cv15)
                                           )
                                    )
DF_cv15$Correct <- ifelse(DF_cv15$ActualHitsLanded==DF_cv15$roundedPrediction,1,0)
accuracy1 <- sum(DF_cv15$Correct)/length(DF_cv15$Correct)
accuracy1
```
After setting the boundaries and rounding within those boundaries, our numeric classification was also able to score 100% accuracy.


```{r}
head(DF_cv15)
```


The non normalized random forest model results when rounding to boundaries or closest class:
```{r}
DF_cv15b$roundedPrediction <- ifelse(DF_cv15b$predRF_cv15 < 0, 0, 
                                    ifelse(DF_cv15b$predRF_cv15 > 3, 3,
                                           round(DF_cv15b$predRF_cv15)
                                           )
                                    )
DF_cv15b$Correct <- ifelse(DF_cv15b$ActualHitsLanded==DF_cv15b$roundedPrediction,1,0)
accuracy2 <- sum(DF_cv15b$Correct)/length(DF_cv15b$Correct)
accuracy2
```

This model also scored 100% accuracy after adjusting for boundaries and rounding within those boundaries for our non-normalized features.


```{r}
head(DF_cv15b)
```


Next, we will use the k-nearest neighbor algorithm with the cv method to train.
```{r}
knn_cv <- train(TotLandsX1 ~ .,
                method='knn',# preProcess=c('center','scale'),
                tuneLength=10, trControl=trainControl(method='cv'),
                data=trainingSet)
```


```{r}
predKNN_cv <- predict(knn_cv, testingSet)

DF_KNN_cv <- data.frame(predKNN_cv, ActualHitsLanded=testingSet$TotLandsX1)

length_KNN_cv <- length(DF_KNN_cv$ActualHitsLanded)

sum_KNN_cv <- sum(DF_KNN_cv$predKNN_cv==DF_KNN_cv$ActualHitsLanded)

accKNN_cv <- (sum_KNN_cv/length_KNN_cv)

accKNN_cv
```


```{r}
head(DF_KNN_cv)
```

Lets add in the same features of rounded and correct fields to this table as well. As the predicted values are not integers.
```{r}
DF_KNN_cv$roundedPrediction <- ifelse(DF_KNN_cv$predKNN_cv<0,0,
                            ifelse(DF_KNN_cv$predKNN_cv>3,3,
                                   round(DF_KNN_cv$predKNN_cv,0)))
DF_KNN_cv$Correct <- ifelse(DF_KNN_cv$ActualHitsLanded==DF_KNN_cv$roundedPrediction,1,0)
accuracy3 <- sum(DF_KNN_cv$Correct)/length(DF_KNN_cv$Correct)
accuracy3
```

The KNN algorithm scored 82.9% accuracy after adjusting the output for min and max bounds and rounding within those bounds.


Next, we will use the k-nearest neighbor algorithm with the cv method to train on the target as a factor instead of numeric.
```{r}
knn_cvb <- train(TotLandsX1 ~ .,
                method='knn',# preProcess=c('center','scale'),
                tuneLength=10, trControl=trainControl(method='cv'),
                data=trainingSetFactor)
```


```{r}
predKNN_cvb <- predict(knn_cvb, testingSetFactor)

DF_KNN_cvb <- data.frame(predKNN_cvb, ActualHitsLanded=testingSet$TotLandsX1)

length_KNN_cvb <- length(DF_KNN_cvb$ActualHitsLanded)

sum_KNN_cvb <- sum(DF_KNN_cvb$predKNN_cvb==DF_KNN_cvb$ActualHitsLanded)

accKNN_cvb <- (sum_KNN_cvb/length_KNN_cvb)

accKNN_cvb
```

The KNN algorithm scored best with the target as a factor to predict instead of numeric, with a score of 85.5% accuracy.


```{r}
head(DF_KNN_cvb)
```


Now lets see how well this data will do in a few other algorithms: glm, rpart, and gbm. We have a factor version target and a numeric target with which to compare results.
```{r, error=FALSE, message=FALSE, warning=FALSE}
glmMod2 <- train(TotLandsX1 ~ .,
                method='glm', data=trainingSet)
```

```{r, error=FALSE, warning=FALSE, message=FALSE}
predglm2 <- predict(glmMod2, testingSet)

DF_glm2 <- data.frame(predglm2, ActualHitsLanded=testingSet$TotLandsX1)

length_glm2 <- length(DF_glm2$ActualHitsLanded)

sum_glm2 <- sum(DF_glm2$predglm2==DF_glm2$ActualHitsLanded)

accglm2 <- (sum_glm2/length_glm2)

accglm2
```


```{r}
head(DF_glm2)
```


```{r}
DF_glm2$roundedPrediction <- ifelse(DF_glm2$predglm2<0,0,
                            ifelse(DF_glm2$predglm2>3,3,
                                   round(DF_glm2$predglm2,0)))
DF_glm2$Correct <- ifelse(DF_glm2$ActualHitsLanded==DF_glm2$roundedPrediction,1,0)
accuracy4 <- sum(DF_glm2$Correct)/length(DF_glm2$Correct)
accuracy4
```
This generalized linear model scored 100% accuracy after adjusting the boundaries and rounding within those boundaries.


```{r}
head(DF_glm2)
```


```{r, error=FALSE, message=FALSE, warning=FALSE}
rpartMod <- train(TotLandsX1 ~ .,
                method='rpart', data=trainingSet)
```

```{r, error=FALSE, warning=FALSE, message=FALSE}
predrpart <- predict(rpartMod, testingSet)

DF_rpart <- data.frame(predrpart, ActualHitsLanded=testingSet$TotLandsX1)

length_rpart <- length(DF_rpart$ActualHitsLanded)

sum_rpart <- sum(DF_rpart$predrpart==DF_rpart$ActualHitsLanded)

accrpart <- (sum_rpart/length_rpart)

accrpart
```

The recursive partitioning trees algorithm scored a 96.6% accuracy without rounding within the boundary adjustments.


```{r}
head(DF_rpart)
```

Lets correct the boundaries and round within the boundaries to see if there are any improvements.
```{r}
DF_rpart$roundedPrediction <- ifelse(DF_rpart$predrpart<0,0,
                            ifelse(DF_rpart$predrpart>3,3,
                                   round(DF_rpart$predrpart,0)))
DF_rpart$Correct <- ifelse(DF_rpart$ActualHitsLanded==DF_rpart$roundedPrediction,1,0)
accuracy5 <- sum(DF_rpart$Correct)/length(DF_rpart$Correct)
accuracy5
```
When adjusting the boundaries and rounding within those boundaries, the improvements were significant for the rpart alogorithm as the accuracy in prediction was 100%.


```{r}
head(DF_rpart)
```

Lets now use the gradient boosted model and compare results.
```{r, error=FALSE, message=FALSE, warning=FALSE}
gbmMod <- train(TotLandsX1 ~ .,
                method='gbm', data=trainingSet)
```


```{r, error=FALSE, warning=FALSE, message=FALSE}
predgbm <- predict(gbmMod, testingSet)

DF_gbm <- data.frame(predgbm, ActualHitsLanded=testingSet$TotLandsX1)

length_gbm <- length(DF_gbm$ActualHitsLanded)

sum_gbm <- sum(DF_gbm$predgbm==DF_gbm$ActualHitsLanded)

accgbm <- (sum_gbm/length_gbm)

accgbm
```

```{r}
head(DF_gbm)
```


```{r}
DF_gbm$roundedPrediction <- ifelse(DF_gbm$predgbm<0,0,
                            ifelse(DF_gbm$predgbm>3,3,
                                   round(DF_gbm$predgbm,0)))
DF_gbm$Correct <- ifelse(DF_gbm$ActualHitsLanded==DF_gbm$roundedPrediction,1,0)
accuracy6 <- sum(DF_gbm$Correct)/length(DF_gbm$Correct)
accuracy6
```

The gbm model scored 96.6% accuracy after adjusting the boundaries and rounding within those boundaries.

```{r}
head(DF_gbm)
```


That is it for the machine learning using the caret package of algorithms in R. Next we will compare python's random forest, gradient boosted models, naive bayes, and convolutional neural network and deep neural networks in predicting hits landed using the reticulate package in R to run python code.

***
***
***

# Machine Learning with Python


```{r}
library(reticulate)
```
```{r}
conda_list(conda="auto")
```



```{r}
use_condaenv(condaenv = "python36")
```


```{python}
import pandas as pd 
import matplotlib.pyplot as plt 
from textblob import TextBlob 
import sklearn 
import numpy as np 
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer 
from sklearn.naive_bayes import MultinomialNB 
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

import re
import string
import nltk 

np.random.seed(47) 
```


Import our ML ready file to run the python algorithms on with sklearn.
```{python}
Amanda = pd.read_csv('Amanda_ml_ready.csv', encoding = 'unicode_escape') 
print(Amanda.shape)
```


```{python}
print(Amanda.columns)
```

```{python}
print(Amanda.head())
```

```{python}
print(Amanda.tail())
```

```{python}
print(Amanda['TotLandsX1'].unique())
```
There are 4 classes to classify our hits landed in our testing set from the model we build.


Reorder the rows as instances randomised to split into train and test sets of the data.
```{python}
import numpy as np

Amanda = Amanda.reindex(np.random.permutation(Amanda.index))

print(Amanda.head())
```

```{python}
print(Amanda.tail())
```


There are 393 instances in this data, and 80% is about 314, the target is the hits landed by X1.
```{python}
# Split/splice into training ~ 80% and testing ~ 20%
Amanda_train = Amanda[:314]
Amanda_test = Amanda[314:]
Amanda_hits_train = Amanda['TotLandsX1'][:314]
Amanda_hits_test = Amanda['TotLandsX1'][314:]

print(Amanda_train.shape)
print(Amanda_test.shape)
```
```{python}
Amanda_train['TotLandsX1'].unique()
Amanda_test['TotLandsX1'].unique()
```
Only the training set has a 3 class, and without the testing set having a 3 class, the confusion matrix will only show the values 0,1, and 2 when analyzing the results of actual versus predicted values.


# Sklearn's Multinomial Naive Bayes classifier
```{python}
mnb_Fit = MultinomialNB().fit(Amanda_train, Amanda_hits_train)
```

```{python}
predictions = mnb_Fit.predict(Amanda_test)

prd = pd.DataFrame(predictions)
prd.columns=['predictions']
prd.index=Amanda_hits_test.index
pred=pd.concat([pd.DataFrame(prd),Amanda_hits_test],axis=1)
print(pred)
```
```{python}
pred['predictions'].unique()
pred['TotLandsX1'].unique()
```
The 3 isn't in the predictions nor the test set. 

```{python}
print('accuracy', accuracy_score(Amanda_hits_test, predictions))
```


```{python}
print('confusion matrix')
print('rows=expected, cols=predicted')
print(confusion_matrix(Amanda_hits_test, predictions))
```
We can see no 0s were classified from the 1s or 2s giving 100% precision for the zeros, but 4 1s and 27 2s were misclassified as 0s, making the recall `r 32/(32+4+27)` or 50.8% for the zeros. And there were 6 1s correctly identified, but 6 2s were misclassified as 1s for a 6/12 % recall on 1s, and 4 0s were misclassified as 1s as well as 2 2s for a precision of 6/(6+4+2) % precision on 1s. For the 2s, there were 2 correctly classified as a 2, but 2 1s were misclassified as a 2, and 6 1s misclassified as 2s and 27 0s misclassified as 2s. Overall the multinomial naive bayes classifier scored a 50.6% accuracy in prediction of hits landed for Amanda.

# Sklearn's Random Forest and Gradient Boosting Classifier results

```{python}

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import precision_recall_fscore_support as score
import time

```

# Random Forest Classifier
```{python}
rf=RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)
start=time.time()
rf_model=rf.fit(Amanda_train,Amanda_hits_train)
end=time.time()
fit_time=(end-start)
fit_time
```


```{python}
start=time.time()
y_pred=rf_model.predict(Amanda_test)
end=time.time()
pred_time=(end-start)
pred_time
```


```{python}
prd = pd.DataFrame(y_pred)
prd.columns=['Predicted']

prd.index=Amanda_hits_test.index
pred=pd.concat([pd.DataFrame(prd),Amanda_hits_test],axis=1)
print(pred)
```


```{python}
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

print('accuracy', accuracy_score(Amanda_hits_test, y_pred))

print('confusion matrix')
print(confusion_matrix(Amanda_hits_test, y_pred))
```
The random forest classifier scored a 96.2% accuracy overall in prediction of hits landed, with almost 100% precision or 63/64 on the 0s, 100% recall on 0s, 11/13 on precision of 1s, and 11/12 recall on 1s, 100% precision on 2s, and 2/4 recall on 2s. The amount of time to predict was also fast as the model ran in parallel in building the trees.

# Gradient Boosted Model
```{python, echo=FALSE}
gb=GradientBoostingClassifier(n_estimators=150,max_depth=11)
start=time.time()
gb_model=gb.fit(Amanda_train,Amanda_hits_train)
end=time.time()
fit_time=(end-start)
fit_time
```


```{python}
start=time.time()
y_pred=gb_model.predict(Amanda_test)
end=time.time()
pred_time=(end-start)
pred_time
```


```{python}
prd = pd.DataFrame(y_pred)
prd.columns=['Predicted']

prd.index=Amanda_hits_test.index
pred=pd.concat([pd.DataFrame(prd),Amanda_hits_test],axis=1)
print(pred)
```


```{python}
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

print('accuracy', accuracy_score(Amanda_hits_test, y_pred))

print('confusion matrix')
print(confusion_matrix(Amanda_hits_test, y_pred))
```
The gradient boosting classifier scored 100% accuracy with 100% precision and 100% recall on all classes listed. Note that because there was only 1 instance of the 3 class in the training set, it wasn't enough data to predict any instances in the testing set as a 3, so that class was not part of the prediction. Lets see if there is any difference with using Convolutional Neural Networks (CNNs) AKA Deep NNs or DNNs.

# Convolutional Neural Networks
```{python}
import numpy as np


```


```{python}
Amanda2 = Amanda
Amanda2
```


```{python}
class_mapping = {label: idx for idx, label in enumerate(np.unique(Amanda2['TotLandsX1']))}
class_mapping
```


```{python}
Amanda_hits_test = pd.DataFrame(Amanda_hits_test)
Amanda_hits_test.columns=['TotLandsX1']
Amanda_hits_test.columns
```

```{python}
Amanda_hits_test['TotLandsX1']=Amanda_hits_test['TotLandsX1'].map(class_mapping)
Amanda_hits_test.head()
```



```{python}
Amanda_hits_train=pd.DataFrame(Amanda_hits_train)
Amanda_hits_train.columns=['TotLandsX1']
Amanda_hits_train.columns
```


```{python}
Amanda_hits_train['TotLandsX1']=Amanda_hits_train['TotLandsX1'].map(class_mapping)
Amanda_hits_train.head()

```


```{python}
Amanda_train.shape

```


RStudio isn't recognizing the module 'tensorflow' in my python environment named python36. This portion of the neural nets will stop here, because tensorflow isn't being recognized, even though 'pip list' in the conda environment says it is a module available and my modules. Reinstalling tensorflow had no change to this error. We aren't really using tensorflow, but keras. I switched to Keras, but had to change the user 'Documents' folder of '.keras' in the text file to 'theano' instead of 'tensorflow' to use this version of Keras instead of the TensorFlow version. Change this back if using tensorflow within Jupyter Notebook as it has worked there without problems. This is a recent problem that wasn't an issue 2 weeks ago. I also installed Tableau Public Desktop recently that could have interfered with tensorflow. Keras isn't working either inside RStudio using reticulate as a wrapper for python. The message said to install a compiler and it was done, but still not working. 

```{python}
import tensorflow as tf
import tensorflow.contrib.keras as keras

#optionally use import tensorflow.keras as keras when no longer experimental contributor package development

#import keras

np.random.seed(123)
tf.set_random_seed(123)
```


```{python}

model6 = keras.models.Sequential()

model6.add(
    keras.layers.Dense(
        units=200,   #output units need to match next layer inputs 
        input_dim=93, #number of features for input above says 93
        kernel_initializer='glorot_uniform',# name of the guy behind Xavier Initialization; the biases to zero
        bias_initializer='zeros',
        activation='tanh'))

model6.add(
    keras.layers.Dense(
        units=100,   #output matches next layer input 
        input_dim=200, #input matches last layer's output
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='tanh'))

model6.add(
    keras.layers.Dense(
        units=4,  #these are the number of class categories in our target  
        input_dim=100,
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='softmax'))#will return the class membership probs summing to 1 of all class probs

# these are hyperparameters that can be tuned if overfitting during training, or to get better accuracy
sgd_optimizer = keras.optimizers.SGD( 
        lr=0.001, decay=1e-7, momentum=.8)

# categorical_crossentropy is used in multiclass classification instead of binary_crossentropy
# to match the softmax function
model6.compile(optimizer=sgd_optimizer,
              loss='sparse_categorical_crossentropy')
# it was 'categorical_crossentropy', but that expects binary matrices of 1s and 0s
# it said to use sparse_categorical_crossentropy
```

```{python}
model6
```

```{python}
import time
start=time.time()

history6 = model6.fit(Amanda_train, Amanda_hits_train,
                    batch_size=64, epochs=50,
                    verbose=1, 
                    validation_split=0.15) 
end=time.time()
fit_time=(end-start)
print(start,end,fit_time)
```


```{python}
y_train_pred6 = model6.predict_classes(Amanda_train, verbose=0)
print('First 3 predictions: ', y_train_pred6[:3])
```


```{python}
y_train_pred6 = model6.predict_classes(Amanda_train, 
                                     verbose=0)
```


```{python}
y_train_pred6 = pd.DataFrame(y_train_pred6)
y_train_pred6.columns=['predicted']

y_train6 = Amanda_hits_train
y_train6 = pd.DataFrame(y_train6)
y_train6.columns=['ActualHitsLandedbyX1']

y_train_pred6.index=y_train6.index

Train6=pd.concat([y_train6['ActualHitsLandedbyX1'],y_train_pred6['predicted']],axis=1)

print(Train6)
```


```{python}
y_test_pred6 = model6.predict_classes(Amanda_test, 
                                    verbose=0)
```


```{python}
y_test_pred6 = pd.DataFrame(y_test_pred6)
y_test_pred6.columns=['predicted']

y_test6 = Amanda_hits_test
y_test6 = pd.DataFrame(y_test6)
y_test6.columns=['ActualHitsLandedbyX1']

y_test_pred6.index=y_test6.index

Test6=pd.concat([y_test6['ActualHitsLandedbyX1'],y_test_pred6['predicted']],axis=1)

print(Test6)
```

```{python}
s = sum(Train6['ActualHitsLandedbyX1']==Train6['predicted'])
l = len(Train6['ActualHitsLandedbyX1'])
accTrain6 = s/l
print('Training Correctly Predicted:',s,'Training Accuracy:',accTrain6,'\n')
```

```{python}
print(confusion_matrix(Amanda_hits_train, y_train_pred6))

```
From the confusion matrix above, the rows=expected or actual, and columns are the predicted value. There were 5 zeros misclassified as 1s, 35 1s misclassified as 0s, 9 2s misclassified as 0s, and the only 3 was misclassified as 0. The precision for 0 is 257/(257+35+9+1) and the recall for 0s is 257/(257+5). The precision for 1 is 7/(7+5) and recall for 1 is 7/(7+35). The precision for 2 is 0/0 or NAN and recall for 2 is 0/9. The recall for 3 is 0/1 and the precision for 3 is 0/0 or NAN. At least the CNN accounted for the type 3 class.

```{python}
s = sum(Test6['ActualHitsLandedbyX1']==Test6['predicted'])
l = len(Test6['ActualHitsLandedbyX1'])
accTest6 = s/l
print('Testing Correctly Predicted:',s,'Testing Accuracy:',accTest6)
```

```{python}
print(confusion_matrix(Amanda_hits_test, y_test_pred6))

```
Since there weren't any 3 classes in the testing set, the confusion matrix shows there is also no expected 3 or predicted 3. In the test set, the class 2 was expected to have 4 but they were misclassified as 3 class 0s and 1 class 1. There were expected to be 12 class 1s, but they were all misclassified as class 0. For class 0, only 1 was misclassified as a class 1. 

In accuracy of prediction for this data, the CNN algorithm takes longer to build, train, and is less accurate in prediction than all algorithms from the R and Python available modules and packages. But CNNs did show the class 3 in training. And we must also note, that the added wrestling features for this data could have greatly skewed the results because by not having that information or having it a zero when in fact the other 3 fighters did have ground work or wrestling with Amanda. We could retry this CNN but only using the data for Amanda's fight with Germaine. We excluded the notes field with Germaine to extract that subset when reading in the data. But we know it can be found by extracting it within R and using those indices before permutating the indices and partitioning the testing and training sets.
```{r}
AmandaWrestling <- Table10[Table10$Notes=='Germaine',]
dim(AmandaWrestling)
```
```{r}
head(AmandaWrestling[1:5,20:22])
```

```{python}
Amanda3 = pd.read_csv('Amanda_ml_ready.csv', encoding = 'unicode_escape') 
print(Amanda.shape)
```
```{python}
Amanda4 = Amanda3[:231]
Amanda4.shape
```


```{python}
import numpy as np

Amanda4 = Amanda4.reindex(np.random.permutation(Amanda4.index))

print(Amanda4.head())
```

Split/splice into training ~ 80% and testing ~ 20%. With 231 samples, 80% is about 184.
```{python}
Amanda_train = Amanda4[:184]
Amanda_test = Amanda4[184:]
Amanda_hits_train = Amanda4['TotLandsX1'][:184]
Amanda_hits_test = Amanda4['TotLandsX1'][184:]

print(Amanda_train.shape)
print(Amanda_test.shape)
```

Now we can run our CNN on this data that isn't missing information.

```{python}
Amanda_train['TotLandsX1'].unique()
Amanda_test['TotLandsX1'].unique()
```

Aside: The fight with Germaine is the fight with 3 hits landed by Amanda. This was when on the ground, it looked like a possible TKO or technical knockout, but the referee didn't call it. Many hits were being landed by Amanda. She was hit in the face with a high knee in the stand up part of this round before going to the ground. Germaine stayed in the round and fight longer by having the open guard that she tried using her feet to kick off Amanda who pushed her legs aside and was able to land some hits, many hits in fact. 


```{python}
Amanda_hits_train=pd.DataFrame(Amanda_hits_train)
Amanda_hits_train.columns=['TotLandsX1']
Amanda_hits_train.columns
```
```{python}
class_mapping = {label: idx for idx, label in enumerate(np.unique(Amanda4['TotLandsX1']))}
class_mapping
```

```{python}
Amanda_hits_train['TotLandsX1']=Amanda_hits_train['TotLandsX1'].map(class_mapping)
Amanda_hits_train.head()

```


```{python}
Amanda_train.shape

```



```{python}
import tensorflow as tf
import tensorflow.contrib.keras as keras

#optionally use import tensorflow.keras as keras when no longer experimental contributor package development

#import keras

np.random.seed(123)
tf.set_random_seed(123)
```


```{python}

model6 = keras.models.Sequential()

model6.add(
    keras.layers.Dense(
        units=200,   #output units need to match next layer inputs 
        input_dim=93, #number of features for input above says 93
        kernel_initializer='glorot_uniform',# name of the guy behind Xavier Initialization; the biases to zero
        bias_initializer='zeros',
        activation='tanh'))

model6.add(
    keras.layers.Dense(
        units=100,   #output matches next layer input 
        input_dim=200, #input matches last layer's output
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='tanh'))

model6.add(
    keras.layers.Dense(
        units=4,  #these are the number of class categories in our target  
        input_dim=100,
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='softmax'))#will return the class membership probs summing to 1 of all class probs

# these are hyperparameters that can be tuned if overfitting during training, or to get better accuracy
sgd_optimizer = keras.optimizers.SGD( 
        lr=0.001, decay=1e-7, momentum=.8)

# categorical_crossentropy is used in multiclass classification instead of binary_crossentropy
# to match the softmax function
model6.compile(optimizer=sgd_optimizer,
              loss='sparse_categorical_crossentropy')
# it was 'categorical_crossentropy', but that expects binary matrices of 1s and 0s
# it said to use sparse_categorical_crossentropy
```

```{python}
model6
```

```{python}
import time
start=time.time()

history6 = model6.fit(Amanda_train, Amanda_hits_train,
                    batch_size=64, epochs=50,
                    verbose=1, 
                    validation_split=0.15) 
end=time.time()
fit_time=(end-start)
print(start,end,fit_time)
```


```{python}
y_train_pred6 = model6.predict_classes(Amanda_train, verbose=0)
print('First 3 predictions: ', y_train_pred6[:3])
```


```{python}
y_train_pred6 = model6.predict_classes(Amanda_train, 
                                     verbose=0)
```


```{python}
y_train_pred6 = pd.DataFrame(y_train_pred6)
y_train_pred6.columns=['predicted']

y_train6 = Amanda_hits_train
y_train6 = pd.DataFrame(y_train6)
y_train6.columns=['ActualHitsLandedbyX1']

y_train_pred6.index=y_train6.index

Train6=pd.concat([y_train6['ActualHitsLandedbyX1'],y_train_pred6['predicted']],axis=1)

print(Train6)
```



```{python}
y_test_pred6 = model6.predict_classes(Amanda_test, 
                                    verbose=0)
```


```{python}
y_test_pred6 = pd.DataFrame(y_test_pred6)
y_test_pred6.columns=['predicted']

y_test6 = Amanda_hits_test
y_test6 = pd.DataFrame(y_test6)
y_test6.columns=['ActualHitsLandedbyX1']

y_test_pred6.index=y_test6.index

Test6=pd.concat([y_test6['ActualHitsLandedbyX1'],y_test_pred6['predicted']],axis=1)

print(Test6)
```

```{python}
s = sum(Train6['ActualHitsLandedbyX1']==Train6['predicted'])
l = len(Train6['ActualHitsLandedbyX1'])
accTrain6 = s/l
print('Training Correctly Predicted:',s,'Training Accuracy:',accTrain6,'\n')
```

```{python}
print(confusion_matrix(Amanda_hits_train, y_train_pred6))

```


```{python}
s = sum(Test6['ActualHitsLandedbyX1']==Test6['predicted'])
l = len(Test6['ActualHitsLandedbyX1'])
accTest6 = s/l
print('Testing Correctly Predicted:',s,'Testing Accuracy:',accTest6)
```

```{python}
print(confusion_matrix(Amanda_hits_test, y_test_pred6))

```

When using the wrestling set applicable to only the fighter whose ground or wrestling features were accounted for, the testing accuracy improved from 72% to 85% in predicting hits landed using the DNN or CNN alternatively named so.

In summary the best models were those that scored 100% accuracy in Python's TensorFlow Keras version for the gradient boosting classifier. In R the best models were the modified boundaries that were rounded to nearest class numeric value for hits landed by X1 on X2. Those R modules scored 100% for random forest, generalized linear models, and recursive partitioned trees. The gradient boosted model in R scored 96%. The caveat is that class 3 in our data only occured 1 time in all and was captured in the CNN model but not in our models that scored 100% in either scripting language. However, the recall and precision were 100% accurate for that class 3 in those models. 

***
***
***

# Recursive Neural Networks

Lets now do some predictive text analysis of this fighter Nunez that will use the notes of Nunez's four fights with NAs removed to fit the format from a script in Sebastian Raschka's 'Python: Machine Learning' book available for Kindle. One will work similar to the CNN by predicting the output of hits landed based on the actions only, and the other will use a text version of all actions to predict the probability of characters next in a sequence for some forecasting of possible actions that Nunez will make based on her sampled fights.

# Recursive Neural Networks (RNNs)

Lets first prepare the data.
```{r}
Amanda4 <- read.csv('Nunez4fights_addedFeatures.csv', sep=',', header=TRUE, na.strings=c('',' ','NA'))
Amanda4RNN <- Amanda4[,c(18,8)]
colnames(Amanda4RNN) <- c('review','sentiment')
Amanda_RNN <- Amanda4RNN[complete.cases(Amanda4RNN$review),]
head(Amanda_RNN)
dim(Amanda_RNN)
```

Our table for the RNN to predict the hits landed or 'sentiment' to match this script of Raschka's will be the Amands_RNN table. Lets make the string character of all the 'review' observations which are our fighter actions/reactions for Nunez also changed to match this script into one long string for the predictive test script after we train this RNN model. This will be set to eval=FALSE in Rmarkdown for the chunk setting so that it only prints once.
```{r, eval=FALSE}
for (i in Amanda_RNN$review){
  
  catAmanda <- cat(i, file='amandaRNN.txt', sep=' ', fill=TRUE, append=TRUE)
}

write.csv(Amanda_RNN,'amanda_RNN.csv', row.names=FALSE)
```
The above will create our file to read in when we get to the predictive text script, 'amandaRNN.txt.'

```{r}
library(reticulate)
```

```{r}
conda_list(conda = "auto") 
```

```{r}
use_condaenv(condaenv = "python36")
```

We are using the same Felicia Spencer data file, but changed the name of TotLandsX1 to sentiment, and the name of the FighterActionsReactionsX1 to review. This file is also in github, modified outside of R and Python as '2columnsSpencer.csv' and the NAs were also removed.
```{python}
import pyprind
import pandas as pd
from string import punctuation
import re
import numpy as np


df = pd.read_csv('amanda_RNN.csv', encoding='utf-8')
print(df.head(10))
```


```{python}
df.shape
```

```{python}
type(df['review'])
```

```{python}
## Separate words and 
## count each word's occurrence


from collections import Counter


counts = Counter()
pbar = pyprind.ProgBar(len(df['review']),
                       title='Counting words occurences')
for i,review in enumerate(df['review']):
    text = ''.join([c if c not in punctuation else ' '+c+' ' \
                    for c in review]).lower()
    df.loc[i,'review'] = text
    pbar.update()
    counts.update(text.split())
```

```{python}
## Create a mapping:
## Map each unique word to an integer

word_counts = sorted(counts, key=counts.get, reverse=True)
print(word_counts[:5])
word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}


mapped_reviews = []
pbar = pyprind.ProgBar(len(df['review']),
                       title='Map reviews to ints')
for review in df['review']:
    mapped_reviews.append([word_to_int[word] for word in review.split()])
    pbar.update()
```

```{python}
len(mapped_reviews)
```

```{python}
sequence_length = 200  ## sequence length (or T in our formulas)
sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)
for i, row in enumerate(mapped_reviews):
    review_arr = np.array(row)
    sequences[i, -len(row):] = review_arr[-sequence_length:]
```

```{python}
sequences.shape
```

```{python}
## Define fixed-length sequences:
## Use the last 200 elements of each sequence
## if sequence length < 200: left-pad with zeros

sequence_length = 200  ## sequence length (or T in our formulas)
sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)
for i, row in enumerate(mapped_reviews):
    review_arr = np.array(row)
    sequences[i, -len(row):] = review_arr[-sequence_length:]

X_train = sequences[:270, :]
y_train = df.loc[:270, 'sentiment'].values
X_test = sequences[270:, :] 
#the number of samples has to be divisible exactly by batch size or predictions will be cut off, if test #samples are 147 and batch size 15, the predictions will only have 135 samples
y_test = df.loc[270:, 'sentiment'].values


np.random.seed(123) # for reproducibility

## Function to generate minibatches:
def create_batch_generator(x, y=None, batch_size=64):
    n_batches = len(x)//batch_size
    x= x[:n_batches*batch_size]
    if y is not None:
        y = y[:n_batches*batch_size]
    for ii in range(0, len(x), batch_size):
        if y is not None:
            yield x[ii:ii+batch_size], y[ii:ii+batch_size]
        else:
            yield x[ii:ii+batch_size]
```


```{python}
import tensorflow as tf


class SentimentRNN(object):
    def __init__(self, n_words, seq_len=200,
                 lstm_size=256, num_layers=1, batch_size=64,
                 learning_rate=0.0001, embed_size=200):
        self.n_words = n_words
        self.seq_len = seq_len
        self.lstm_size = lstm_size   ## number of hidden units
        self.num_layers = num_layers
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.embed_size = embed_size

        self.g = tf.Graph()
        with self.g.as_default():
            tf.set_random_seed(123)
            self.build()
            self.saver = tf.train.Saver()
            self.init_op = tf.global_variables_initializer()

    def build(self):
        ## Define the placeholders
        tf_x = tf.placeholder(tf.int32,
                    shape=(self.batch_size, self.seq_len),
                    name='tf_x')
        tf_y = tf.placeholder(tf.float32,
                    shape=(self.batch_size),
                    name='tf_y')
        tf_keepprob = tf.placeholder(tf.float32,
                    name='tf_keepprob')
        ## Create the embedding layer
        embedding = tf.Variable(
                    tf.random_uniform(
                        (self.n_words, self.embed_size),
                        minval=-1, maxval=1),
                    name='embedding')
        embed_x = tf.nn.embedding_lookup(
                    embedding, tf_x, 
                    name='embeded_x')

        ## Define LSTM cell and stack them together
        cells = tf.contrib.rnn.MultiRNNCell(
                [tf.contrib.rnn.DropoutWrapper(
                   tf.contrib.rnn.BasicLSTMCell(self.lstm_size),
                   output_keep_prob=tf_keepprob)
                 for i in range(self.num_layers)])

        ## Define the initial state:
        self.initial_state = cells.zero_state(
                 self.batch_size, tf.float32)
        print('  << initial state >> ', self.initial_state)

        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(
                 cells, embed_x,
                 initial_state=self.initial_state)
        ## Note: lstm_outputs shape: 
        ##  [batch_size, max_time, cells.output_size]
        print('\n  << lstm_output   >> ', lstm_outputs)
        print('\n  << final state   >> ', self.final_state)

        ## Apply a FC layer after on top of RNN output:
        logits = tf.layers.dense(
                 inputs=lstm_outputs[:, -1],
                 units=1, activation=None,
                 name='logits')
        
        logits = tf.squeeze(logits, name='logits_squeezed')
        print ('\n  << logits        >> ', logits)
        
        y_proba = tf.nn.sigmoid(logits, name='probabilities')
        predictions = {
            'probabilities': y_proba,
            'labels' : tf.cast(tf.round(y_proba), tf.int32,
                 name='labels')
        }
        print('\n  << predictions   >> ', predictions)

        ## Define the cost function
        cost = tf.reduce_mean(
                 tf.nn.sigmoid_cross_entropy_with_logits(
                 labels=tf_y, logits=logits),
                 name='cost')
        
        ## Define the optimizer
        optimizer = tf.train.AdamOptimizer(self.learning_rate)
        train_op = optimizer.minimize(cost, name='train_op')

    def train(self, X_train, y_train, num_epochs):
        with tf.Session(graph=self.g) as sess:
            sess.run(self.init_op)
            iteration = 1
            for epoch in range(num_epochs):
                state = sess.run(self.initial_state)
                
                for batch_x, batch_y in create_batch_generator(
                            X_train, y_train, self.batch_size):
                    feed = {'tf_x:0': batch_x,
                            'tf_y:0': batch_y,
                            'tf_keepprob:0': 0.5,
                            self.initial_state : state}
                    loss, _, state = sess.run(
                            ['cost:0', 'train_op', 
                             self.final_state],
                            feed_dict=feed)

                    if iteration % 20 == 0:
                        print("Epoch: %d/%d Iteration: %d "
                              "| Train loss: %.5f" % (
                               epoch + 1, num_epochs,
                               iteration, loss))

                    iteration +=1
                if (epoch+1)%10 == 0:
                    self.saver.save(sess,
                        "model/sentiment-%d.ckpt" % epoch)

    def predict(self, X_data, return_proba=False):
        preds = []
        with tf.Session(graph = self.g) as sess:
            self.saver.restore(
                sess, tf.train.latest_checkpoint('model/'))
            test_state = sess.run(self.initial_state)
            for ii, batch_x in enumerate(
                create_batch_generator(
                    X_data, None, batch_size=self.batch_size), 1):
                feed = {'tf_x:0' : batch_x,
                        'tf_keepprob:0': 1.0,
                        self.initial_state : test_state}
                if return_proba:
                    pred, test_state = sess.run(
                        ['probabilities:0', self.final_state],
                        feed_dict=feed)
                else:
                    pred, test_state = sess.run(
                        ['labels:0', self.final_state],
                        feed_dict=feed)
                    
                preds.append(pred)
                
        return np.concatenate(preds)
```

```{python}
## Train:

n_words = max(list(word_to_int.values())) + 1

rnn = SentimentRNN(n_words=n_words, 
                   seq_len=sequence_length,
                   embed_size=256, 
                   lstm_size=128, 
                   num_layers=1, 
# batch_size has to divide evenly into the number of testing set samples, or some samples will be removed
                   batch_size=10,
                   learning_rate=0.001)
```

```{python}
rnn.train(X_train, y_train, num_epochs=20)
```

```{python}
X_test.shape
```

```{python}
y_test
```

```{python}
y_test.shape
```

```{python}
X_test
```

```{python}
## Test: 
preds = rnn.predict(X_test)
y_true = y_test[:len(preds)]
print('Test Acc.: %.3f' % (
      np.sum(preds == y_true) / len(y_true)))
```


```{python}
preds
```


```{python}
preds.shape
```


```{python}
prd = pd.DataFrame(preds)
prd.columns=['Predicted']
type(prd)
prd
```

```{python}
true = pd.DataFrame(y_test)
true.columns=['trueValue']
prd.index=true.index
pred=pd.concat([pd.DataFrame(prd),true],axis=1)
print(pred)
```

```{python}
## Get probabilities:
proba = rnn.predict(X_test, return_proba=True)
```

```{python}
proba
```

```{python}
proba.shape
```

***
***
***

This next RNN will make an attempt to predict the text as a limited output based on the actions over sequential data of instances for Felicia Spencer as a text document.
```{python}
import numpy as np


## Reading and processing text
with open('amandaRNN.txt', 'r', encoding='utf-8') as f: 
    text=f.read()
```

```{python}
text
```
```{python}
import re

re.sub('\n', ' ', text, count=0, flags=0)
```

```{python}
chars = set(text)
char2int = {ch:i for i,ch in enumerate(chars)}
int2char = dict(enumerate(chars))
text_ints = np.array([char2int[ch] for ch in text], 
                     dtype=np.int32)
```

```{python}
def reshape_data(sequence, batch_size, num_steps):
    tot_batch_length = batch_size * num_steps
    num_batches = int(len(sequence) / tot_batch_length)
    if num_batches*tot_batch_length + 1 > len(sequence):
        num_batches = num_batches - 1
    ## Truncate the sequence at the end to get rid of 
    ## remaining charcaters that do not make a full batch
    x = sequence[0 : num_batches*tot_batch_length]
    y = sequence[1 : num_batches*tot_batch_length + 1]
    ## Split x & y into a list batches of sequences: 
    x_batch_splits = np.split(x, batch_size)
    y_batch_splits = np.split(y, batch_size)
    ## Stack the batches together
    ## batch_size x tot_batch_length
    x = np.stack(x_batch_splits)
    y = np.stack(y_batch_splits)
    
    return x, y

## Testing:
train_x, train_y = reshape_data(text_ints, 64, 10)
print(train_x.shape)
print(train_x[0, :10])
print(train_y[0, :10])
print(''.join(int2char[i] for i in train_x[0, :50]))
```

```{python}
np.random.seed(123)

def create_batch_generator(data_x, data_y, num_steps):
    batch_size, tot_batch_length = data_x.shape    
    num_batches = int(tot_batch_length/num_steps)
    for b in range(num_batches):
        yield (data_x[:, b*num_steps: (b+1)*num_steps], 
               data_y[:, b*num_steps: (b+1)*num_steps])
        
bgen = create_batch_generator(train_x[:,:100], train_y[:,:100], 15)
for b in bgen:
    print(b[0].shape, b[1].shape, end='  ')
    print(''.join(int2char[i] for i in b[0][0,:]).replace('\n', '*'), '    ',
          ''.join(int2char[i] for i in b[1][0,:]).replace('\n', '*'))
```

```{python}
import tensorflow as tf
import os

class CharRNN(object):
    def __init__(self, num_classes, batch_size=64, 
                 num_steps=100, lstm_size=128, 
                 num_layers=1, learning_rate=0.001, 
                 keep_prob=0.5, grad_clip=5, 
                 sampling=False):
        self.num_classes = num_classes
        self.batch_size = batch_size
        self.num_steps = num_steps
        self.lstm_size = lstm_size
        self.num_layers = num_layers
        self.learning_rate = learning_rate
        self.keep_prob = keep_prob
        self.grad_clip = grad_clip
        
        self.g = tf.Graph()
        with self.g.as_default():
            tf.set_random_seed(123)

            self.build(sampling=sampling)
            self.saver = tf.train.Saver()
            self.init_op = tf.global_variables_initializer()
            
    def build(self, sampling):
        if sampling == True:
            batch_size, num_steps = 1, 1
        else:
            batch_size = self.batch_size
            num_steps = self.num_steps

        tf_x = tf.placeholder(tf.int32, 
                              shape=[batch_size, num_steps], 
                              name='tf_x')
        tf_y = tf.placeholder(tf.int32, 
                              shape=[batch_size, num_steps], 
                              name='tf_y')
        tf_keepprob = tf.placeholder(tf.float32, 
                              name='tf_keepprob')

        # One-hot encoding:
        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)
        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)

        ### Build the multi-layer RNN cells
        cells = tf.contrib.rnn.MultiRNNCell(
            [tf.contrib.rnn.DropoutWrapper(
                tf.contrib.rnn.BasicLSTMCell(self.lstm_size), 
                output_keep_prob=tf_keepprob) 
            for _ in range(self.num_layers)])
        
        ## Define the initial state
        self.initial_state = cells.zero_state(
                    batch_size, tf.float32)

        ## Run each sequence step through the RNN 
        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(
                    cells, x_onehot, 
                    initial_state=self.initial_state)
        
        print('  << lstm_outputs  >>', lstm_outputs)

        seq_output_reshaped = tf.reshape(
                    lstm_outputs, 
                    shape=[-1, self.lstm_size],
                    name='seq_output_reshaped')

        logits = tf.layers.dense(
                    inputs=seq_output_reshaped, 
                    units=self.num_classes,
                    activation=None,
                    name='logits')

        proba = tf.nn.softmax(
                    logits, 
                    name='probabilities')
        print(proba)

        y_reshaped = tf.reshape(
                    y_onehot, 
                    shape=[-1, self.num_classes],
                    name='y_reshaped')
        cost = tf.reduce_mean(
                    tf.nn.softmax_cross_entropy_with_logits(
                        logits=logits, 
                        labels=y_reshaped),
                    name='cost')

        # Gradient clipping to avoid "exploding gradients"
        tvars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(
                    tf.gradients(cost, tvars), 
                    self.grad_clip)
        optimizer = tf.train.AdamOptimizer(self.learning_rate)
        train_op = optimizer.apply_gradients(
                    zip(grads, tvars),
                    name='train_op')
        
    def train(self, train_x, train_y, 
              num_epochs, ckpt_dir='./model/'):
        ## Create the checkpoint directory
        ## if does not exists
        if not os.path.exists(ckpt_dir):
            os.mkdir(ckpt_dir)
            
        with tf.Session(graph=self.g) as sess:
            sess.run(self.init_op)

            n_batches = int(train_x.shape[1]/self.num_steps)
            iterations = n_batches * num_epochs
            for epoch in range(num_epochs):

                # Train network
                new_state = sess.run(self.initial_state)
                loss = 0
                ## Minibatch generator:
                bgen = create_batch_generator(
                        train_x, train_y, self.num_steps)
                for b, (batch_x, batch_y) in enumerate(bgen, 1):
                    iteration = epoch*n_batches + b
                    
                    feed = {'tf_x:0': batch_x,
                            'tf_y:0': batch_y,
                            'tf_keepprob:0': self.keep_prob,
                            self.initial_state : new_state}
                    batch_cost, _, new_state = sess.run(
                            ['cost:0', 'train_op', 
                                self.final_state],
                            feed_dict=feed)
                    if iteration % 10 == 0:
                        print('Epoch %d/%d Iteration %d'
                              '| Training loss: %.4f' % (
                              epoch + 1, num_epochs, 
                              iteration, batch_cost))

                ## Save the trained model    
                self.saver.save(
                        sess, os.path.join(
                            ckpt_dir, 'language_modeling.ckpt'))
                              
                              
                
    def sample(self, output_length, 
               ckpt_dir, starter_seq="The "):
        observed_seq = [ch for ch in starter_seq]        
        with tf.Session(graph=self.g) as sess:
            self.saver.restore(
                sess, 
                tf.train.latest_checkpoint(ckpt_dir))
            ## 1: run the model using the starter sequence
            new_state = sess.run(self.initial_state)
            for ch in starter_seq:
                x = np.zeros((1, 1))
                x[0,0] = char2int[ch]
                feed = {'tf_x:0': x,
                        'tf_keepprob:0': 1.0,
                        self.initial_state: new_state}
                proba, new_state = sess.run(
                        ['probabilities:0', self.final_state], 
                        feed_dict=feed)

            ch_id = get_top_char(proba, len(chars))
            observed_seq.append(int2char[ch_id])
            
            ## 2: run the model using the updated observed_seq
            for i in range(output_length):
                x[0,0] = ch_id
                feed = {'tf_x:0': x,
                        'tf_keepprob:0': 1.0,
                        self.initial_state: new_state}
                proba, new_state = sess.run(
                        ['probabilities:0', self.final_state], 
                        feed_dict=feed)

                ch_id = get_top_char(proba, len(chars))
                observed_seq.append(int2char[ch_id])

        return ''.join(observed_seq)
```

```{python}
def get_top_char(probas, char_size, top_n=5):
    p = np.squeeze(probas)
    p[np.argsort(p)[:-top_n]] = 0.0
    p = p / np.sum(p)
    ch_id = np.random.choice(char_size, 1, p=p)[0]
    return ch_id
```

```{python}
batch_size = 64
num_steps = 100 
train_x, train_y = reshape_data(text_ints, 
                                batch_size, 
                                num_steps)

rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)
rnn.train(train_x, train_y, 
          num_epochs=100,
          #if you don't have this checkpoint directory, it is made to store results
          ckpt_dir='./model-100/') 
```

```{python}
np.random.seed(123)
rnn = CharRNN(len(chars), sampling=True)

print(rnn.sample(ckpt_dir='./model-100/', 
                 output_length=500))
```

```{python}
## run for 200 epochs
batch_size = 64
num_steps = 100 

rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)
rnn.train(train_x, train_y, 
          num_epochs=200,
          ckpt_dir='./model-200/')
```

```{python}
del rnn

np.random.seed(123)
rnn = CharRNN(len(chars), sampling=True)
print(rnn.sample(ckpt_dir='./model-200/', 
                 output_length=500))
```

