---
title: "UFC ML actions and reactions females: Felicia Spencer"
author: "Janis Corona"
date: "5/18/2020-5/21/2020"
output: html_document
---

This script builds on top of a previous script to extract actions and reactions from the notes on fighter X1 and X2 in the UFC. These extracted features are used as dummy variables to predict the outcome of a landed hit or other selected target variable out of the given features and added features using machine learning on these numeric values. This takes the first round of three separate fights for **Felicia Spencer** with **Zarah, Anderson, and Cyborg**. 

Github files [repository](https://github.com/JanJanJan2018/UFC-ML-for-June-2020)

These are the actions extracting and a brief description:

- **cross**: punch/strike/overhead hit/etc to hit with the top four knuckles of hand upon opponent's body or more likely face or side of the head, this could mean right cross or left cross, even when in the mount position of ground work the fighter could choose to hit opponent with a cross instead of a hammer or hook or upper cut punch, usually lifting the hand to add in gravity and acceleration with own strength and downward body movement intending to knock out, add to get opponent to release grip if in a lock, or to damage the face or cause pain

- **jab**: This is done normally when breaking the ice or trying to open up opponent, it is not intended to be a power shot or heavy hit, usually used repetitively or to stun before delivering a cross which is a heavier hit powered from momentum added from the hips etc. Typically done in the stand up dance when testing out opponent when neither is getting too close for combat

- **hammer**: This is a strike typically done in a ground in pound mount position, but can be while in the mount on top or on the bottom loosening a choke hold. The hit is done to smash and stun the face of opponent, and typically not a favorite, because it can hurt the fighter as much as the opponent due to the part of the hand being used. This type of strike is used for fast repetitive bursts to stun and get out of a lock when on the floor striking or preventing a choke or joint lock. The side of the fist used is the pinky side in alignment with the elbow. Can be lifted above the head to get more power in accelerating with gravity and body weight to hit opponent's face. This is why some fighters hit with a regular jab or cross instead to lessen damage to wrist and hand while stiking opponent. All strikes cause damage, but this is not likely to knock an opponent out, also the hand can be thrown off side to make the person get off balance and lose body control in the ground fight. The male fighters use this more than the females.

- **hook**: This is a heavy hit or used in combination with other strikes like the jab or cross, because if the first hit, opponent can just back away unless their against the cage or ropes. It stuns and can knock the breath out of opponent, adds to chances of getting a TKO or technical knock out or knockout. Its a sneaky punch delivered when opponents arms are up or down blocking head shots and/or body kicks. 

- **upper**: This is a puch delivered from upward momentum to strike the chin when up close in a clinch or hold of some sort. Not a powerful punch compared to other strikes, used to stun and to set up for other heavy hits or barrages of various strikes at once.

- **kick**: The push kick is the most powerful to move opponent off balance with little effort, but the muay thai kick (mt abrev.) is a very powerful kick if your skilled in the pivot and trained your shins to deliver bone to bone or head kicks with its snapping movement. It breaks down the balance and you see fighters deliver this to the lead leg to knock the opponent off balance and prevent leverage needed for heavy hits or crosses, it can also make the opponent stumble if they get their lead leg kicked hard enough to knock out of stance. Some go for the inner leg as well to knock off balance. This kick is also shown aimed at the outer lead leg Iliotibial or IT band to cause pain and test out their kick to lead up to a head kick or the opponent's defenses. Usually some punches or an attempt at a takedown of opponent occurs, unless they step backwards.

- **elbow**: This is a strike used standing up at close range, or in a ground and pound full mount position to drop with gravity while aiming the elbow towards the opponents face, not sure if the target is the forehead, but most opponents getting hit with the elbow end up with cuts/slashes/gashes on their forehead. This could be a technical knockout or some sort of stoppage if the opponent can't fight because blood gets in their eyes, and a doctor dismisses the fight due to inability to continue from injury. When cut, blood drips into the eyes. And it is said can cause blindness if the calcium hardens from the blood in the eye.

- **takedown**: This happens when the fighter thinks they want to either body slam the opponent or have a better chance at ground and pound and a chance at mounting opponent once taken down, or will out wrestle the opponent with a joint lock or choke. Wrestling may look less violent, but wrestling can leave joints in repair for 6-12 months in rehab, break ribs from the chest locks, and make the opponent lose oxygen with neck holds and pass out. Usually fighters tap out if they think they will pass out, and would rather tap out than get choked out or know they are not a strong enough wrestler to break apart the hold. Once it is locked, it can be held for many minutes until the fight round ends, but usually skilled fighters will find other ways to disarm their opponent with wrestling, hits, body shifting, etc.

- **knees**: These are used in clinches or holds of some sort, and can be aimed with high knees, jumping knees, knees striking the IT bands, or the abdomen, to stun and loosen grip to gain control. They can be very powerful if used right. Mazvidal knocked out a guy in under 5 seconds with a flying knee to the forehead. That was the only time I recognized seeing a knockout from a knee. It can cut the face and make the nose or lips bleed if hit with the knee.

We are going to run this script first, but the notes do keep track of which arm was used when making a cross, knee, or other strike other than takedown. Left is L and right is R before the name of the action/strike used and after either lands or misses.

As the extracted actions are listed, there is not an extracted description of which arm was used for the strike or action. There are also some types of wrestling moves noted in the notes, but the entirety of the observations didn't continually list the hold if still in a specific hold that second, only any changes observed. We can add in these features at the end.

Here is a brief description pulled from Wikipedia on **BJJ moves** that aren't extracted as features but could be useful:

**Guard**: person dominating on back with legs around opponent's waste, controls top opponent

**half Guard**: person dominating on back controling with one leg of the top opponent preventing the opponent from passing or gaining side control

**open Guard**: variations where person is on back but legs not wrapped around top opponent's waste, preventing the top opponent from striking or passing with feet or shins

**side control**: top person dominates at the side pinning shoulders and/or hips while striking and trying to gain a lock or choke

**full mount**: person on top dominates with legs above person on ground's hips and knees in armpits to prevent bottom opponent from striking or trying to gain control

**back mount**: ankles in thighs of person who is being dominated by person on back to maneuver a choke or lock

Aside: Having experienced this myself I know that there are things that happen in the hold that aren't illustrated to the viewers. Such as, if the person locks there legs around the rib cage, they are squeezing in an attempt to break their ribs or lower their ability to breathe with limited lung cavity movement, and the idea is to cut off the carotid with a foot, ankle arm and your arm when choking until the person taps out of passes out. FYI, experience matters. Having had 3 months training and going up against someone with 2 years experience and a height advantage I did not last longer than 59 seconds in limited mma, where punches to the face are illegal. FYI I was 27 and the opponent was 14 and both of us females at that time (I plan on dying a female with no changes in that feature until that finite point in time). Do not underestimate age. Also, it takes quite a bit of maneuvering to get out of a hold once locked. Also, too much wrestling leaves viewers who want to see hits or strikes and movement on the floor towards the opponent and not dancing around the ring to avoid opponent is how these fighters make a name for themselves. In other words, these fighters have to be to some extent bat crazy.



```{r, error=FALSE, message=FALSE, warning=FALSE}
library(dplyr)

```


```{r}
felicia <- read.csv('Felicia3fights.csv', header=TRUE, sep=',', 
                 na.strings=c('','NA'))

colnames(felicia)

```

Remove the instances with no action from either fighter X1 or X2.
```{r}
Added <- filter(felicia, felicia$FighterActionReactions.X1 !=0 | felicia$FightersActionsReactions.X2 !=0)
```

Look at the notes on each fighter. The X1 is Felicia, the X2 is one of the three opponent's.
```{r}
head(Added$FighterActionReactions.X1, 10)
head(Added$FightersActionsReactions.X2,10)
```

List the unique notes for X1.
```{r}
unique(Added$FighterActionReactions.X1)
```

There are 134 unique actions for Felicia amongst these three opponents. Out of how many observations?
```{r}
dim(Added)
```
There are 524 observations in the data that excludes observations with no noticeable action or reaction from X1 or X2.

The unique actions/reactions from the opponent's separately but collectively are:
```{r}
unique(Added$FightersActionsReactions.X2)
```
There are 92 unique actions from these three opponent's as X2, in this case the opponents are Zarah, Megan Anderson, and Cyborg.

***

Get fighter X1's list of actions and reactions to split on.
```{r}
Sym <- strsplit(as.character(Added$FighterActionReactions.X1), ',')

```

Create 1st sequence to grab actions from the table by index of observation that
the action occured; this grabs the index of occurence in the table from each observation as a vector.
```{r}
sq1 <- lapply(Sym,'[',1)
head(sq1,10)
```

```{r}
sq2 <- lapply(Sym,'[',2)
sq3 <- lapply(Sym,'[',3)
```

Lets also get the ground moves added to our data as features. The ground moves are the 'holding...hold' for the fighter holding some hold or body part as either an arm, back, full mount, back mount, etc. There is also a note of when the fighter 'loses...hold' or stops holding that was added but doesn't mean it isn't ground work done or submitted but that in some instances switched from a full mount hold to a side mount control hold, of lost back mount hold to holding back when legs not fully locked, etc. There is also the options for the opponent who 'breaks...hold' or 'caught in...hold' to add to this data for machine learning. I also want to distinguish between muay thai (mt) and push kick instead of just counting all kicks. The ground work was intentionally added to the first sequence before the comma separator, so we just need to add it as a grab from each fighter. But there are multiple holds in some instances and breaks, caught, and lost holds. So we need to account for the multiple holds. 
```{r}
hold <- grep('holding.+hold',sq1)
hold
```
There are a lot of instances for this fighter that involves ground work, clinches or holds while standing, and wrestling from the number of instances that a hold is used by this fighter. Now look at the number of instances this fighter is caught in a hold.This can be an arm hold when clinching while standing up or on the ground when the opponent grabs the arm to prevent getting hit by it or prevent a wrestling submission.
```{r}
caught <- grep('caught.*hold',sq1)
caught
```
There aren't that many instances that the fighter is caught in a hold compared to holding the opponent.

Lets look at the lost holds.
```{r}
lost <- grep('loses.*hold', sq1)
lost
```
Not many holds were lost or quit by this fighter.

Now for the number of holds this fighter broke out of or the opponent quit holding.
```{r}
breaks <- grep('breaks.*hold',sq1)
breaks
```
There were some holds this fighter broke out of. They could have been quit by the opponent, and some are arm or limb holds of some sort.

Now lets check out the number of muay thai kicks and push kicks separately in the first sequence of each instance.
```{r}
mtKicks <- grep('mt kick',sq1)
pushKicks <- grep('push kick',sq1)
mtKicks;pushKicks
```
Lets look at the right (R) or left (L) legs used to deliver the muay thai kicks or push kicks separately.
```{r}
LmtKicks <- grep('L mt kick',sq1)
RmtKicks <- grep('R mt kick',sq1)
Lpush <- grep('L push kick', sq1)
Rpush <- grep('R push kick', sq1)

LmtKicks;RmtKicks;Lpush;Rpush
```
For muay thai kicks this fighter prefers the left leg to deliver them, and for push kicks this fighter prefers using the right leg. And this is only in the start of a second in time of a three sequence max second of time as an observation. 

Lets look at the muay thai and push kicks by X1 in the 2nd and 3rd sequences as well to get an idea of how this fighter rates their own kicks in sequential order or usefulness.
```{r}
mtKicks2 <- grep('mt kick',sq2)
pushKicks2 <- grep('push kick',sq2)
mtKicks2;pushKicks2

mtKicks3 <- grep('mt kick',sq3)
pushKicks3 <- grep('push kick',sq3)
mtKicks3;pushKicks3

```
This fighter X1 didn't use any kicks in the 2nd and 3rd sequence, so there is no point in seeing which ones are delivered with the right or left leg.

***

Lets now look at the opponent's or X2's preference in the first second as an action or reaction by starting with the number of holds that X2 used. 

This creates a list of the first sequence of three to select actions from for X2.
```{r}
SymX2 <- strsplit(as.character(paste(Added$FightersActionsReactions.X2)),',')
sq1X2 <- lapply(SymX2,'[',1)
sq2X2 <- lapply(SymX2,'[',2)
sq3X2 <- lapply(SymX2,'[',3)

```

```{r}
holdX2 <- grep('holding.+hold',sq1X2)
holdX2
```
The opponent's acting as X2 didn't use as many holds as this X1 fighter did.


Lets see the number of holds X2 was caught in.
```{r}
caughtX2 <- grep('caught.*hold',sq1X2)
caughtX2
```
The opponent's were caught in many holds from X1, as we expect to see because X1 had a lot of holds initiated for many instances.


Lets look at the lost holds by X2.
```{r}
lostX2 <- grep('loses.*hold', sq1X2)
lostX2
```
The opponents lost some of the holds they had, but not all of them on X1.


Now for the number of holds that x2 broke out of or the opponent quit holding.
```{r}
breaksX2 <- grep('breaks.*hold',sq1X2)
breaksX2
```
The opponents broke out of quite a bit of holds. Some are mounts and moving up to the striking instead of wrestling ground work.


Now lets check out the number of muay thai kicks and push kicks separately in the first sequence of each instance that X2 used.
```{r}
mtKicksX2 <- grep('mt kick',sq1X2)
pushKicksX2 <- grep('push kick',sq1X2)
mtKicksX2;pushKicksX2
```
None of these opponents used any push kicks, but did use some muay thai kicks in the first sequence. Keep in mind there are two other sequences that kicks could have been delivered, like in an open guard, where the opponent X2 uses an 'upward kick' which could be considered a push kick. Lets see.
```{r}
openPushX2 <- grep('upward.*kick',sq1X2)
openPushX2
```
But not for these opponents, in the first sequence any how. How about in the 2nd and 3rd sequences, did X2 use any muay thai or push kicks?
```{r}
mtKicksX2b <- grep('mt kick',sq2X2)
pushKicksX2b <- grep('push kick',sq2X2)
mtKicksX2b;pushKicksX2b

mtKicksX2c <- grep('mt kick',sq3X2)
pushKicksX2c <- grep('push kick',sq3X2)
mtKicksX2c;pushKicksX2c

```
No kicks by X2 in the 2nd or 3rd sequences of any instance.


Lets look at the right (R) or left (L) legs used by X2 to deliver the muay thai kicks.
```{r}
LmtKicksX2 <- grep('L mt kick',sq1X2)
RmtKicksX2 <- grep('R mt kick',sq1X2)

LmtKicksX2;RmtKicksX2
```
It was almost an equal amount of muay thai kicks by all opponents in left or right leg used, but the majority of muay thai kicks delivered to X1 by X2 (3 different opponents separately) used the left leg. This could be toward the lead leg to lessen the stability of X1 or knock off balance to reduce striking power delivered by X1's right cross, or to the head or body. Lets see.
```{r}
sq1X2[LmtKicksX2]
```
It looks like the opponents are equal in landed muay thai kicks to the inner or L low leg, for this X1's lead leg is the left (L) leg, or the body. 

***

This will grab the indices of each specific action grep'd from the action text fields to count each action. We will do this for up to three sequences of actions/reactions in a second. An example of a few of the action vectors are displayed.
```{r}
kicks_sq1 <- grep('land.*kick', sq1)
elbows_sq1 <- grep('land.*elbow', sq1)
knees_sq1 <- grep('land.*knee', sq1)
jab_sq1 <- grep('land.*jab', sq1)
cross_sq1 <- grep('land.*cross', sq1)
hook_sq1 <- grep('land.*hook', sq1)
upper_sq1 <- grep('land.*uppercut', sq1)
takedown_sq1 <- grep('land.*takedown', sq1)
hammer_sq1 <- grep('land.*hammer', sq1)

hammer_sq1;takedown_sq1;upper_sq1;cross_sq1
```

Missed in 1st sequence; Blocks aren't accounted for and when missed, it is assumed because the opponent was blocking or ducking, not that the fighter didn't land the hit but because it was blocked either with a block an opposing strike, grab, or ducked.
```{r}
kicks_sq1m <- grep('miss.*kick', sq1)
elbows_sq1m <- grep('miss.*elbow', sq1)
knees_sq1m <- grep('miss.*knee', sq1)
jab_sq1m <- grep('miss.*jab', sq1)
cross_sq1m <- grep('miss.*cross', sq1)
hook_sq1m <- grep('miss.*hook', sq1)
upper_sq1m <- grep('miss.*upp', sq1)
takedown_sq1m <- grep('miss.*takedown', sq1)
hammer_sq1m <- grep('miss.*hammer', sq1)

```


landed in second sequence
```{r}
kicks_sq2 <- grep('land*kick', sq2)
elbows_sq2 <- grep('land.*elbow', sq2)
knees_sq2 <- grep('land.*knee', sq2)
jab_sq2 <- grep('land.*jab', sq2)
cross_sq2 <- grep('land.*cross', sq2)
hook_sq2 <- grep('land.*hook', sq2)
upper_sq2 <- grep('land.*upp', sq2)
takedown_sq2 <- grep('land.*takedown', sq2)
hammer_sq2 <- grep('land.*hammer', sq2)

```

missed in 2nd sequence
```{r}
kicks_sq2m <- grep('miss.*kick', sq2)
elbows_sq2m <- grep('miss.*elbow', sq2)
knees_sq2m <- grep('miss.*knee', sq2)
jab_sq2m <- grep('miss.*jab', sq2)
cross_sq2m <- grep('miss.*cross', sq2)
hook_sq2m <- grep('miss.*hook', sq2)
upper_sq2m <- grep('miss.*upp', sq2)
takedown_sq2m <- grep('miss.*takedown', sq2)
hammer_sq2m <- grep('miss.*hammer', sq2)

```


landed in 3rd sequence
```{r}
kicks_sq3 <- grep('land.*kick', sq3)
elbows_sq3 <- grep('land.*elbow', sq3)
knees_sq3 <- grep('land.*knee', sq3)
jab_sq3 <- grep('land.*jab', sq3)
cross_sq3 <- grep('land.*cross', sq3)
hook_sq3 <- grep('land.*hook', sq3)
upper_sq3 <- grep('land.*upp', sq3)
takedown_sq3 <- grep('land.*takedown', sq3)
hammer_sq3 <- grep('land.*hammer', sq3)

```

missed in 3rd sequence
```{r}
kicks_sq3m <- grep('miss.*kick', sq3)
elbows_sq3m <- grep('miss.*elbow', sq3)
knees_sq3m <- grep('miss.*knee', sq3)
jab_sq3m <- grep('miss.*jab', sq3)
cross_sq3m <- grep('miss.*cross', sq3)
hook_sq3m <- grep('miss.*hook', sq3)
upper_sq3m <- grep('miss.*upp', sq3)
takedown_sq3m <- grep('miss.*takedown', sq3)
hammer_sq3m <- grep('miss.*hammer', sq3)

```

get fighter2's list of actions/reactions
```{r}
sq1b <- sq1X2

```

lands 1st sequence X2, ends with 'b', no 'l' to either for lands
```{r}
kicks_sq1b <- grep('land.*kick', sq1b)
elbows_sq1b <- grep('land.*elbow', sq1b)
knees_sq1b <- grep('land.*knee', sq1b)
jab_sq1b <- grep('land.*jab', sq1b)
cross_sq1b <- grep('land.*cross', sq1b)
hook_sq1b <- grep('land.*hook', sq1b)
upper_sq1b <- grep('land.*upp', sq1b)
takedown_sq1b <- grep('land.*takedown', sq1b)
hammer_sq1b <- grep('land.*hammer', sq1b)

```

received by X1 in 1st sequence, duplicated above as 
equivalent to hits landed 1st seq of x2
```{r}
kicks_sq1r <- grep('land.*kick', sq1b)
elbows_sq1r <- grep('land.*elbow', sq1b)
knees_sq1r <- grep('land.*knee', sq1b)
jab_sq1r <- grep('land.*jab', sq1b)
cross_sq1r <- grep('land.*cross', sq1b)
hook_sq1r <- grep('land.*hook', sq1b)
upper_sq1r <- grep('land.*upp', sq1b)
takedown_sq1r <- grep('land.*takedown', sq1b)
hammer_sq1r <- grep('land.*hammer', sq1b)

```

missed in 1st sequence X2
```{r}
kicks_sq1bm <- grep('miss.*kick', sq1b)
elbows_sq1bm <- grep('miss.*elbow', sq1b)
knees_sq1bm <- grep('miss.*knee', sq1b)
jab_sq1bm <- grep('miss.*jab', sq1b)
cross_sq1bm <- grep('miss.*cross', sq1b)
hook_sq1bm <- grep('miss.*hook', sq1b)
upper_sq1bm <- grep('miss.*upp', sq1b)
takedown_sq1bm <- grep('miss.*takedown', sq1b)
hammer_sq1bm <- grep('miss.*hammer', sq1b)

```

received by x2 in 1st seq equivalent to lands by x1
```{r}
kicks_sq1br <- grep('land.*kick', sq1)
elbows_sq1br <- grep('land.*elbow', sq1)
knees_sq1br <- grep('land.*knee', sq1)
jab_sq1br <- grep('land.*jab', sq1)
cross_sq1br <- grep('land.*cross', sq1)
hook_sq1br <- grep('land.*hook', sq1)
upper_sq1br <- grep('land.*upp', sq1)
takedown_sq1br <- grep('land.*takedown', sq1)
hammer_sq1br <- grep('land.*hammer', sq1)

```

lands 2nd sequence x2
```{r}
sq2b <- sq2X2

```

```{r}
kicks_sq2b <- grep('land.*kick', sq2b)
elbows_sq2b <- grep('land.*elbow', sq2b)
knees_sq2b <- grep('land.*knee', sq2b)
jab_sq2b <- grep('land.*jab', sq2b)
cross_sq2b <- grep('land.*cross', sq2b)
hook_sq2b <- grep('land.*hook', sq2b)
upper_sq2b <- grep('land.*upp', sq2b)
takedown_sq2b <- grep('land.*takedown', sq2b)
hammer_sq2b <- grep('land.*hammer', sq2b)

```

received by X1 in 2nd sequence equivalent to hits landed by x2 seq 2
```{r}
kicks_sq2r <- grep('land.*kick', sq2b)
elbows_sq2r <- grep('land.*elbow', sq2b)
knees_sq2r <- grep('land.*knee', sq2b)
jab_sq2r <- grep('land.*jab', sq2b)
cross_sq2r <- grep('land.*cross', sq2b)
hook_sq2r <- grep('land.*hook', sq2b)
upper_sq2r <- grep('land.*upp', sq2b)
takedown_sq2r <- grep('land.*takedown', sq2b)
hammer_sq2r <- grep('land.*hammer', sq2b)

```

missed in 2nd sequence x2
```{r}
kicks_sq2bm <- grep('miss.*kick', sq2b)
elbows_sq2bm <- grep('miss.*elbow', sq2b)
knees_sq2bm <- grep('miss.*knee', sq2b)
jab_sq2bm <- grep('miss.*jab', sq2b)
cross_sq2bm <- grep('miss.*cross', sq2b)
hook_sq2bm <- grep('miss.*hook', sq2b)
upper_sq2bm <- grep('miss.*upp', sq2b)
takedown_sq2bm <- grep('miss.*takedown', sq2b)
hammer_sq2bm <- grep('miss.*hammer', sq2b)

```

received 2nd seq by x2 equivalent to hits landed by x1 in seq 2
```{r}
kicks_sq2br <- grep('land.*kick', sq2)
elbows_sq2br <- grep('land.*elbow', sq2)
knees_sq2br <- grep('land.*knee', sq2)
jab_sq2br <- grep('land.*jab', sq2)
cross_sq2br <- grep('land.*cross', sq2)
hook_sq2br <- grep('land.*hook', sq2)
upper_sq2br <- grep('land.*upp', sq2)
takedown_sq2br <- grep('land.*takedown', sq2)
hammer_sq2br <- grep('land.*hammer', sq2)

```

lands 3rd sequence x2
```{r}
sq3b <- sq3X2

```

```{r}
kicks_sq3b <- grep('land.*kick', sq3b)
elbows_sq3b <- grep('land.*elbow', sq3b)
knees_sq3b <- grep('land.*knee', sq3b)
jab_sq3b <- grep('land.*jab', sq3b)
cross_sq3b <- grep('land.*cross', sq3b)
hook_sq3b <- grep('land.*hook', sq3b)
upper_sq3b <- grep('land.*upp', sq3b)
takedown_sq3b <- grep('land.*takedown', sq3b)
hammer_sq3b <- grep('land.*hammer', sq3b)


```

received by X1 in 3rd sequence equivalent to hits landed by X2 in seq 3
```{r}
kicks_sq3r <- grep('land.*kick', sq3b)
elbows_sq3r <- grep('land.*elbow', sq3b)
knees_sq3r <- grep('land.*knee', sq3b)
jab_sq3r <- grep('land.*jab', sq3b)
cross_sq3r <- grep('land.*cross', sq3b)
hook_sq3r <- grep('land.*hook', sq3b)
upper_sq3r <- grep('land.*upp', sq3b)
takedown_sq3r <- grep('land.*takedown', sq3b)
hammer_sq3r <- grep('land.*hammer', sq3b)

```

missed in 3rd sequence x2
```{r}
kicks_sq3bm <- grep('miss.*kick', sq3b)
elbows_sq3bm <- grep('miss.*elbow', sq3b)
knees_sq3bm <- grep('miss.*knee', sq3b)
jab_sq3bm <- grep('miss.*jab', sq3b)
cross_sq3bm <- grep('miss.*cross', sq3b)
hook_sq3bm <- grep('miss.*hook', sq3b)
upper_sq3bm <- grep('miss.*upp', sq3b)
takedown_sq3bm <- grep('miss.*takedown', sq3b)
hammer_sq3bm <- grep('miss.*hammer', sq3b)

```

received in seq 3 by x2 equivalent to hits landed by x1 in seq3
```{r}
kicks_sq3br <- grep('land.*kick', sq3)
elbows_sq3br <- grep('land.*elbow', sq3)
knees_sq3br <- grep('land.*knee', sq3)
jab_sq3br <- grep('land.*jab', sq3)
cross_sq3br <- grep('land.*cross', sq3)
hook_sq3br <- grep('land.*hook', sq3)
upper_sq3br <- grep('land.*upp', sq3)
takedown_sq3br <- grep('land.*takedown', sq3)
hammer_sq3br <- grep('land.*hammer', sq3)

```

This adds the fields (54 fields for each sequence) to the table of actions per second, by
creating table extensions of Added, then renaming the 3rd sequence of actions.
```{r}
added_landed <- mutate(Added, Crossl.X1=0, Kneel.X1=0, Elbowl.X1=0, Hookl.X1=0, Jabl.X1=0, Kickl.X1=0,
                Crossl.X2=0, Kneel.X2=0, Elbowl.X2=0, Hookl.X2=0, Jabl.X2=0, Kickl.X2=0, upperl.X1=0,
                upperl.X2=0, takedownl.X1=0, takedownl.X2=0, hammerl.X1=0, hammerl.X2=0
                , Cross2l.X1=0, Knee2l.X1=0, Elbow2l.X1=0, Hook2l.X1=0, Jab2l.X1=0, Kick2l.X1=0,
                Cross2l.X2=0, Knee2l.X2=0, Elbow2l.X2=0, Hook2l.X2=0, Jab2l.X2=0, Kick2l.X2=0, upper2l.X1=0,
                upper2l.X2=0, takedown2l.X1=0, takedown2l.X2=0, hammer2l.X1=0, hammer2l.X2=0
                , Cross3l.X1=0, Knee3l.X1=0, Elbow3l.X1=0, Hook3l.X1=0, Jab3l.X1=0, Kick3l.X1=0,
                Cross3l.X2=0, Knee3l.X2=0, Elbow3l.X2=0, Hook3l.X2=0, Jab3l.X2=0, Kick3l.X2=0, upper3l.X1=0,
                upper3l.X2=0, takedown3l.X1=0, takedown3l.X2=0, hammer3l.X1=0, hammer3l.X2=0)

added_missed <- mutate(added_landed, Crossm.X1=0, Kneem.X1=0, Elbowm.X1=0, Hookm.X1=0, Jabm.X1=0, Kickm.X1=0,
                       Crossm.X2=0, Kneem.X2=0, Elbowm.X2=0, Hookm.X2=0, Jabm.X2=0, Kickm.X2=0, upperm.X1=0,
                       upperm.X2=0, takedownm.X1=0, takedownm.X2=0, hammerm.X1=0, hammerm.X2=0
                       , Cross2m.X1=0, Knee2m.X1=0, Elbow2m.X1=0, Hook2m.X1=0, Jab2m.X1=0, Kick2m.X1=0,
                       Cross2m.X2=0, Knee2m.X2=0, Elbow2m.X2=0, Hook2m.X2=0, Jab2m.X2=0, Kick2m.X2=0, upper2m.X1=0,
                       upper2m.X2=0, takedown2m.X1=0, takedown2m.X2=0, hammer2m.X1=0, hammer2m.X2=0
                       , Cross3m.X1=0, Knee3m.X1=0, Elbow3m.X1=0, Hook3m.X1=0, Jab3m.X1=0, Kick3m.X1=0,
                       Cross3m.X2=0, Knee3m.X2=0, Elbow3m.X2=0, Hook3m.X2=0, Jab3m.X2=0, Kick3m.X2=0, upper3m.X1=0,
                       upper3m.X2=0, takedown3m.X1=0, takedown3m.X2=0, hammer3m.X1=0, hammer3m.X2=0)

added_received <- mutate(added_missed, Crossr.X1=0, Kneer.X1=0, Elbowr.X1=0, Hookr.X1=0, Jabr.X1=0, Kickr.X1=0,
                       Crossr.X2=0, Kneer.X2=0, Elbowr.X2=0, Hookr.X2=0, Jabr.X2=0, Kickr.X2=0, upperr.X1=0,
                       upperr.X2=0, takedownr.X1=0, takedownr.X2=0, hammerr.X1=0, hammerr.X2=0
                       , Cross2r.X1=0, Knee2r.X1=0, Elbow2r.X1=0, Hook2r.X1=0, Jab2r.X1=0, Kick2r.X1=0,
                       Cross2r.X2=0, Knee2r.X2=0, Elbow2r.X2=0, Hook2r.X2=0, Jab2r.X2=0, Kick2r.X2=0, upper2r.X1=0,
                       upper2r.X2=0, takedown2r.X1=0, takedown2r.X2=0, hammer2r.X1=0, hammer2r.X2=0
                       , Cross3r.X1=0, Knee3r.X1=0, Elbow3r.X1=0, Hook3r.X1=0, Jab3r.X1=0, Kick3r.X1=0,
                       Cross3r.X2=0, Knee3r.X2=0, Elbow3r.X2=0, Hook3r.X2=0, Jab3r.X2=0, Kick3r.X2=0, upper3r.X1=0,
                       upper3r.X2=0, takedown3r.X1=0, takedown3r.X2=0, hammer3r.X1=0, hammer3r.X2=0)

```

Save original Added data table and make a new table called Added that is the combined
received, missed, and landed binary/dummy columns just mutated to each other above using dplyr.
```{r}
Added1 <- Added
```

```{r}
Added <- added_received
head(Added,10)
```

The following code adds a value of 1 if the grep'd binary field has a count in that index
of observation, otherwise, it will be 0.
```{r}
Added[cross_sq1,'Crossl.X1'] <- 1
Added[cross_sq1b,'Crossl.X2'] <- 1
Added[hook_sq1,'Hookl.X1'] <- 1
Added[hook_sq1b,'Hookl.X2'] <- 1
Added[jab_sq1,'Jabl.X1'] <- 1
Added[jab_sq1b,'Jabl.X2'] <- 1
Added[knees_sq1,'Kneel.X1'] <- 1
Added[knees_sq1b,'Kneel.X2'] <- 1
Added[elbows_sq1,'Elbowl.X1'] <- 1
Added[elbows_sq1b,'Elbowl.X2'] <- 1
Added[kicks_sq1,'Kickl.X1'] <- 1
Added[kicks_sq1b,'Kickl.X2'] <- 1
Added[upper_sq1,'upperl.X1'] <- 1
Added[upper_sq1b,'upperl.X2'] <- 1
Added[takedown_sq1,'takedownl.X1'] <- 1
Added[takedown_sq1b,'takedownl.X2'] <- 1
Added[hammer_sq1,'hammerl.X1'] <- 1
Added[hammer_sq1b,'hammerl.X2'] <- 1

Added[cross_sq2,'Cross2l.X1'] <- 1
Added[cross_sq2b,'Cross2l.X2'] <- 1
Added[hook_sq2,'Hook2l.X1'] <- 1
Added[hook_sq2b,'Hook2l.X2'] <- 1
Added[jab_sq2,'Jab2l.X1'] <- 1
Added[jab_sq2b,'Jab2l.X2'] <- 1
Added[knees_sq2,'Knee2l.X1'] <- 1
Added[knees_sq2b,'Knee2l.X2'] <- 1
Added[elbows_sq2,'Elbow2l.X1'] <- 1
Added[elbows_sq2b,'Elbow2l.X2'] <- 1
Added[kicks_sq2,'Kick2l.X1'] <- 1
Added[kicks_sq2b,'Kick2l.X2'] <- 1
Added[upper_sq2,'upper2l.X1'] <- 1
Added[upper_sq2b,'upper2l.X2'] <- 1
Added[takedown_sq2,'takedown2l.X1'] <- 1
Added[takedown_sq2b,'takedown2l.X2'] <- 1
Added[hammer_sq2,'hammer2l.X1'] <- 1
Added[hammer_sq2b,'hammer2l.X2'] <- 1

Added[cross_sq3,'Cross3l.X1'] <- 1
Added[cross_sq3b,'Cross3l.X2'] <- 1
Added[hook_sq3,'Hook3l.X1'] <- 1
Added[hook_sq3b,'Hook3l.X2'] <- 1
Added[jab_sq3,'Jab3l.X1'] <- 1
Added[jab_sq3b,'Jab3l.X2'] <- 1
Added[knees_sq3,'Knee3l.X1'] <- 1
Added[knees_sq3b,'Knee3l.X2'] <- 1
Added[elbows_sq3,'Elbow3l.X1'] <- 1
Added[elbows_sq3b,'Elbow3l.X2'] <- 1
Added[kicks_sq3,'Kick3l.X1'] <- 1
Added[kicks_sq3b,'Kick3l.X2'] <- 1
Added[upper_sq3,'upper3l.X1'] <- 1
Added[upper_sq3b,'upper3l.X2'] <- 1
Added[takedown_sq3,'takedown3l.X1'] <- 1
Added[takedown_sq3b,'takedown3l.X2'] <- 1
Added[hammer_sq3,'hammer3l.X1'] <- 1
Added[hammer_sq3b,'hammer3l.X2'] <- 1

Added[cross_sq1m,'Crossm.X1'] <- 1
Added[cross_sq1bm,'Crossm.X2'] <- 1
Added[hook_sq1m,'Hookm.X1'] <- 1
Added[hook_sq1bm,'Hookm.X2'] <- 1
Added[jab_sq1m,'Jabm.X1'] <- 1
Added[jab_sq1bm,'Jabm.X2'] <- 1
Added[knees_sq1m,'Kneem.X1'] <- 1
Added[knees_sq1bm,'Kneem.X2'] <- 1
Added[elbows_sq1m,'Elbowm.X1'] <- 1
Added[elbows_sq1bm,'Elbowm.X2'] <- 1
Added[kicks_sq1m,'Kickm.X1'] <- 1
Added[kicks_sq1bm,'Kickm.X2'] <- 1
Added[upper_sq1m,'upperm.X1'] <- 1
Added[upper_sq1bm,'upperm.X2'] <- 1
Added[takedown_sq1m,'takedownm.X1'] <- 1
Added[takedown_sq1bm,'takedownm.X2'] <- 1
Added[hammer_sq1m,'hammerm.X1'] <- 1
Added[hammer_sq1bm,'hammerm.X2'] <- 1

Added[cross_sq2m,'Cross2m.X1'] <- 1
Added[cross_sq2bm,'Cross2m.X2'] <- 1
Added[hook_sq2m,'Hook2m.X1'] <- 1
Added[hook_sq2bm,'Hook2m.X2'] <- 1
Added[jab_sq2m,'Jab2m.X1'] <- 1
Added[jab_sq2bm,'Jab2m.X2'] <- 1
Added[knees_sq2m,'Knee2m.X1'] <- 1
Added[knees_sq2bm,'Knee2m.X2'] <- 1
Added[elbows_sq2m,'Elbow2m.X1'] <- 1
Added[elbows_sq2bm,'Elbow2m.X2'] <- 1
Added[kicks_sq2m,'Kick2m.X1'] <- 1
Added[kicks_sq2bm,'Kick2m.X2'] <- 1
Added[upper_sq2m,'upper2m.X1'] <- 1
Added[upper_sq2bm,'upper2m.X2'] <- 1
Added[takedown_sq2m,'takedown2m.X1'] <- 1
Added[takedown_sq2bm,'takedown2m.X2'] <- 1
Added[hammer_sq2m,'hammer2m.X1'] <- 1
Added[hammer_sq2bm,'hammer2m.X2'] <- 1

Added[cross_sq3m,'Cross3m.X1'] <- 1
Added[cross_sq3bm,'Cross3m.X2'] <- 1
Added[hook_sq3m,'Hook3m.X1'] <- 1
Added[hook_sq3bm,'Hook3m.X2'] <- 1
Added[jab_sq3m,'Jab3m.X1'] <- 1
Added[jab_sq3bm,'Jab3m.X2'] <- 1
Added[knees_sq3m,'Knee3m.X1'] <- 1
Added[knees_sq3bm,'Knee3m.X2'] <- 1
Added[elbows_sq3m,'Elbow3m.X1'] <- 1
Added[elbows_sq3bm,'Elbow3m.X2'] <- 1
Added[kicks_sq3m,'Kick3m.X1'] <- 1
Added[kicks_sq3bm,'Kick3m.X2'] <- 1
Added[upper_sq3m,'upper3m.X1'] <- 1
Added[upper_sq3bm,'upper3m.X2'] <- 1
Added[takedown_sq3m,'takedown3m.X1'] <- 1
Added[takedown_sq3bm,'takedown3m.X2'] <- 1
Added[hammer_sq3m,'hammer3m.X1'] <- 1
Added[hammer_sq3bm,'hammer3m.X2'] <- 1

Added[cross_sq1r,'Crossr.X1'] <- 1
Added[cross_sq1br,'Crossr.X2'] <- 1
Added[hook_sq1r,'Hookr.X1'] <- 1
Added[hook_sq1br,'Hookr.X2'] <- 1
Added[jab_sq1r,'Jabr.X1'] <- 1
Added[jab_sq1br,'Jabr.X2'] <- 1
Added[knees_sq1r,'Kneer.X1'] <- 1
Added[knees_sq1br,'Kneer.X2'] <- 1
Added[elbows_sq1r,'Elbowr.X1'] <- 1
Added[elbows_sq1br,'Elbowr.X2'] <- 1
Added[kicks_sq1r,'Kickr.X1'] <- 1
Added[kicks_sq1br,'Kickr.X2'] <- 1
Added[upper_sq1r,'upperr.X1'] <- 1
Added[upper_sq1br,'upperr.X2'] <- 1
Added[takedown_sq1r,'takedownr.X1'] <- 1
Added[takedown_sq1br,'takedownr.X2'] <- 1
Added[hammer_sq1r,'hammerr.X1'] <- 1
Added[hammer_sq1br,'hammerr.X2'] <- 1

Added[cross_sq2r,'Cross2r.X1'] <- 1
Added[cross_sq2br,'Cross2r.X2'] <- 1
Added[hook_sq2r,'Hook2r.X1'] <- 1
Added[hook_sq2br,'Hook2r.X2'] <- 1
Added[jab_sq2r,'Jab2r.X1'] <- 1
Added[jab_sq2br,'Jab2r.X2'] <- 1
Added[knees_sq2r,'Knee2r.X1'] <- 1
Added[knees_sq2br,'Knee2r.X2'] <- 1
Added[elbows_sq2r,'Elbow2r.X1'] <- 1
Added[elbows_sq2br,'Elbow2r.X2'] <- 1
Added[kicks_sq2r,'Kick2r.X1'] <- 1
Added[kicks_sq2br,'Kick2r.X2'] <- 1
Added[upper_sq2r,'upper2r.X1'] <- 1
Added[upper_sq2br,'upper2r.X2'] <- 1
Added[takedown_sq2r,'takedown2r.X1'] <- 1
Added[takedown_sq2br,'takedown2r.X2'] <- 1
Added[hammer_sq2r,'hammer2r.X1'] <- 1
Added[hammer_sq2br,'hammer2r.X2'] <- 1

Added[cross_sq3r,'Cross3r.X1'] <- 1
Added[cross_sq3br,'Cross3r.X2'] <- 1
Added[hook_sq3r,'Hook3r.X1'] <- 1
Added[hook_sq3br,'Hook3r.X2'] <- 1
Added[jab_sq3r,'Jab3r.X1'] <- 1
Added[jab_sq3br,'Jab3r.X2'] <- 1
Added[knees_sq3r,'Knee3r.X1'] <- 1
Added[knees_sq3br,'Knee3r.X2'] <- 1
Added[elbows_sq3r,'Elbow3r.X1'] <- 1
Added[elbows_sq3br,'Elbow3r.X2'] <- 1
Added[kicks_sq3r,'Kick3r.X1'] <- 1
Added[kicks_sq3br,'Kick3r.X2'] <- 1
Added[upper_sq3r,'upper3r.X1'] <- 1
Added[upper_sq3br,'upper3r.X2'] <- 1
Added[takedown_sq3r,'takedown3r.X1'] <- 1
Added[takedown_sq3br,'takedown3r.X2'] <- 1
Added[hammer_sq3r,'hammer3r.X1'] <- 1
Added[hammer_sq3br,'hammer3r.X2'] <- 1

```

```{r}
colnames(Added)
```

```{r}
head(Added,10)
```


Removes SecondsIntoRound.
```{r}
Added2 <- Added[,-2] 
Seconds <- mutate(Added2, SecondsIntoRound=300-(as.numeric(Added2$Time)))
seconds <- Seconds[,c(1,181,2:180)]

```


```{r}
seconds$lastAction <- as.character(paste(lag(seconds$SecondsIntoRound,1)))
seconds$lastAction <- gsub('NA','0',seconds$lastAction)
seconds$lastAction <- as.numeric(paste(seconds$lastAction))
```

Get the first and last neighborhood of features or columns in this data table.
```{r}
colnames(seconds)[c(1:5,180:182)]
```

Rearrange the columns and remove the empty SecondsLastRoundAction field.
```{r}
seconds <- seconds[,c(1:2,182,4:181)]
colnames(seconds)[1:8]
```

Extension of seconds using mutate() of dplyr library, keeps value of seconds into round as
seconds since last action if no action that observation.
```{r}
last <- mutate(seconds, SecondsLastRoundAction = if_else(seconds$SecondsIntoRound -
                                                         seconds$lastAction > 0,
                                                       seconds$SecondsIntoRound -
                                                         seconds$lastAction,
                                                       seconds$SecondsIntoRound))

```

Reorders so that SecondsLastRoundAction is at front fields location.
```{r}
last <- last[,c(1:3,182,4:181)] 

```

Rearrange the order of the actions by fighter and landed, missed, and received. Also, add the counts for each accumulated landed actions, missed actions, and received actions per second observed.
```{r}
landX1 <- colnames(last)[c(21:26,33,35,37,39:44,51,53,55,57:62,69,71,73)]
landX2 <- colnames(last)[c(27:32,34,36,38,45:50,52,54,56,63:68,70,72,74)]

missX1 <- colnames(last)[c(75:80,87,89,91,93:98,105,107,109,111:116,123,125,127)]
missX2 <- colnames(last)[c(81:86,88,90,92,99:104,106,108,110,117:122,124,126,128)]

recvX1 <- colnames(last)[c(129:134,141,143,145,147:152,159,161,163,165:170,177,179,181)]
recvX2 <- colnames(last)[c(135:140,142,144,146,153:158,160,162,164,171:176,178,180,182)]

x1l <- mutate(last, TotLandsX1=last[,21]+last[,22]+last[,23]+last[,24]+last[,25]+
                last[,26]+last[,33]+last[,35]+last[,37]+last[,39]+last[,40]+
                last[,41]+last[,42]+last[,43]+last[,44]+last[,51]+last[,53]+
                last[,55]+last[,57]+last[,58]+last[,59]+last[,60]+last[,61]+
                last[,62]+last[,69]+last[,71]+last[,73])
x1m <- mutate(x1l, TotMissedX1=last[,75]+last[,76]+last[,77]+last[,78]+last[,79]+
                last[,80]+last[,87]+last[,89]+last[,91]+last[,93]+last[,94]+
                last[,95]+last[,96]+last[,97]+last[,98]+last[,105]+last[,107]+
                last[,109]+last[,111]+last[,112]+last[,113]+last[,114]+last[,115]+
                last[,116]+last[,123]+last[,125]+last[,127])
x1r <- mutate(x1m, TotReceivedX1=last[,129]+last[,130]+last[,131]+last[,132]+last[,133]+
                last[,134]+last[,141]+last[,143]+last[,145]+last[,147]+last[,148]+
                last[,149]+last[,150]+last[,151]+last[,152]+last[,159]+last[,161]+
                last[,163]+last[,165]+last[,166]+last[,167]+last[,168]+last[,169]+
                last[,170]+last[,177]+last[,179]+last[,181])

x2l <- mutate(x1r, TotLandsX2=last[,27]+last[,28]+last[,29]+last[,30]+last[,31]+
                last[,32]+last[,34]+last[,36]+last[,38]+last[,45]+last[,46]+
                last[,47]+last[,48]+last[,49]+last[,50]+last[,52]+last[,54]+
                last[,56]+last[,63]+last[,64]+last[,65]+last[,66]+last[,67]+
                last[,68]+last[,70]+last[,72]+last[,74])
x2m <- mutate(x2l, TotMissedX2=last[,81]+last[,82]+last[,83]+last[,84]+last[,85]+
                last[,86]+last[,88]+last[,90]+last[,92]+last[,99]+last[,100]+
                last[,101]+last[,102]+last[,103]+last[,104]+last[,106]+last[,108]+
                last[,110]+last[,117]+last[,118]+last[,119]+last[,120]+last[,121]+
                last[,122]+last[,124]+last[,126]+last[,128])
x2r <- mutate(x2m, TotReceivedX2=last[,135]+last[,136]+last[,137]+last[,138]+last[,139]+
                last[,140]+last[,142]+last[,144]+last[,146]+last[,153]+last[,154]+
                last[,155]+last[,156]+last[,157]+last[,158]+last[,160]+last[,162]+
                last[,164]+last[,171]+last[,172]+last[,173]+last[,174]+last[,175]+
                last[,176]+last[,178]+last[,180]+last[,182])

Added3 <- x2r[,c(1,2,3,4:7,183:185,11:13,186:188,17:182)]

colnames(Added3)
```

Create break points where last action in seconds is greater than the amount of seconds of the round. This is because there are three round 1's from three fights, where the cut off needs to be in place for the end of a round when counting seconds since last action of that round.
```{r}
break1 <- Added3$lastAction > Added3$SecondsIntoRound
break1
```

```{r}
break2 <- row.names(Added3[break1,])
break2
```

```{r}
bk2 <- as.numeric(break2)
bk2
```

```{r}
split1 <- bk2[1]
split1
```

```{r}
split2 <- bk2[2]
split2
```
First opponent:
```{r}
Table1 <- Added3[1:(split1-1),]
Table1
```
Second opponent:
```{r}
Table2 <- Added3[split1:(split2-1),]
Table2
```
Third opponent:
```{r}
Table3 <- Added3[split2:(length(Added3$Round)),]
Table3
```

This next section creates the cumulative actions of hits landed,missed, or received for each second
into the round for each of the three table splits by opponent, since only first round of various fights extracted and just created above.
```{r}
Table4 <- mutate(Table1, cmTotHitsR.X1=cumsum(TotReceivedX1), 
                cmTotHitsL.X1=cumsum(TotLandsX1),
                cmTotHitsM.X1=cumsum(TotMissedX1),
                cmTotHitsR.X2=cumsum(TotReceivedX2),
                cmTotHitsL.X2=cumsum(TotLandsX2),
                cmTotHitsM.X2=cumsum(TotMissedX2))

Table5 <- mutate(Table2, cmTotHitsR.X1=cumsum(TotReceivedX1), 
                 cmTotHitsL.X1=cumsum(TotLandsX1),
                 cmTotHitsM.X1=cumsum(TotMissedX1),
                 cmTotHitsR.X2=cumsum(TotReceivedX2),
                 cmTotHitsL.X2=cumsum(TotLandsX2),
                 cmTotHitsM.X2=cumsum(TotMissedX2))

Table6 <- mutate(Table3, cmTotHitsR.X1=cumsum(TotReceivedX1), 
                 cmTotHitsL.X1=cumsum(TotLandsX1),
                 cmTotHitsM.X1=cumsum(TotMissedX1),
                 cmTotHitsR.X2=cumsum(TotReceivedX2),
                 cmTotHitsL.X2=cumsum(TotLandsX2),
                 cmTotHitsM.X2=cumsum(TotMissedX2))

```

This combines the three table splits with cumulative sum of actions for each unique round or fighter.
```{r}
Table7 <- rbind(Table4,Table5,Table6)
colnames(Table7)

```

The following rearranges the table columns by actions landed, missed, and received into a new table.
```{r}
landX1 <- c(21:26,33,35,37,39:44,51,53,55,57:62,69,71,73)
landX2 <- c(27:32,34,36,38,45:50,52,54,56,63:68,70,72,74)

missX1 <- c(75:80,87,89,91,93:98,105,107,109,111:116,123,125,127)
missX2 <- c(81:86,88,90,92,99:104,106,108,110,117:122,124,126,128)

recvX1 <- c(129:134,141,143,145,147:152,159,161,163,165:170,177,179,181)
recvX2 <- c(135:140,142,144,146,153:158,160,162,164,171:176,178,180,182)

Table8 <- Table7[,c(1:20,landX1,landX2,missX1,missX2,recvX1,recvX2)]
colnames(Table8)

```

We should now add in the wrestling holds,caught in, breaks hold, and lost hold features for each fighter of X1 or X2 in each instance, not sequence. We already made these fields at the beginning of this script. Lets also add in the muay thai and push kicks for each sequence noting that these are already counted in the 'kicks' feature. We will just count them all as attempted action or reaction regardless if it landed or missed opponent. The target was blocked if not landed or ducked. We can account for these changes later in a different script if needed or you could do it yourself and not be critical of my work donations. Aside: mutalate misogynist monsters as a stepping stone instead of small creatures as a message to anybody who gets this nerdy break-down of violent and graphic content. Leave the squirrels, bunnys, guneau pigs, and females alone.

The 'breaks','caught','hold', and 'lost' are for X1, and the 'breaksX2','holdX2','caughtX2', and 
'lostX2' are for X2 ground or submit type wrestling actions or reactions. For the kicks, 'mtKicks' and 'pushKicks' are for X1 and 'mtKicksX2' and 'pushKicksX2' are for X2 actions or reactions. Those will be taken from only the 1st sequence for this fighter because none of the 2nd and 3rd sequences had any push or muay thai kicks from either fighter. The lists to pull from are sq1 for X1 and sq1b for X2. 

Lets make a new table from Table8.
```{r}
Table9 <- Table8
```

Create the placeholders for each action feature added.
```{r}
Table9$holdingX1 <- 0
Table9$holdingX2 <- 0
Table9$breaksHoldX1 <- 0
Table9$breaksHoldX2 <- 0 
Table9$caughtHoldX1 <- 0  
Table9$caughtHoldX2 <- 0  
Table9$lostHoldX1 <- 0  
Table9$lostHoldX2 <- 0   
```

```{r}
colnames(Table9)[183:190]
```

```{r}
Table9[hold,"holdingX1"] <- 1 
Table9[holdX2,"holdingX2"] <- 1 
Table9[breaks,"breaksHoldX1"] <- 1
Table9[breaksX2,"breaksHoldX2"] <- 1 
Table9[caught,"caughtHoldX1"] <- 1  
Table9[caughtX2,"caughtHoldX2"] <- 1  
Table9[lost,"lostHoldX1"] <- 1  
Table9[lostX2,"lostHoldX2"] <- 1   

```
Now add in the muay thai and push kicks to this table.
```{r}
Table9$muayThaiKickX1 <- 0
Table9$muayThaiKickX2 <- 0
Table9$pushKickX1 <- 0
Table9$pushKickX2 <- 0
```

```{r}
Table9[mtKicks,"muayThaiKickX1"] <- 1
Table9[mtKicksX2,"muayThaiKickX2"] <- 1
Table9[pushKicks,"pushKickX1"] <- 1
Table9[pushKicksX2,"pushKickX2"] <- 1

```


We should also add in the cumulative count of each instance of holding, lost holds, broken holds, and instances caught in a hold as well as the cumulative counts of muay thai and push kicks attempted by each fighter. Since X2 has three different fighters, we need to split the Table 9 into a table of each opponent as we did earlier, get the cumulative counts, then recombine the table splits.
```{r}
break_1 <- Table9$lastAction > Table9$SecondsIntoRound
break_1
```

```{r}
breaks_3 <- row.names(Table9)[break_1]
breaks_3
```


```{r}
breaks_split <- as.numeric(paste(breaks_3))
breaks_split
```
We had these values already stored in bk2, but the above was a refresher on how we obtained the X2 fighter splits.

Create the three tables of the separate fighters of X2.
```{r}
table1_a <- Table9[1:(breaks_split[1]-1),]
table1_b <- Table9[breaks_split[1]:(breaks_split[2]-1),]
table1_c <- Table9[breaks_split[2]:length(Table9$Round),]

dim(table1_a)[1]+dim(table1_b)[1]+dim(table1_c)[1]==dim(Table9)[1]
```
All observations are accounted for as the number of observations in all three subset tables of Table 9 add up to the total observations in Table 9.

Now lets create those cumulative fields for each table. The cumulative sum of holds and caught holds is for seconds in a hold as caught or holding a hold, while the cumulative sums of the broken holds and lost holds are for number of times in that round and this fighter the hold was broken if caught in a hold or lost if holding the hold.

The first fighter as X2 table.
```{r}
table1_a$totalHoldsX1 <- cumsum(table1_a$holdingX1)
table1_a$totalHoldsX2 <- cumsum(table1_a$holdingX2)
table1_a$totalLostHoldsX1 <- cumsum(table1_a$lostHoldX1)
table1_a$totalLostHoldsX2 <- cumsum(table1_a$lostHoldX2)
table1_a$totalCaughtHoldsX1 <- cumsum(table1_a$caughtHoldX1)
table1_a$totalCaughtHoldsX2 <- cumsum(table1_a$caughtHoldX2)
table1_a$totalBreakOutHoldsX1 <- cumsum(table1_a$breaksHoldX1)
table1_a$totalBreakOutHoldsX2 <- cumsum(table1_a$breaksHoldX2)
table1_a$totalMuayThaiKicksX1 <- cumsum(table1_a$muayThaiKickX1)
table1_a$totalMuayThaiKicksX2 <- cumsum(table1_a$muayThaiKickX2)
table1_a$totalPushKicksX1 <- cumsum(table1_a$pushKickX1)
table1_a$totalPushKicksX2 <- cumsum(table1_a$pushKickX2)

tail(table1_a[,195:206],10)
```

The 2nd fighter as x2 table.
```{r}
table1_b$totalHoldsX1 <- cumsum(table1_b$holdingX1)
table1_b$totalHoldsX2 <- cumsum(table1_b$holdingX2)
table1_b$totalLostHoldsX1 <- cumsum(table1_b$lostHoldX1)
table1_b$totalLostHoldsX2 <- cumsum(table1_b$lostHoldX2)
table1_b$totalCaughtHoldsX1 <- cumsum(table1_b$caughtHoldX1)
table1_b$totalCaughtHoldsX2 <- cumsum(table1_b$caughtHoldX2)
table1_b$totalBreakOutHoldsX1 <- cumsum(table1_b$breaksHoldX1)
table1_b$totalBreakOutHoldsX2 <- cumsum(table1_b$breaksHoldX2)
table1_b$totalMuayThaiKicksX1 <- cumsum(table1_b$muayThaiKickX1)
table1_b$totalMuayThaiKicksX2 <- cumsum(table1_b$muayThaiKickX2)
table1_b$totalPushKicksX1 <- cumsum(table1_b$pushKickX1)
table1_b$totalPushKicksX2 <- cumsum(table1_b$pushKickX2)

tail(table1_b[,195:206],10)
```

The 3rd fighter as X2 table.
```{r}
table1_c$totalHoldsX1 <- cumsum(table1_c$holdingX1)
table1_c$totalHoldsX2 <- cumsum(table1_c$holdingX2)
table1_c$totalLostHoldsX1 <- cumsum(table1_c$lostHoldX1)
table1_c$totalLostHoldsX2 <- cumsum(table1_c$lostHoldX2)
table1_c$totalCaughtHoldsX1 <- cumsum(table1_c$caughtHoldX1)
table1_c$totalCaughtHoldsX2 <- cumsum(table1_c$caughtHoldX2)
table1_c$totalBreakOutHoldsX1 <- cumsum(table1_c$breaksHoldX1)
table1_c$totalBreakOutHoldsX2 <- cumsum(table1_c$breaksHoldX2)
table1_c$totalMuayThaiKicksX1 <- cumsum(table1_c$muayThaiKickX1)
table1_c$totalMuayThaiKicksX2 <- cumsum(table1_c$muayThaiKickX2)
table1_c$totalPushKicksX1 <- cumsum(table1_c$pushKickX1)
table1_c$totalPushKicksX2 <- cumsum(table1_c$pushKickX2)

tail(table1_c[,195:206],10)
```

Now we need to create a new table of these combined tables of cumulative sums.
```{r}
Table10 <- rbind(table1_a,table1_b,table1_c)
dim(Table10)
dim(Table9)
```

```{r}
head(Table10)
```




Write this table of added action/reaction features to csv file.
```{r}
write.csv(Table10, 'Felicia3Fights_addedFeatures.csv', row.names=F)

```

So far, we have a great table of actions, reactions, and cumulative totals for all actions, for attempted muay thai kicks, attempted push kicks, cumulative seconds of holds and being caught in a hold for each fighter, and cumulative counts of holds broken or lost for each fighter in the first round of their fight with Felicia as either Zarah, Megan Anderson, or Cyborg. Next we will select some or all features to target the outcome as a hit landed for that instant as predicted with the features selected. We will see how far our machine learning can go as far as predicting if the main outcome for X1 is a hit landed based on all given features or a select few using a model built on the training subset of this data to test on the remaining samples in accuracy of prediction. 



***
***
***

# Machine Learning with R packages 

This section will test out these features in part to predict the target of X1 landing a hit or missing a hit. We have to exclude the features where X2 received a hit if the instance shows a hit landed, so the model learns well. Random forest, recursive partitioned trees, generalized linear models, knn, and gradient boosted models within the R packages of Caret will be used to see how well these features predict the accuracy in a hit landed. Note that we do have all wrestling features for this data set. 

```{r}
library(caret)
```

Make a table to predict the hit landed for X1 using all features except for the time, notes, X1 landed, X1 missed, X1 received, and X2 received features. 
```{r}
colnames(Table10)
```

```{r}
ML_table <- Table10[,c(2:16,48:74,102:128,183:206)]
str(ML_table)
```

Write this table out to use in Python later. Its easier than selecting the columns in Python.
```{r}
write.csv(ML_table, 'Felicia_ml_ready.csv', row.names=FALSE)
```

Our target variable is TotLandsX1 which will be a hit landed for X1 based on the other features that include the wrestling features of both X1 and X2, and actions of X2 for hits landed and missed to see how this can predict if X1 will land more hits based on the number of attempts on X1, the seconds holding and being held, the number of holds broken or lost, the seconds since last action by either X1 or X2, the seconds into the round, and cumulative totals for all landed, missed, and received hits for X1 and X2. Since all values are numeric, this will make it easier to classify the hit with linear regression as closer to zero or up to the max hits landed in an observation. We should see what that value is for the max number of hits landed in one second.
```{r}
max(ML_table$TotLandsX1)
```
This fighter, X1 or Felicia in this case, landed two hits at most in any second observed in the first round with one of three different fighters: Zarah, Megan, or Christiane.

Lets split the data into a 70% training set and a 30% testing set.
```{r}
set.seed(12345)
inTrain <- createDataPartition(y=ML_table$TotLandsX1, p=0.7, list=FALSE)

trainingSet <- ML_table[inTrain,]
testingSet <- ML_table[-inTrain,]
```

Lets get the dimensions of the testing and training set to see how many observations are in each subset of our data.
```{r}
dim(testingSet)
dim(trainingSet)
```
We could normalize these features to subtract the mean from each feature and divide by the number of samples or observations in each, but I won't do that here. And actually this preprocessing can be done within each model using the parameters from the Caret R module. We will use python with the reticulate R package later to do this as well and compare results. We see that the training set has 371 samples and the testing set has 158 samples. Lets see how many of each total landed hits by X1 are in each subset.
```{r}
library(dplyr)

```


```{r}
summaryLandedHitsX1 <- trainingSet %>% group_by(TotLandsX1) %>% count(TotLandsX1)
summaryLandedHitsX1
```
There are 330 instances where no hits were landed by X1, 39 instances where 1 hit was landed by X1, and 2 instances where there were 2 hits landed by X1 in our training set that will build our varioud models to predict on the testing set. 


```{r}
summaryLandedHitsX1b <- testingSet %>% group_by(TotLandsX1) %>% count(TotLandsX1)
summaryLandedHitsX1b
```
In our testing set, we will predict the hits landed by the model built on our training set to predict 143 instances that 0 hits landed, 9 instances where 1 hit was landed by X1, and 6 instances where 2 hits were landed by X1. We will probably lose precision on the instances where 2 hits were landed, because our model is only using 2 instances that actually had 2 hits landed when building it. The zeros will probably be accurate, and the 1s possibly as well. We will have to see which of our ML models based on the R Carets module perform the best as far as accuracy is concerned.

Lets start with the random forest model using the boot method and also will be centering and scaling the features in our model. We will use the cv or cross validation which tests on subsets when training to build the model, and set classProbs to TRUE so that regression isn't done on our numeric values, but classification for classes: 0,1,or 2.
```{r, error=FALSE, message=FALSE, warning=FALSE}
rf_cv15 <- train(TotLandsX1~., method='rf', 
               na.action=na.pass,
               data=(trainingSet),  preProc = c("center", "scale"),
               trControl=trainControl(method='cv', classProbs = T), number=15)
```

```{r}
predRF_cv15 <- predict(rf_cv15, testingSet)

DF_cv15 <- data.frame(predRF_cv15, ActualHitsLanded=testingSet$TotLandsX1)

length_cv15 <- length(DF_cv15$ActualHitsLanded)

sum_cv15 <- sum(DF_cv15$predRF_boot==DF_cv15$ActualHitsLanded)

accRF_cv15 <- (sum_cv15/length_cv15)

accRF_cv15
```


```{r}
head(DF_cv15,30)
```
The values predicted are outside their class probabilities, probably due to the centered scaling or normalisaion. 

Lets try this again without preprocessing which will 
```{r, error=FALSE, message=FALSE, warning=FALSE}
rf_cv15b <- train(TotLandsX1~., method='rf', 
               na.action=na.pass,
               data=(trainingSet),  #preProc = c("center", "scale"),
               trControl=trainControl(method='cv', classProbs = T), number=15)
```

```{r}
predRF_cv15b <- predict(rf_cv15b, testingSet)

DF_cv15b <- data.frame(predRF_cv15b, ActualHitsLanded=testingSet$TotLandsX1)

length_cv15b <- length(DF_cv15b$ActualHitsLanded)

sum_cv15b <- sum(DF_cv15b$predRF_boot==DF_cv15b$ActualHitsLanded)

accRF_cv15b <- (sum_cv15b/length_cv15b)

accRF_cv15b
```


```{r}
head(DF_cv15b,30)
```

Lets add in an extra feature that will round these values to 0, 1, or 2 on both random forest models to get a realistic output. Even when not normalizing and using classProbs set to True, the classification is producing probability classes instead. Could be due to the numeric type of the target. Lets see if we change the numeric type to a factor if it classifies better. Or the same as our rounded features of the random forest models normalized and not normalized.
```{r}
trainingSetFactor <- trainingSet
testingSetFactor <- testingSet

trainingSetFactor$TotLandsX1 <- as.factor(paste(trainingSetFactor$TotLandsX1))
testingSetFactor$TotLandsX1 <- as.factor(paste(testingSetFactor$TotLandsX1))
```



```{r, error=FALSE, message=FALSE, warning=FALSE}
rf_cv15c <- train(TotLandsX1~., method='rf', 
               na.action=na.pass,
               data=(trainingSetFactor),  #preProc = c("center", "scale"),
               trControl=trainControl(method='cv'), number=15)
```



```{r}
predRF_cv15c <- predict(rf_cv15c, testingSetFactor)

DF_cv15c <- data.frame(predRF_cv15c, ActualHitsLanded=testingSet$TotLandsX1)

length_cv15c <- length(DF_cv15c$ActualHitsLanded)

sum_cv15c <- sum(DF_cv15c$predRF_cv15c==DF_cv15c$ActualHitsLanded)

accRF_cv15c <- (sum_cv15c/length_cv15c)

accRF_cv15c
```


```{r}
head(DF_cv15c,30)
```
It did work better, because the accuracy went from 0 to 96% accuracy in predicting the hits landed correctly.
Lets still add in the features to the previous random forest models to compare.
```{r}
DF_cv15$roundedPrediction <- ifelse(DF_cv15$predRF_cv15 < 0, 0, 
                                    ifelse(DF_cv15$predRF_cv15 > 2, 2,
                                           round(DF_cv15$predRF_cv15)
                                           )
                                    )
DF_cv15$Correct <- ifelse(DF_cv15$ActualHitsLanded==DF_cv15$roundedPrediction,1,0)
accuracy1 <- sum(DF_cv15$Correct)/length(DF_cv15$Correct)
accuracy1
```

```{r}
head(DF_cv15)
```

The non normalized random forest model results when rounding to boundaries or closest class:
```{r}
DF_cv15b$roundedPrediction <- ifelse(DF_cv15b$predRF_cv15 < 0, 0, 
                                    ifelse(DF_cv15b$predRF_cv15 > 2, 2,
                                           round(DF_cv15b$predRF_cv15)
                                           )
                                    )
DF_cv15b$Correct <- ifelse(DF_cv15b$ActualHitsLanded==DF_cv15b$roundedPrediction,1,0)
accuracy2 <- sum(DF_cv15b$Correct)/length(DF_cv15b$Correct)
accuracy2
```

```{r}
head(DF_cv15b)
```

When classifying with random forest on numeric classes then rounding to the closest boundaries and closest class after closest min or max boundary, the results were the same for normalized or non normalized random forest classification. And the results were better than using the random forest model to classify on numeric values that are actually integers to factors by .06%. Because the former models scored 96.8% accuracy and the latter model scored 96.2% accuracy on the test set.

Next, we will use the k-nearest neighbor algorithm with the cv method to train.
```{r}
knn_cv <- train(TotLandsX1 ~ .,
                method='knn',# preProcess=c('center','scale'),
                tuneLength=10, trControl=trainControl(method='cv'),
                data=trainingSet)
```


```{r}
predKNN_cv <- predict(knn_cv, testingSet)

DF_KNN_cv <- data.frame(predKNN_cv, ActualHitsLanded=testingSet$TotLandsX1)

length_KNN_cv <- length(DF_KNN_cv$ActualHitsLanded)

sum_KNN_cv <- sum(DF_KNN_cv$predKNN_cv==DF_KNN_cv$ActualHitsLanded)

accKNN_cv <- (sum_KNN_cv/length_KNN_cv)

accKNN_cv
```

```{r}
head(DF_KNN_cv)
```
Lets add in the same features of rounded and correct fields to this table as well. As the predicted values are not integers.
```{r}
DF_KNN_cv$roundedPrediction <- ifelse(DF_KNN_cv$predKNN_cv<0,0,
                            ifelse(DF_KNN_cv$predKNN_cv>2,2,
                                   round(DF_KNN_cv$predKNN_cv,0)))
DF_KNN_cv$Correct <- ifelse(DF_KNN_cv$ActualHitsLanded==DF_KNN_cv$roundedPrediction,1,0)
accuracy3 <- sum(DF_KNN_cv$Correct)/length(DF_KNN_cv$Correct)
accuracy3
```

Next, we will use the k-nearest neighbor algorithm with the cv method to train on the target as a factor instead of numeric.
```{r}
knn_cvb <- train(TotLandsX1 ~ .,
                method='knn',# preProcess=c('center','scale'),
                tuneLength=10, trControl=trainControl(method='cv'),
                data=trainingSetFactor)
```


```{r}
predKNN_cvb <- predict(knn_cvb, testingSetFactor)

DF_KNN_cvb <- data.frame(predKNN_cvb, ActualHitsLanded=testingSet$TotLandsX1)

length_KNN_cvb <- length(DF_KNN_cvb$ActualHitsLanded)

sum_KNN_cvb <- sum(DF_KNN_cvb$predKNN_cvb==DF_KNN_cvb$ActualHitsLanded)

accKNN_cvb <- (sum_KNN_cvb/length_KNN_cvb)

accKNN_cvb
```

The KNN algorithm didn't score as high in accuracy for classifying the total hits landed by X1 in our testing set. But using the KNN to classify on the numeric data then rounding the results to the closest min or max boundaries if out of bounds and then rounding to the nearest number in our 3 classes of 0, 1, or 2 did score exactly the same at 90.5% accuracy.


```{r}
head(DF_KNN_cvb)
```


Now lets see how well this data will do in a few other algorithms: glm, rpart, and gbm. We have a factor version target and a numeric target with which to compare results.
```{r, error=FALSE, message=FALSE, warning=FALSE}
glmMod2 <- train(TotLandsX1 ~ .,
                method='glm', data=trainingSet)
```

```{r, error=FALSE, warning=FALSE, message=FALSE}
predglm2 <- predict(glmMod2, testingSet)

DF_glm2 <- data.frame(predglm2, ActualHitsLanded=testingSet$TotLandsX1)

length_glm2 <- length(DF_glm2$ActualHitsLanded)

sum_glm2 <- sum(DF_glm2$predglm2==DF_glm2$ActualHitsLanded)

accglm2 <- (sum_glm2/length_glm2)

accglm2
```


```{r}
head(DF_glm2)
```

```{r}
DF_glm2$roundedPrediction <- ifelse(DF_glm2$predglm2<0,0,
                            ifelse(DF_glm2$predglm2>2,2,
                                   round(DF_glm2$predglm2,0)))
DF_glm2$Correct <- ifelse(DF_glm2$ActualHitsLanded==DF_glm2$roundedPrediction,1,0)
accuracy4 <- sum(DF_glm2$Correct)/length(DF_glm2$Correct)
accuracy4
```


```{r}
head(DF_glm2)
```
The glm or generalized linear model algorithm when correcting for min/max boundaries and rounding the predicted result with the actual result scored 96.8% accuracy. This makes it in a tie with the two random forest models using the same boundary and rounded modifications on numeric classes when classifying the hits landed for each instance as a 0, 1, or 2.


```{r, error=FALSE, message=FALSE, warning=FALSE}
rpartMod <- train(TotLandsX1 ~ .,
                method='rpart', data=trainingSet)
```

```{r, error=FALSE, warning=FALSE, message=FALSE}
predrpart <- predict(rpartMod, testingSet)

DF_rpart <- data.frame(predrpart, ActualHitsLanded=testingSet$TotLandsX1)

length_rpart <- length(DF_rpart$ActualHitsLanded)

sum_rpart <- sum(DF_rpart$predrpart==DF_rpart$ActualHitsLanded)

accrpart <- (sum_rpart/length_rpart)

accrpart
```

```{r}
head(DF_rpart)
```
The rpart algorithm scored as well as the KNN at 90.5% accuracy in prediction.


```{r}
DF_rpart$roundedPrediction <- ifelse(DF_rpart$predrpart<0,0,
                            ifelse(DF_rpart$predrpart>2,2,
                                   round(DF_rpart$predrpart,0)))
DF_rpart$Correct <- ifelse(DF_rpart$ActualHitsLanded==DF_rpart$roundedPrediction,1,0)
accuracy5 <- sum(DF_rpart$Correct)/length(DF_rpart$Correct)
accuracy5
```
When correcting for min/max boundaries and rounding as a modification used on the other models the rpart model scored near 6% better in accuracy at 96.2%. But still not as good as our best models of glm and random forest classifications on numeric data.

```{r}
head(DF_rpart)
```

Lets look at the gradient boosted models or gbm model that uses the previous output to improve on a gradient and weights adjusted by the previous iteration. You will see a lengthly output of the process, but it shows how with each iteration the model tries to improve.
```{r, error=FALSE, message=FALSE, warning=FALSE}
gbmMod <- train(TotLandsX1 ~ .,
                method='gbm', data=trainingSet)
```

```{r, error=FALSE, warning=FALSE, message=FALSE}
predgbm <- predict(gbmMod, testingSet)

DF_gbm <- data.frame(predgbm, ActualHitsLanded=testingSet$TotLandsX1)

length_gbm <- length(DF_gbm$ActualHitsLanded)

sum_gbm <- sum(DF_gbm$predgbm==DF_gbm$ActualHitsLanded)

accgbm <- (sum_gbm/length_gbm)

accgbm
```
```{r}
head(DF_gbm)
```


```{r}
DF_gbm$roundedPrediction <- ifelse(DF_gbm$predgbm<0,0,
                            ifelse(DF_gbm$predgbm>2,2,
                                   round(DF_gbm$predgbm,0)))
DF_gbm$Correct <- ifelse(DF_gbm$ActualHitsLanded==DF_gbm$roundedPrediction,1,0)
accuracy6 <- sum(DF_gbm$Correct)/length(DF_gbm$Correct)
accuracy6
```
When correcting for min/max boundaries and rounding to closest numeric class of 0, 1, or 2, the gbm algorithm scored as well as the modified rpart model at 96.2% accuracty in prediction. This model is also not the best for this data, but as good as the random forest classifier on the target feature as a factor instead of numeric.
```{r}
head(DF_gbm)
```

That is it for the machine learning using the caret package of algorithms in R. Next we will compare python's random forest, gradient boosted models, naive bayes, and convolutional neural network and deep neural networks in predicting hits landed using the reticulate package in R to run python code.

***
***
***

```{r}
library(reticulate)
```

```{r}
conda_list(conda = "auto") 
```

```{r}
use_condaenv(condaenv = "python36")
```

```{python}
import pandas as pd 
import sklearn 
import numpy as np 
from sklearn.naive_bayes import MultinomialNB 

from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

np.random.seed(47) 
```

Import our ML ready file to run the python algorithms on with sklearn.
```{python}
Felicia = pd.read_csv('Felicia_ml_ready.csv', encoding = 'unicode_escape') 
print(Felicia.shape)
```

```{python}
print(Felicia.columns)
```

```{python}
print(Felicia.head())
```

```{python}
print(Felicia.tail())
```

```{python}
print(Felicia['TotLandsX1'].unique())
```
Reorder the rows as instances randomised to split into train and test sets of the data.
```{python}
import numpy as np

Felicia = Felicia.reindex(np.random.permutation(Felicia.index))

print(Felicia.head())
```

```{python}
print(Felicia.tail())
```


There are 529 instances in this data, and 80% is about 424, the target is the hits landed by X1.
```{python}
# Split/splice into training ~ 80% and testing ~ 20%
Felicia_train = Felicia[:424]
Felicia_test = Felicia[424:]
Felicia_hits_train = Felicia['TotLandsX1'][:424]
Felicia_hits_test = Felicia['TotLandsX1'][424:]

print(Felicia_train.shape)
print(Felicia_test.shape)
```

# Sklearn's Multinomial Naive Bayes classifier
```{python}
mnb_Fit = MultinomialNB().fit(Felicia_train, Felicia_hits_train)
```

```{python}
predictions = mnb_Fit.predict(Felicia_test)

prd = pd.DataFrame(predictions)
prd.columns=['predictions']
prd.index=Felicia_hits_test.index
pred=pd.concat([pd.DataFrame(prd),Felicia_hits_test],axis=1)
print(pred)
```

```{python}
print('accuracy', accuracy_score(Felicia_hits_test, predictions))
```


```{python}
print('confusion matrix')
print('rows=expected, cols=predicted')
print(confusion_matrix(Felicia_hits_test, predictions))
```
Think of the recall as the rows that all expected instances need to be found and those
outside the correct value reduce the 'recall' of the test.
Also, think of the columns as the predicted that mean those predicted that value in the same column reduce the precision. 

The seed wasn't set, and setting it in numpy didn't work, because the output is different. In tensorflow it works when we get to that CNN.

So we can see for 0 hits landed it expected 47, misidentified a 1 hit landed as 0 for 47/48 precision accuracy, and misidentified 17 1 hits and 30 2 hits as a 0 hits landed with a recall
of 47/(47+17+30) accuracy. The 2s were mostly classified incorrectly as 0s when they were 2s,
and the 1s were correctly identified for 8 instances on the test set but 17 were classified incorrectly as 0s and 2 classes that weren't a 1 were also classified as a 1. This model for multinomial naive bayes scored the 0s accurately but misclassified the 1s by about one third and the 2s were misclassified as 0s or 1 1/32 for very inaccurate results. 



# Sklearn's Random Forest and Gradient Boosting Classifier results
```{python}

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import precision_recall_fscore_support as score
import time

```

# Random Forest Classifier
```{python}
rf=RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)
start=time.time()
rf_model=rf.fit(Felicia_train,Felicia_hits_train)
end=time.time()
fit_time=(end-start)
fit_time
```


```{python}
start=time.time()
y_pred=rf_model.predict(Felicia_test)
end=time.time()
pred_time=(end-start)
pred_time
```


```{python}
prd = pd.DataFrame(y_pred)
prd.columns=['Predicted']

prd.index=Felicia_hits_test.index
pred=pd.concat([pd.DataFrame(prd),Felicia_hits_test],axis=1)
print(pred)
```


```{python}
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

print('accuracy', accuracy_score(Felicia_hits_test, y_pred))

print('confusion matrix')
print(confusion_matrix(Felicia_hits_test, y_pred))
```

Random Forest scored 100% accuracy in predicting a hit landed in our test set. All 94 0 hits landed were identified and none of the other classes for 1 or 2 hits landed were incorrectly identified as 0, same for the 1s and the 2s. It was also unbelievably faster than I thought it would take.

# Gradient Boosted Model
```{python}
gb=GradientBoostingClassifier(n_estimators=150,max_depth=11)
start=time.time()
gb_model=gb.fit(Felicia_train,Felicia_hits_train)
end=time.time()
fit_time=(end-start)
fit_time
```


```{python}
start=time.time()
y_pred=gb_model.predict(Felicia_test)
end=time.time()
pred_time=(end-start)
pred_time
```


```{python}
prd = pd.DataFrame(y_pred)
prd.columns=['Predicted']

prd.index=Felicia_hits_test.index
pred=pd.concat([pd.DataFrame(prd),Felicia_hits_test],axis=1)
print(pred)
```


```{python}
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

print('accuracy', accuracy_score(Felicia_hits_test, y_pred))

print('confusion matrix')
print(confusion_matrix(Felicia_hits_test, y_pred))
```
The gradient boosted model scored 100% accuracy on the test set in predicting hits landed as did the previous algorithm, random forest classifier.


Lets use convolutional neural nets now. To see if any difference. The last two sklearn model were
fast and the best in accuracy as they both scored 100% on the test sets used.

# Convolutional Neural Networks
```{python}
import numpy as np


```


```{python}
Felicia2 = Felicia
Felicia2
```


```{python}
class_mapping = {label: idx for idx, label in enumerate(np.unique(Felicia2['TotLandsX1']))}
class_mapping
```


```{python}
Felicia_hits_test = pd.DataFrame(Felicia_hits_test)
Felicia_hits_test.columns=['TotLandsX1']
Felicia_hits_test.columns
```

```{python}
Felicia_hits_test['TotLandsX1']=Felicia_hits_test['TotLandsX1'].map(class_mapping)
Felicia_hits_test.head()
```


```{python}
Felicia_hits_train=pd.DataFrame(Felicia_hits_train)
Felicia_hits_train.columns=['TotLandsX1']
Felicia_hits_train.columns
```


```{python}
Felicia_hits_train['TotLandsX1']=Felicia_hits_train['TotLandsX1'].map(class_mapping)
Felicia_hits_train.head()

```

```{python}
Felicia_train.shape

```


RStudio isn't recognizing the module 'tensorflow' in my python environment named python36. This portion of the neural nets will stop here, because tensorflow isn't being recognized, even though 'pip list' in the conda environment says it is a module available and my modules.

```{python}
import tensorflow as tf
import tensorflow.contrib.keras as keras
#optionally use import tensorflow.keras as keras when no longer experimental contributor package development

np.random.seed(123)
tf.set_random_seed(123)
```


```{python}

model6 = keras.models.Sequential()

model6.add(
    keras.layers.Dense(
        units=200,   #output units need to match next layer inputs 
        input_dim=93, #number of features for input above says 93
        kernel_initializer='glorot_uniform',# name of the guy behind Xavier Initialization; the biases to zero
        bias_initializer='zeros',
        activation='tanh'))

model6.add(
    keras.layers.Dense(
        units=100,   #output matches next layer input 
        input_dim=200, #input matches last layer's output
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='tanh'))

model6.add(
    keras.layers.Dense(
        units=100,   #output matches next layer input 
        input_dim=100, #input matches last layer's output
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='tanh'))

model6.add(
    keras.layers.Dense(
        units=100,   #output matches next layer input 
        input_dim=100, #input matches last layer's output
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='relu'))


model6.add(
    keras.layers.Dense(
        units=3,  #these are the number of class categories in our target  
        input_dim=100,
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='softmax'))#will return the class membership probs summing to 1 of all class probs

# these are hyperparameters that can be tuned if overfitting during training, or to get better accuracy
sgd_optimizer = keras.optimizers.SGD( 
        lr=0.001, decay=1e-7, momentum=.8)

# categorical_crossentropy is used in multiclass classification instead of binary_crossentropy
# to match the softmax function
model6.compile(optimizer=sgd_optimizer,
              loss='sparse_categorical_crossentropy')
# it was 'categorical_crossentropy', but that expects binary matrices of 1s and 0s
# it said to use sparse_categorical_crossentropy
```


```{python}
import time
start=time.time()

history6 = model6.fit(Felicia_train, Felicia_hits_train,
                    batch_size=64, epochs=50,
                    verbose=1, 
                    validation_split=0.15) 
end=time.time()
fit_time=(end-start)
print(start,end,fit_time)
```


```{python}
y_train_pred6 = model6.predict_classes(Felicia_train, verbose=0)
print('First 3 predictions: ', y_train_pred6[:3])
```


```{python}
y_train_pred6 = model6.predict_classes(Felicia_train, 
                                     verbose=0)
```


```{python}
y_train_pred6 = pd.DataFrame(y_train_pred6)
y_train_pred6.columns=['predicted']

y_train6 = Felicia_hits_train
y_train6 = pd.DataFrame(y_train6)
y_train6.columns=['ActualHitsLandedbyX1']

y_train_pred6.index=y_train6.index

Train6=pd.concat([y_train6['ActualHitsLandedbyX1'],y_train_pred6['predicted']],axis=1)

print(Train6)
```


```{python}
y_test_pred6 = model6.predict_classes(Felicia_test, 
                                    verbose=0)
```


```{python}
y_test_pred6 = pd.DataFrame(y_test_pred6)
y_test_pred6.columns=['predicted']

y_test6 = Felicia_hits_test
y_test6 = pd.DataFrame(y_test6)
y_test6.columns=['ActualHitsLandedbyX1']

y_test_pred6.index=y_test6.index

Test6=pd.concat([y_test6['ActualHitsLandedbyX1'],y_test_pred6['predicted']],axis=1)

print(Test6)
```

```{python}
s = sum(Train6['ActualHitsLandedbyX1']==Train6['predicted'])
l = len(Train6['ActualHitsLandedbyX1'])
accTrain6 = s/l
print('Training Correctly Predicted:',s,'Training Accuracy:',accTrain6,'\n')
```

```{python}
y_train_pred6['predicted'].unique()
Felicia_hits_train['ActualHitsLandedbyX1'].unique()

print(confusion_matrix(Felicia_hits_train, y_train_pred6))
```
The training set with validation training for our CNN model only predicted 0 for all possible classes. The accuracy was 89.85% with this run. In it a 3X3 array for the confusion matrix was printed. There were 381 correctly predicted class 0 hits landed, but all 35 class 1 hits landed were predicted to be 0 and all class 2 hits landed were predicted to be 0. 


```{python}
s = sum(Test6['ActualHitsLandedbyX1']==Test6['predicted'])
l = len(Test6['ActualHitsLandedbyX1'])
accTest6 = s/l
print('Testing Correctly Predicted:',s,'Testing Accuracy:',accTest6)
```
```{python}
y_test_pred6['predicted'].unique()
Felicia_hits_test['ActualHitsLandedbyX1'].unique()

print(confusion_matrix(Felicia_hits_test, y_test_pred6))
```
In our CNN model when using it to predict on our testing set, it again predicted all 0s for the class in hits landed by X1, but only had class 0 and class 1 to predict. All 13 class 1 hits landed were misclassified as 0 hits landed.


In summary, the best accuracy in prediction were the Random Forest and Gradient Boosted classifiers in Sklearn of Python and the modified boundaries and rounded class numeric values within those boundaries for the R Caret algorithms random forest and generalized linear models at 96.8% and rpart and gbm with 96.2%.

Later we will put this together to test the accuracy of another fighter or make predictions on hits landed when we switch out Nunez with X2 and run the machine learning models of our best catch accuracy in prediction to see who is more likely to land hits in their upcoming fight June 2, 2020. 

***


# Predicting who has the most hits landed in two separate data tables of X1

In this section we will use Felicia's hits as X1, but substitute in the X2 fighter with Amanda's table as x1, using R instead of Python. 

Lets read in Amanda's table with all fights and Felicia's ML ready table.
```{r}
AmandaML <- read.csv('Nunez4Fights_addedFeatures.csv', header=T, sep=',', na.strings=c('',' ','NA'))
FeliciaML <- read.csv('Felicia_ml_ready.csv', sep=',', header=T, na.strings=c('',' ','NA'))
```

```{r}
colnames(AmandaML)
```

All columns of X2 in AmandaML have 'X2' in them, so we can grab only Amanda's columns from her data by only selecting her opponent, X2, and removing it from her data table, except for the TotLandsX2 (column 14) because we want to switch that as TotLandsX1 for comparing Felicia to Germaine in Amanda's fight and as a placeholder to predict the outcome when Felicia is substituted for Germaine as an opponent. We will rename column 14 as 'OPPONENT_Landed' and rename it after gathering are neccessary columns to TotLandsX1 as a placeholder for Felicia's predicted hits landed against Amanda as X2.
```{r}
colnames(AmandaML)[14] <- 'OPPONENT_LANDED'

X2amanda <- grep('X2',colnames(AmandaML))
X2amanda

```

```{r}
AmandaX1 <- AmandaML[,-X2amanda]
colnames(AmandaX1)
```

We can now substitute the 'X1' for 'X2' so that Amanda will be X2 to compare to Felicia as X1.
```{r}
colnames(AmandaX1) <- gsub('X1','X2',colnames(AmandaX1))
colnames(AmandaX1)
```

Lets only keep her fight with Germaine, since it's the only one that Amanda's wrestling actions were recorded. And all of Felicia's were recorded.
```{r}
germaine <- grep('Germaine', AmandaX1$Notes)
AmandaX2 <- AmandaX1[germaine,]
AmandaX2[,14]
```

We should remove the TotReceivedX2 column from this ML table, because it is multicollinear with TotLandsX1 our target, and could be why some of the ML programs scored 100% accuracy.
```{r}
colnames(FeliciaML)
```


```{r}
FeliciaML2 <- FeliciaML[,-15]
colnames(FeliciaML2)
```

Lets get the X2 columns we want from Amanda as X2's data table.
```{r}
listFX2 <- grep('X2',colnames(FeliciaML2))
listFX2b <- colnames(FeliciaML2)[listFX2]
listFX2b
```

We want to keep the other information that involves the round, seconds in the round, last action, seconds last round action, and time to combine with the data of Felicia's when predicting hits landed based on the features we have chosen earlier. We have to get rid of the received hits of X2 and keep the features other than X2's information. They have 'r.X2' in the name, the columns we want removed in Amanda's data.
```{r}
listAX2c <- colnames(AmandaX1)
listAX2c
```

```{r}
listAX2d <- grep('r.X2',listAX2c)
listAX2e <- listAX2c[-listAX2d]
listAX2e
```

We should remove the total recieved hits by X2, round, time, notes, and action/reactions columns as well from Amanda's data.
```{r}
listAX2f <- listAX2e[-c(1,10,12:14)]
AmandaX2b <- AmandaX2[,listAX2f]
colnames(AmandaX2b)
```

We removed the wrestling holds of X1 from the table of Amanda's because we are assuming those holds wouldn't align with Felicia's actions or reactions, and so we must also remove the X1 holds from Felicia's table and exclude these features in predicting hits landed by Felicia when up against Amanda's actions as X2. We want to exclude all columns with 'X1' except TotLandsX1 that is column 7 listed as the 4th index of columns with 'X1'.
```{r}
colnames(FeliciaML2)
listFa <- grep('X1',colnames(FeliciaML2))
listFa
listFb <- listFa[-4]
listFb

```


```{r}
FeliciaML3 <- FeliciaML2[,-listFb]
colnames(FeliciaML3)
colnames(AmandaX2b)
```

We also need to remove the open guard kicks, that were exclusive only to Amanda's fight with Germaine, and not in Felicia's table. Note that Felicia also likes to do flying elbows and we didn't add that feature to her table nor Amanda's table, but it was included as a hit landed/missed for elbow as is. Also, change the placeholder for X1 hits landed to TotLandsX1. Also, reorder so that TotLandsX1 is before cmTotHitsR.X2 in column order to align with Felicia's data table.
```{r}
list3 <- grep('open', colnames(AmandaX2b))
AmandaX2c <- AmandaX2b[,-list3]
colnames(AmandaX2c)[9] <- 'TotLandsX1'
AmandaX2d <- AmandaX2c[,c(1:3,9,4:8,10:75)]
colnames(AmandaX2d)
```

Lets check that the columns are aligned the same.
```{r}
colnames(AmandaX2d)==colnames(FeliciaML3)
```
They are aligned the same, so we will make Felicia's table the training set, and Amanda's data table the testing set to see how Felicia's actions compare to Amanda's. We will use the best performing ML algorithms in R and Python. Note that we need to validate the data in training the model of Felicia's actions and reactions with our new set of features but keeping the same target feature of hits landed of Felicia's. We errored earlier in evaluating the algorithms on Felicia when including the total hits received by X2, a multicollinear (identical or identical partial units to each other i.e. x1=(1/3)x2 or x1=x2) vector or feature to our target of TotHitsLandX1 some of the decision tree algorithms might have picked up on this hence their fast computation time and near or at 100% accuracy in prediction. We will rerun those algorithms adjusting for the new data table that omits that multicollinear variable.

First lets save the data as we want them to recall them later with ease. 

```{r}
TrainingSetFelicia <- FeliciaML3
TestingSetAmanda <- AmandaX2d
```

Lets write these tables out to use in python's ML algorithms later.
```{r}
write.csv(TrainingSetFelicia,'FeliciaM-asX2-L-v2.csv',row.names=FALSE)
write.csv(TestingSetAmanda,'AmandaML-asX1-v2.csv', row.names=FALSE)
```

```{r}
dim(TrainingSetFelicia)
dim(TestingSetAmanda)

231/529
```
The ratio of samples to build the training model is about 56 to 44 training to testing split. This is only as a comparison. Where we will just sum up the hits landed as predicted by Felicia's training model to the sum of hits landed as the actual hits landed by Amanda's opponent Germaine. We have that actual value already as follows:
```{r}
AmandasOpponentsHitsLanded <- sum(TestingSetAmanda$TotLandsX1)
AmandasOpponentsHitsLanded
```
So, for this data there are 13 hits landed by the opponent we are comparing Felicia's predicted hits landed to, after building her prediction model.

Lets split the data into a 70% training set and a 30% testing set.
```{r}
set.seed(12345)
inTrain <- createDataPartition(y=TrainingSetFelicia$TotLandsX1, p=0.7, list=FALSE)

trainingSet <- TrainingSetFelicia[inTrain,]
testingSet <- TrainingSetFelicia[-inTrain,]
```

Lets get the dimensions of the testing and training set to see how many observations are in each subset of our data.
```{r}
dim(testingSet)
dim(trainingSet)
```

```{r}
library(caret)
```


We will test the random forest model of R's caret package used earlier to score 96.8% accuracy.
```{r, error=FALSE, message=FALSE, warning=FALSE}
rf_cv15 <- train(TotLandsX1~., method='rf', 
               na.action=na.pass,
               data=(trainingSet),  preProc = c("center", "scale"),
               trControl=trainControl(method='cv', classProbs = T), number=15)
```

```{r}
predRF_cv15 <- predict(rf_cv15, testingSet)

DF_cv15 <- data.frame(predRF_cv15, ActualHitsLanded=testingSet$TotLandsX1)

length_cv15 <- length(DF_cv15$ActualHitsLanded)

sum_cv15 <- sum(DF_cv15$predRF_boot==DF_cv15$ActualHitsLanded)

accRF_cv15 <- (sum_cv15/length_cv15)

accRF_cv15
```

```{r}
head(DF_cv15,30)
```

```{r}
DF_cv15$roundedPrediction <- ifelse(DF_cv15$predRF_cv15 < 0, 0, 
                                    ifelse(DF_cv15$predRF_cv15 > 2, 2,
                                           round(DF_cv15$predRF_cv15)
                                           )
                                    )
DF_cv15$Correct <- ifelse(DF_cv15$ActualHitsLanded==DF_cv15$roundedPrediction,1,0)
accuracy1 <- sum(DF_cv15$Correct)/length(DF_cv15$Correct)
accuracy1
```

```{r}
head(DF_cv15)
```

Now we'll test the generalized linear model of R's caret package that also scored 96.8% after modifications for boundaries and rounding.
```{r, error=FALSE, message=FALSE, warning=FALSE}
glmMod2 <- train(TotLandsX1 ~ .,
                method='glm', data=trainingSet)
```

```{r, error=FALSE, warning=FALSE, message=FALSE}
predglm2 <- predict(glmMod2, testingSet)

DF_glm2 <- data.frame(predglm2, ActualHitsLanded=testingSet$TotLandsX1)

length_glm2 <- length(DF_glm2$ActualHitsLanded)

sum_glm2 <- sum(DF_glm2$predglm2==DF_glm2$ActualHitsLanded)

accglm2 <- (sum_glm2/length_glm2)

accglm2
```


```{r}
head(DF_glm2)
```


```{r}
DF_glm2$roundedPrediction <- ifelse(DF_glm2$predglm2<0,0,
                            ifelse(DF_glm2$predglm2>2,2,
                                   round(DF_glm2$predglm2,0)))
DF_glm2$Correct <- ifelse(DF_glm2$ActualHitsLanded==DF_glm2$roundedPrediction,1,0)
accuracy4 <- sum(DF_glm2$Correct)/length(DF_glm2$Correct)
accuracy4
```


```{r}
head(DF_glm2)
```
With our adjusted data when validating the prediction of hits landed for Felicia in the reduced and corrected feature data, the Random Forest and Generalized Linear Models scored 90.5% and 89.9% accuracy in prediction respectively.


Now, lets use the random forest and gradient boosted models of python's sklearn package to test the new data of Felicia's when comparing actions to Amanda's later. 

```{r}
library(reticulate)
```

```{r}
conda_list(conda = "auto") 
```

```{r}
use_condaenv(condaenv = "python36")
```

```{python}
import pandas as pd 
import sklearn 
import numpy as np 
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

np.random.seed(47) 
```

Import our ML ready file to run the python algorithms on with sklearn.
```{python}
Felicia = pd.read_csv('FeliciaM-asX2-L-v2.csv', encoding = 'unicode_escape') 
print(Felicia.shape)
```

```{python}
print(Felicia.columns)
```

```{python}
print(Felicia.head())
```

```{python}
print(Felicia.tail())
```

```{python}
print(Felicia['TotLandsX1'].unique())
```
Reorder the rows as instances randomised to split into train and test sets of the data.
```{python}
import numpy as np

Felicia = Felicia.reindex(np.random.permutation(Felicia.index))

print(Felicia.head())
```

```{python}
print(Felicia.tail())
```


There are 529 instances in this data, and 80% is about 424, the target is the hits landed by X1.
```{python}
# Split/splice into training ~ 80% and testing ~ 20%
Felicia_train = Felicia[:424]
Felicia_test = Felicia[424:]
Felicia_hits_train = Felicia['TotLandsX1'][:424]
Felicia_hits_test = Felicia['TotLandsX1'][424:]

print(Felicia_train.shape)
print(Felicia_test.shape)
```


Python's Sklearn's Random Forest and Gradient Boosting Classifier results
```{python}

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import precision_recall_fscore_support as score
import time

```

# Random Forest Classifier
```{python}
rf=RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)
start=time.time()
rf_model=rf.fit(Felicia_train,Felicia_hits_train)
end=time.time()
fit_time=(end-start)
fit_time
```


```{python}
start=time.time()
y_pred=rf_model.predict(Felicia_test)
end=time.time()
pred_time=(end-start)
pred_time
```


```{python}
prd = pd.DataFrame(y_pred)
prd.columns=['Predicted']

prd.index=Felicia_hits_test.index
pred=pd.concat([pd.DataFrame(prd),Felicia_hits_test],axis=1)
print(pred)
```
```{python}
accuracy_Felicia_valid_RF = sum(pred['Predicted']==pred['TotLandsX1'])/len(pred['Predicted'])
accuracy_Felicia_valid_RF
```
The python Random Forest Classifier scored 99% accuracy in prediction for Felicia's actions and reactions. This is a great measure and score to compare her model to Amanda's later.

```{python}
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

print('accuracy', accuracy_score(Felicia_hits_test, y_pred))

print('confusion matrix')
print(confusion_matrix(Felicia_hits_test, y_pred))
```
The above confusion matrix for the random forest classifier for Felicia shows that there weren't any class 2 in the testing set, and that 12/13 1s were classified correctly and that all 0s were classified correctly but that one 1 value for hits landed was classified as a 0 hits landed. The rows are the expected or actual values and the columns are the predicted values in order top to bottom or left to right as lowest to highest value numerically.

Python's Sklearn's Gradient Boosted Model
```{python}
gb=GradientBoostingClassifier(n_estimators=150,max_depth=11)
start=time.time()
gb_model=gb.fit(Felicia_train,Felicia_hits_train)
end=time.time()
fit_time=(end-start)
fit_time
```


```{python}
start=time.time()
y_pred=gb_model.predict(Felicia_test)
end=time.time()
pred_time=(end-start)
pred_time
```


```{python}
prd = pd.DataFrame(y_pred)
prd.columns=['Predicted']

prd.index=Felicia_hits_test.index
pred=pd.concat([pd.DataFrame(prd),Felicia_hits_test],axis=1)
print(pred)
```
```{python}
accuracy_Felicia_valid_GB = sum(pred['Predicted']==pred['TotLandsX1'])/len(pred['Predicted'])
accuracy_Felicia_valid_GB
```
The python Gradient Boosted Classifier scored 100% accuracy in prediction for Felicia's actions and reactions. This is a great measure and score to compare her model to Amanda's later.

```{python}
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

print('accuracy', accuracy_score(Felicia_hits_test, y_pred))

print('confusion matrix')
print(confusion_matrix(Felicia_hits_test, y_pred))
```

Now that we have validated our model and show that our new limited data of Felicia's actions and reactions are 90-100% accurate using R we scored 90% with random forest and generalized linear modeling, and with Python we scored 99-100% with random forest and gradient boosted classifiers, we can start comparing how well Felicia would do when facing Amanda as an opponent.


***

# Comparing Felicia's actions to a fight with Amanda

We are going to use R to show the predictions of hit landed for Felicia against Amanda using random forest and glm as we just did in validating Felicia's prediction model.

```{r}
# library(caret)
```


We will test the random forest model of R's caret package used earlier to score 96.8% accuracy.
```{r, error=FALSE, message=FALSE, warning=FALSE}
rf_cv15 <- train(TotLandsX1~., method='rf', 
               na.action=na.pass,
               data=(TrainingSetFelicia),  preProc = c("center", "scale"),
               trControl=trainControl(method='cv', classProbs = T), number=15)
```

```{r}
predRF_cv15 <- predict(rf_cv15, TestingSetAmanda)

DF_cv15 <- data.frame(predRF_cv15, ActualHitsLanded=TestingSetAmanda$TotLandsX1)

length_cv15 <- length(DF_cv15$ActualHitsLanded)

sum_cv15 <- sum(DF_cv15$predRF_boot==DF_cv15$ActualHitsLanded)

accRF_cv15 <- (sum_cv15/length_cv15)

accRF_cv15
```

```{r}
head(DF_cv15,30)
```

```{r}
DF_cv15$roundedPrediction <- ifelse(DF_cv15$predRF_cv15 < 0, 0, 
                                    ifelse(DF_cv15$predRF_cv15 > 2, 2,
                                           round(DF_cv15$predRF_cv15)
                                           )
                                    )
DF_cv15$Correct <- ifelse(DF_cv15$ActualHitsLanded==DF_cv15$roundedPrediction,1,0)
accuracy1 <- sum(DF_cv15$Correct)/length(DF_cv15$Correct)
accuracy1
```

```{r}
head(DF_cv15)
```
```{r}
sum(DF_cv15$roundedPrediction)
unique(DF_cv15$roundedPrediction)
```
Even though the prediction accuracy for Felicia's hits landed compared to Amanda's opponent Germaine scored a 96% accuracy with random forest, it predicted all hits landed by Felicia to be 0. And recall that the hits landed by Germaine were 13/231. So, it looks like our model using R's random forest for Felicia's training model has a 90% chance that Felicia's hits landed when compared to hits landed by Germaine to Amanda of 96% chance of not landing one hit. Germaine and Felicia do have different fighting styles as Germaine's was mostly on the ground.

Lets see how the generalized linear model scores Felicia's hits landed with Amanda, or how it predicts a hit landed by Felicia.
```{r, error=FALSE, message=FALSE, warning=FALSE}
glmMod2 <- train(TotLandsX1 ~ .,
                method='glm', data=TrainingSetFelicia)
```

```{r, error=FALSE, warning=FALSE, message=FALSE}
predglm2 <- predict(glmMod2, TestingSetAmanda)

DF_glm2 <- data.frame(predglm2, ActualHitsLanded=TestingSetAmanda$TotLandsX1)

length_glm2 <- length(DF_glm2$ActualHitsLanded)

sum_glm2 <- sum(DF_glm2$predglm2==DF_glm2$ActualHitsLanded)

accglm2 <- (sum_glm2/length_glm2)

accglm2
```


```{r}
head(DF_glm2)
```


```{r}
DF_glm2$roundedPrediction <- ifelse(DF_glm2$predglm2<0,0,
                            ifelse(DF_glm2$predglm2>2,2,
                                   round(DF_glm2$predglm2,0)))
DF_glm2$Correct <- ifelse(DF_glm2$ActualHitsLanded==DF_glm2$roundedPrediction,1,0)
accuracy4 <- sum(DF_glm2$Correct)/length(DF_glm2$Correct)
accuracy4
```

```{r}
head(DF_glm2)
```


```{r}
sum(DF_glm2$roundedPrediction)
unique(DF_glm2$roundedPrediction)
```
The GLM model scored 96% accuracy in prediction, similarly as with random forest, but in this model it predicted one instance where Felicia will land a hit out of 231. Lets see what instance that is.
```{r}
hit <- DF_glm2$roundedPrediction==1
FeliciaHit <- TestingSetAmanda[hit,]
FeliciaHit
```
It looks like the prediciton of one hit landed by Felicia is not an actual hit landed by Germaine against Amanda. But under the conditions that Amanda lands no hits that second, 88 seconds into the round, with an accumulated 14 hits by Amanda missed and 4 hits landed by Amanda and a cumulative total of 2 hits recieved by Amanda (from 2 hits landed by opponent), and 1 second since an action taken by either fighter, as well as a knee and hook missed by Amanda and holding some part of the body while being caught in a hold of some sort, Amanda having had 6 holds up to 88 seconds in the round, and Amanda having lost 1 of those holds, Amanda having been caught in 10 holds and broken out of 1 hold, attempted 5 muay thai kicks and 0 push kicks.
Lets see what those holds are that Amanda is in and that Germaine was in, that the predictive model is predicting Felicia to land a hit. It is the instance 88 seconds into the round with Germaine from Amanda's data that included the notes and action notes of each fighter. In this exact data table X1 is Amanda, and that's who's actions we're interested in.
```{r}
AllElseEqual <- subset(AmandaML, AmandaML$SecondsIntoRound==88 & AmandaML$Notes=='Germaine')
AllElseEqual$FighterActionReactions.X1
AllElseEqual$FightersActionsReactions.X2
```
So, when Amanda is caught in an open guard and holding opponent's Left knee and also misses a downward Right hook to the head of her opponent (Germaine) the glm model predicts that Felicia would react unlike Germaine and will land a hit. Because Germaine didn't actually land a hit in this instance but based on Felicia's model we trained, it predicts Felicia would land a hit without specifying which one. Interesting. Now, lets see how Python compares the fight between Felicia and Amanda.


Python models to predict hits landed for Felicia against Amanda.
```{r}
library(reticulate)
```

```{r}
conda_list(conda = "auto") 
```

```{r}
use_condaenv(condaenv = "python36")
```

```{python}
import pandas as pd 
import sklearn 
import numpy as np 
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

np.random.seed(47) 
```

Import our ML ready file to run the python algorithms on with sklearn.
```{python}
TrainingSetFelicia = pd.read_csv('FeliciaM-asX2-L-v2.csv', encoding = 'unicode_escape') 
print(TrainingSetFelicia.shape)
TestingSetAmanda = pd.read_csv('AmandaML-asX1-v2.csv', encoding='unicode_escape')
print(TestingSetAmanda.shape)
```

```{python}
Felicia_train = TrainingSetFelicia
Felicia_test = TestingSetAmanda
Felicia_hits_train = TrainingSetFelicia['TotLandsX1']
Felicia_hits_test = TestingSetAmanda['TotLandsX1']

print(Felicia_train.shape)
print(Felicia_test.shape)
```


Python's Sklearn's Random Forest and Gradient Boosting Classifier results
```{python}

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import precision_recall_fscore_support as score
import time

```

# Random Forest Classifier
```{python}
rf=RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)
start=time.time()
rf_model=rf.fit(Felicia_train,Felicia_hits_train)
end=time.time()
fit_time=(end-start)
fit_time
```

```{python}
start=time.time()
y_pred=rf_model.predict(Felicia_test)
end=time.time()
pred_time=(end-start)
pred_time
```


```{python}
prd = pd.DataFrame(y_pred)
prd.columns=['Predicted']

prd.index=Felicia_hits_test.index
pred=pd.concat([pd.DataFrame(prd),Felicia_hits_test],axis=1)
print(pred)
```

```{python}
accuracy_Felicia_valid_RF = sum(pred['Predicted']==pred['TotLandsX1'])/len(pred['Predicted'])
accuracy_Felicia_valid_RF
```
The python Random Forest Classifier scored 97.8% accuracy in prediction for Felicia's actions and reactions. 
```{python}
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

print('accuracy', accuracy_score(Felicia_hits_test, y_pred))

print('confusion matrix')
print(confusion_matrix(Felicia_hits_test, y_pred))
```

All 0 hits were predicted to be 0, all 1 hits were predicted to be 1s, but all 2 hits landed were predicted to be 1 hit landed. There were 13 total hits by Germaine that we are predicting if Felicia will land any hits as actions compared to the actions of Amanda when they compete against each other. This model predicted that Felicia will also land 13 hits but they aren't all the exact same instances that Germaine landed her hits against Amanda. Lets see which instances those are.
```{python}
hitsPredicted = pred[pred['Predicted']>0]
hitsPredicted
```

```{python}
rows = hitsPredicted.index
rows

```

```{python}
TestingSetAmanda.shape
```
```{python}
TestingSetAmanda.iloc[rows]
```
Lets look at these indices in the R version of the table with the action notes as descriptive information.
```{r}
predHitsPyRF <- c(5, 7, 51, 57, 58, 60, 62, 65)
AmandaML[predHitsPyRF,19]
```

From the above data, we can see that Python's sklearn's random forest predicts that when X1 as Felicia is against X2 as Amanda under the above circumstances Felicia will land a hit. Amanda is open stand up, Felicia is in open guard trying to kick Amanda off of her, Amanda is holding Felicia's upper body and Felicia is holding Amanda's Right arm, Amanda is holding Felicia's upper body and Felicia is holding Amanda's R arm, and when Amanda holds Felicia against the cage holding her upper body and Felicia is holding her right arm then Felicia will land a Right knee to the upper leg of Amanda.

Now lets see how the gradient boosting classifier predicts hits landed for Felicia against Amanda.
```{python}
gb=GradientBoostingClassifier(n_estimators=150,max_depth=11)
start=time.time()
gb_model=gb.fit(Felicia_train,Felicia_hits_train)
end=time.time()
fit_time=(end-start)
fit_time
```


```{python}
start=time.time()
y_pred=gb_model.predict(Felicia_test)
end=time.time()
pred_time=(end-start)
pred_time
```


```{python}
prd = pd.DataFrame(y_pred)
prd.columns=['Predicted']

prd.index=Felicia_hits_test.index
pred=pd.concat([pd.DataFrame(prd),Felicia_hits_test],axis=1)
print(pred)
```

```{python}
accuracy_Felicia_valid_GB = sum(pred['Predicted']==pred['TotLandsX1'])/len(pred['Predicted'])
accuracy_Felicia_valid_GB
```
The python Gradient Boosted Classifier scored 100% accuracy in prediction for Felicia's actions and reactions. 
```{python}
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

print('accuracy', accuracy_score(Felicia_hits_test, y_pred))

print('confusion matrix')
print(confusion_matrix(Felicia_hits_test, y_pred))
```
We can see that the predicted 0s, 1s, and 2s, of hits landed per second by Felicia are the same predicted hits landed by Germaine against Amanda. The gradient boosted model was able to correctly predict all the 2 hits per second landed that the random forest classifier didn't. This says that the Gradient Boosted Classifer thinks Amanda's hits against her three opponents' actions will be identical to the same type of actions Germaine had when fighting Amanda.
```{python}
hitsPredicted = pred[pred['Predicted']==1]
hitsPredicted
hitsPredicted2 = pred[pred['Predicted']==2]
hitsPredicted2

```

```{python}
rows = hitsPredicted.index
rows
rows2 = hitsPredicted2.index
rows2
```

```{python}
TestingSetAmanda.iloc[rows]
TestingSetAmanda.iloc[rows2]

```
The following circumstances or conditions are for Amanda, when under these conditions, it is predicted that Felicia will land 1 hit per second.
```{r}
predHits1PyGB <- c(5, 7, 51)
AmandaML[predHits1PyGB,19]
```

The following actions/reactions are for Amanda, when Amanda is under these similar circumstances Felicia is predicted to land 2 hits per second. 
```{r}
predHits2PyGB <- c(57, 58, 60, 62, 65)
AmandaML[predHits2PyGB,19]

```

That was interesting to see how these models predicted that Felicia Spencer would do against her opponent in the upcoming June 2, 2020 fight with Amanda Nunez. Given the data we have on Felicia we trained a model to test on her fighter reaction with 3 fighters' first round fights with Felicia, and then tested those reactions with 90-100% accuracy in prediction. We then used those models to test how well they would predict Felicia to react with a successful landed hit against the actions of Amanda. It looks like the sklearn Gradient Boosted classifier thinks that Amanda is 100% similar to Germaine's reactions against Amanda for hits landed, sklearn's random forest thinks she is 97% similar but not going to land as many fast hits per second as Germaine. And for Random forest in R's caret package, it predicts Felicia will not land any hits in her fight with Amanda while the generalized linear models R caret package predicts she will land one hit against Amanda if she is on her back in open guard where Amanda misses a Right hook to Felicia's head and while Amanda is holding Felicia's Left knee in her open guard. 

There is more that could be done to do action by action of the most actions used, and predict one of those actions being used or landed based on Amanda's actions, and a great number of variations of this model could predict likely results. We can have fun assuming this model works and see how the fight plays out. These fighters could be learning from their mistakes and the others' previous fights available of their opponent to master the others weaknesses much like modifications and tuned parameter variations of our models to improve hits landed and go for their desired outcome. Outcomes weren't recorded for this ML project, but Amanda won against Germaine after five rounds as a unanimous decision when she almost had a technical knockout in the first round of Germaine but the referee didn't call it. And the Felicia fights ended in her victory over Zarah and Megan in submissions of a Rear Naked Choke or RNC in the first round and a unanimous decision loss to Christiane Cyborg after all five rounds of five minutes each. 

